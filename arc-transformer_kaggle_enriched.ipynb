{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-16T01:05:37.273319Z",
     "iopub.status.busy": "2025-11-16T01:05:37.273003Z",
     "iopub.status.idle": "2025-11-16T01:05:40.679506Z",
     "shell.execute_reply": "2025-11-16T01:05:40.678507Z",
     "shell.execute_reply.started": "2025-11-16T01:05:37.273293Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "AUGMENTATION=True\n",
    "EVAL_COLOR_DIST=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:05:43.593585Z",
     "iopub.status.busy": "2025-11-16T01:05:43.592715Z",
     "iopub.status.idle": "2025-11-16T01:05:43.606280Z",
     "shell.execute_reply": "2025-11-16T01:05:43.605547Z",
     "shell.execute_reply.started": "2025-11-16T01:05:43.593557Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ARCTokenizer:\n",
    "    \"\"\"Tokenizer for ARC challenges with special tokens for structure\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Value tokens (0-9)\n",
    "        self.value_tokens = list(range(10))\n",
    "        \n",
    "        # Special tokens\n",
    "        self.PAD_TOKEN = 10\n",
    "        self.SOS_TOKEN = 11  # Start of sequence\n",
    "        self.EOS_TOKEN = 12  # End of sequence\n",
    "        self.TRAIN_TOKEN = 13  # Start of training example\n",
    "        self.TEST_TOKEN = 14  # Start of test example\n",
    "        self.INPUT_TOKEN = 15  # Start of input grid\n",
    "        self.OUTPUT_TOKEN = 16  # Start of output grid\n",
    "        self.NEWLINE_TOKEN = 17  # Grid separator (], [)\n",
    "        \n",
    "        self.vocab_size = 18\n",
    "        \n",
    "        # Token mappings\n",
    "        self.token_to_id = {\n",
    "            'PAD': self.PAD_TOKEN,\n",
    "            'SOS': self.SOS_TOKEN,\n",
    "            'EOS': self.EOS_TOKEN,\n",
    "            'TRAIN': self.TRAIN_TOKEN,\n",
    "            'TEST': self.TEST_TOKEN,\n",
    "            'INPUT': self.INPUT_TOKEN,\n",
    "            'OUTPUT': self.OUTPUT_TOKEN,\n",
    "            'NEWLINE': self.NEWLINE_TOKEN\n",
    "        }\n",
    "    \n",
    "    def grid_to_tokens(self, grid: List[List[int]]) -> List[int]:\n",
    "        \"\"\"Convert 2D grid to token sequence\"\"\"\n",
    "        if not grid or not grid[0]:\n",
    "            return []\n",
    "        \n",
    "        tokens = []\n",
    "        for i, row in enumerate(grid):\n",
    "            for j, value in enumerate(row):\n",
    "                tokens.append(value)  # Just the value, position will be encoded separately\n",
    "            if i < len(grid) - 1:  # Add newline between rows (except last)\n",
    "                tokens.append(self.NEWLINE_TOKEN)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def tokens_to_grid(self, tokens: List[int], target_shape: Tuple[int, int]) -> List[List[int]]:\n",
    "        \"\"\"Convert token sequence back to 2D grid\"\"\"\n",
    "        h, w = target_shape\n",
    "        grid = [[0 for _ in range(w)] for _ in range(h)]\n",
    "        \n",
    "        # Filter out special tokens and newlines\n",
    "        values = [t for t in tokens if t < 10]  # Only keep value tokens (0-9)\n",
    "        \n",
    "        idx = 0\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                if idx < len(values):\n",
    "                    grid[i][j] = values[idx]\n",
    "                    idx += 1\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "    def create_input_sequence(self, train_examples: List[Dict], test_input: List[List[int]]) -> List[int]:\n",
    "        \"\"\"Create input sequence from training examples and test input\"\"\"\n",
    "        sequence = [self.SOS_TOKEN]\n",
    "        \n",
    "        # Add training examples (exactly 2)\n",
    "        for i, example in enumerate(train_examples[:2]):\n",
    "            sequence.append(self.TRAIN_TOKEN)\n",
    "            \n",
    "            # Add input\n",
    "            sequence.append(self.INPUT_TOKEN)\n",
    "            input_tokens = self.grid_to_tokens(example['input'])\n",
    "            sequence.extend(input_tokens)\n",
    "            \n",
    "            # Add output\n",
    "            sequence.append(self.OUTPUT_TOKEN)\n",
    "            output_tokens = self.grid_to_tokens(example['output'])\n",
    "            sequence.extend(output_tokens)\n",
    "        \n",
    "        # Add test input\n",
    "        sequence.append(self.TEST_TOKEN)\n",
    "        sequence.append(self.INPUT_TOKEN)\n",
    "        test_tokens = self.grid_to_tokens(test_input)\n",
    "        sequence.extend(test_tokens)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def create_target_sequence(self, target_grid: List[List[int]]) -> List[int]:\n",
    "        \"\"\"Create target sequence for training\"\"\"\n",
    "        sequence = [self.SOS_TOKEN]\n",
    "        sequence.append(self.OUTPUT_TOKEN)\n",
    "        target_tokens = self.grid_to_tokens(target_grid)\n",
    "        sequence.extend(target_tokens)\n",
    "        sequence.append(self.EOS_TOKEN)\n",
    "        return sequence\n",
    "    \n",
    "    def pad_sequence(self, sequence: List[int], max_length: int) -> List[int]:\n",
    "        \"\"\"Pad sequence to max_length\"\"\"\n",
    "        if len(sequence) > max_length:\n",
    "            return sequence[:max_length]\n",
    "        return sequence + [self.PAD_TOKEN] * (max_length - len(sequence))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:05:43.738813Z",
     "iopub.status.busy": "2025-11-16T01:05:43.738521Z",
     "iopub.status.idle": "2025-11-16T01:05:43.754339Z",
     "shell.execute_reply": "2025-11-16T01:05:43.753443Z",
     "shell.execute_reply.started": "2025-11-16T01:05:43.738793Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TokenTo3DConverter:\n",
    "    \"\"\"Converts token sequences to 3D vectors [value, x, y] with coordinate information\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer: ARCTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def tokens_to_3d(self, \n",
    "                     tokens: List[int],\n",
    "                     input_dims: List[Tuple[int, int]],\n",
    "                     output_dims: List[Tuple[int, int]],\n",
    "                     test_input_dims: Tuple[int, int],\n",
    "                     test_output_dims: Optional[Tuple[int, int]] = None,\n",
    "                     is_target: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert token sequence to 3D vectors [value, x, y]\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of token IDs\n",
    "            input_dims: List of (height, width) for training input grids\n",
    "            output_dims: List of (height, width) for training output grids\n",
    "            test_input_dims: (height, width) for test input grid\n",
    "            is_target: If True, this is a target sequence (starts with OUTPUT_TOKEN)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [seq_len, 3] where each row is [value, x, y]\n",
    "            Special tokens have x=-1, y=-1\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        \n",
    "        # Track current grid context\n",
    "        current_grid_type = None  # 'train_input', 'train_output', 'test_input'\n",
    "        current_grid_idx = 0\n",
    "        current_row = 0\n",
    "        current_col = 0\n",
    "        current_grid_dims = None\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "            \n",
    "            # Handle special tokens that change context\n",
    "            if token == self.tokenizer.SOS_TOKEN:\n",
    "                result.append([token, -1, -1])\n",
    "                i += 1\n",
    "                continue\n",
    "            elif token == self.tokenizer.EOS_TOKEN:\n",
    "                result.append([token, -1, -1])\n",
    "                i += 1\n",
    "                continue\n",
    "            elif token == self.tokenizer.PAD_TOKEN:\n",
    "                result.append([token, -1, -1])\n",
    "                i += 1\n",
    "                continue\n",
    "            elif token == self.tokenizer.TRAIN_TOKEN:\n",
    "                current_grid_type = None\n",
    "                current_grid_idx = 0\n",
    "                result.append([token, -1, -1])\n",
    "                i += 1\n",
    "                continue\n",
    "            elif token == self.tokenizer.TEST_TOKEN:\n",
    "                current_grid_type = None\n",
    "                result.append([token, -1, -1])\n",
    "                i += 1\n",
    "                continue\n",
    "            elif token == self.tokenizer.INPUT_TOKEN:\n",
    "                # Determine which input grid we're in\n",
    "                if is_target:\n",
    "                    # In target sequence, INPUT_TOKEN shouldn't appear\n",
    "                    result.append([token, -1, -1])\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                if current_grid_type is None:\n",
    "                    # First INPUT after TRAIN - this is training input\n",
    "                    if current_grid_idx < len(input_dims):\n",
    "                        current_grid_dims = input_dims[current_grid_idx]\n",
    "                        current_grid_type = 'train_input'\n",
    "                elif current_grid_type == 'train_output':\n",
    "                    # INPUT after OUTPUT in training - next training example\n",
    "                    current_grid_idx += 1\n",
    "                    if current_grid_idx < len(input_dims):\n",
    "                        current_grid_dims = input_dims[current_grid_idx]\n",
    "                        current_grid_type = 'train_input'\n",
    "                elif current_grid_type is None:\n",
    "                    # INPUT after TEST - this is test input\n",
    "                    current_grid_dims = test_input_dims\n",
    "                    current_grid_type = 'test_input'\n",
    "                \n",
    "                current_row = 0\n",
    "                current_col = 0\n",
    "                result.append([token, -1, -1])\n",
    "                i += 1\n",
    "                continue\n",
    "            elif token == self.tokenizer.OUTPUT_TOKEN:\n",
    "                # Determine which output grid we're in\n",
    "                if current_grid_type == 'train_input':\n",
    "                    # OUTPUT after INPUT in training\n",
    "                    if current_grid_idx < len(output_dims):\n",
    "                        current_grid_dims = output_dims[current_grid_idx]\n",
    "                        current_grid_type = 'train_output'\n",
    "                elif current_grid_type is None:\n",
    "                    # OUTPUT at start (for target sequence) or after TEST\n",
    "                    if is_target:\n",
    "                        # For target sequence, use test_output_dims if available\n",
    "                        if test_output_dims is not None:\n",
    "                            current_grid_dims = test_output_dims\n",
    "                        elif len(output_dims) > 0:\n",
    "                            current_grid_dims = output_dims[0]  # Fallback to first output dims\n",
    "                        else:\n",
    "                            current_grid_dims = (1, 1)  # Default fallback\n",
    "                    elif len(output_dims) > 0:\n",
    "                        current_grid_dims = output_dims[0]\n",
    "                    current_grid_type = 'train_output'\n",
    "                \n",
    "                current_row = 0\n",
    "                current_col = 0\n",
    "                result.append([token, -1, -1])\n",
    "                i += 1\n",
    "                continue\n",
    "            elif token == self.tokenizer.NEWLINE_TOKEN:\n",
    "                # Move to next row\n",
    "                if current_grid_dims is not None:\n",
    "                    current_row += 1\n",
    "                    current_col = 0\n",
    "                result.append([token, -1, -1])\n",
    "                i += 1\n",
    "                continue\n",
    "            elif token < 10:  # Value token (0-9)\n",
    "                # This is a grid value - add coordinates\n",
    "                if current_grid_dims is not None:\n",
    "                    h, w = current_grid_dims\n",
    "                    # Clamp to valid ranges\n",
    "                    row = min(current_row, h - 1)\n",
    "                    col = min(current_col, w - 1)\n",
    "                    result.append([token, col, row])  # x=col, y=row\n",
    "                    \n",
    "                    # Move to next column\n",
    "                    current_col += 1\n",
    "                else:\n",
    "                    # No grid context, treat as special\n",
    "                    result.append([token, -1, -1])\n",
    "                i += 1\n",
    "            else:\n",
    "                # Unknown token, treat as special\n",
    "                result.append([token, -1, -1])\n",
    "                i += 1\n",
    "        \n",
    "        return torch.tensor(result, dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:05:43.892367Z",
     "iopub.status.busy": "2025-11-16T01:05:43.892036Z",
     "iopub.status.idle": "2025-11-16T01:05:43.901651Z",
     "shell.execute_reply": "2025-11-16T01:05:43.900584Z",
     "shell.execute_reply.started": "2025-11-16T01:05:43.892343Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ARCDataset:\n",
    "    \"\"\"Dataset class for ARC challenges with data augmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, challenges_path: str, solutions_path: str = None):\n",
    "        self.challenges_path = challenges_path\n",
    "        self.solutions_path = solutions_path\n",
    "        \n",
    "        # Load challenges\n",
    "        with open(challenges_path, 'r') as f:\n",
    "            self.challenges = json.load(f)\n",
    "        \n",
    "        # Load solutions if provided\n",
    "        self.solutions = None\n",
    "        if solutions_path:\n",
    "            with open(solutions_path, 'r') as f:\n",
    "                self.solutions = json.load(f)\n",
    "    \n",
    "    def get_challenge_data(self, challenge_id: str) -> Dict:\n",
    "        \"\"\"Get data for a specific challenge\"\"\"\n",
    "        challenge = self.challenges[challenge_id]\n",
    "        \n",
    "        # Get training examples\n",
    "        train_examples = challenge.get('train', [])\n",
    "        \n",
    "        # Get test examples\n",
    "        test_examples = challenge.get('test', [])\n",
    "        \n",
    "        # Get solution if available\n",
    "        solution = None\n",
    "        if self.solutions and challenge_id in self.solutions:\n",
    "            solution = self.solutions[challenge_id][0]  # First solution\n",
    "        \n",
    "        return {\n",
    "            'train_examples': train_examples,\n",
    "            'test_examples': test_examples,\n",
    "            'solution': solution,\n",
    "            'challenge_id': challenge_id\n",
    "        }\n",
    "    \n",
    "    def get_all_challenges(self) -> List[str]:\n",
    "        \"\"\"Get list of all challenge IDs\"\"\"\n",
    "        return list(self.challenges.keys())\n",
    "    \n",
    "    def create_augmented_samples(self, challenge_id: str) -> List[Dict]:\n",
    "        \"\"\"Create a single training sample from a challenge with 2 train examples and 1 test input\"\"\"\n",
    "        data = self.get_challenge_data(challenge_id)\n",
    "        \n",
    "        samples = []\n",
    "        \n",
    "        # Always use first 2 training examples and first test input\n",
    "        if len(data['train_examples']) >= 2:\n",
    "            train_examples = data['train_examples'][:2]  # First 2 training examples\n",
    "            test_input = data['test_examples'][0]['input'] if data['test_examples'] else []\n",
    "            test_output = data['solution']\n",
    "            \n",
    "            samples.append({\n",
    "                'train_examples': train_examples,\n",
    "                'test_input': test_input,\n",
    "                'test_output': test_output,\n",
    "                'challenge_id': challenge_id,\n",
    "                'sample_id': challenge_id  # Simple ID, no augmentation suffix\n",
    "            })\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:05:44.040940Z",
     "iopub.status.busy": "2025-11-16T01:05:44.040098Z",
     "iopub.status.idle": "2025-11-16T01:05:44.052970Z",
     "shell.execute_reply": "2025-11-16T01:05:44.052047Z",
     "shell.execute_reply.started": "2025-11-16T01:05:44.040909Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ARCTorchDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for ARC challenges\"\"\"\n",
    "    \n",
    "    def __init__(self, arc_dataset: ARCDataset, tokenizer: ARCTokenizer, \n",
    "                 token_converter = None):  # Optional converter\n",
    "        self.arc_dataset = arc_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_converter = token_converter  # Optional converter for 3D vectors\n",
    "        \n",
    "        # Create all samples with augmentation\n",
    "        self.samples = []\n",
    "        for challenge_id in arc_dataset.get_all_challenges():\n",
    "            samples = arc_dataset.create_augmented_samples(challenge_id)\n",
    "            self.samples.extend(samples)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Create input sequence\n",
    "        input_seq = self.tokenizer.create_input_sequence(\n",
    "            sample['train_examples'], \n",
    "            sample['test_input']\n",
    "        )\n",
    "        \n",
    "        # Create target sequence\n",
    "        if sample['test_output']:\n",
    "            target_seq = self.tokenizer.create_target_sequence(sample['test_output'])\n",
    "        else:\n",
    "            # Create dummy target for test data\n",
    "            target_seq = [self.tokenizer.SOS_TOKEN, self.tokenizer.EOS_TOKEN]\n",
    "        \n",
    "        # Pad sequences\n",
    "        input_seq = self.tokenizer.pad_sequence(input_seq, 5400)   # 5 * 30x30 + bunch of extra tokens + possible target 30x30= 6*30x30\n",
    "        target_seq = self.tokenizer.pad_sequence(target_seq, 1000) # max 30x30 + punch of extra tokens\n",
    "        \n",
    "        # Calculate dimensions\n",
    "        input_dims = []\n",
    "        output_dims = []\n",
    "        \n",
    "        for example in sample['train_examples']:\n",
    "            input_dims.append((len(example['input']), len(example['input'][0]) if example['input'] else 0))\n",
    "            output_dims.append((len(example['output']), len(example['output'][0]) if example['output'] else 0))\n",
    "        \n",
    "        test_input_dims = (len(sample['test_input']), len(sample['test_input'][0]) if sample['test_input'] else 0)\n",
    "        test_output_dims = (len(sample['test_output']), len(sample['test_output'][0]) if sample['test_output'] else 0)\n",
    "        \n",
    "        # Convert to 3D vectors if converter is provided\n",
    "        if self.token_converter is not None:\n",
    "            input_3d = self.token_converter.tokens_to_3d(\n",
    "                input_seq,\n",
    "                input_dims,\n",
    "                output_dims,\n",
    "                test_input_dims,\n",
    "                test_output_dims=test_output_dims,\n",
    "                is_target=False\n",
    "            )\n",
    "            target_3d = self.token_converter.tokens_to_3d(\n",
    "                target_seq,\n",
    "                input_dims,\n",
    "                output_dims,\n",
    "                test_input_dims,\n",
    "                test_output_dims=test_output_dims,\n",
    "                is_target=True\n",
    "            )\n",
    "            return {\n",
    "                'input': input_3d,  # Shape: [seq_len, 3] - [value, x, y]\n",
    "                'target': target_3d,  # Shape: [seq_len, 3] - [value, x, y]\n",
    "                'input_tokens': torch.tensor(input_seq, dtype=torch.int8),  # Keep original tokens too (int8 for memory efficiency)\n",
    "                'target_tokens': torch.tensor(target_seq, dtype=torch.int8),  # Keep original tokens too (int8 for memory efficiency)\n",
    "                'sample_id': sample['sample_id'],\n",
    "                'challenge_id': sample['challenge_id'],\n",
    "                'input_dims': input_dims,\n",
    "                'output_dims': output_dims,\n",
    "                'test_input_dims': test_input_dims,\n",
    "                'test_output_dims': test_output_dims\n",
    "            }\n",
    "        else:\n",
    "            # Return original token format\n",
    "            return {\n",
    "                'input': torch.tensor(input_seq, dtype=torch.int8),  # int8 for memory efficiency\n",
    "                'target': torch.tensor(target_seq, dtype=torch.int8),  # int8 for memory efficiency\n",
    "                'sample_id': sample['sample_id'],\n",
    "                'challenge_id': sample['challenge_id'],\n",
    "                'input_dims': input_dims,\n",
    "                'output_dims': output_dims,\n",
    "                'test_input_dims': test_input_dims,\n",
    "                'test_output_dims': test_output_dims\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:05:45.119481Z",
     "iopub.status.busy": "2025-11-16T01:05:45.118760Z",
     "iopub.status.idle": "2025-11-16T01:05:45.132776Z",
     "shell.execute_reply": "2025-11-16T01:05:45.131962Z",
     "shell.execute_reply.started": "2025-11-16T01:05:45.119442Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ARCExplodedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Explodes ARCTorchDataset into trainable samples.\n",
    "    \n",
    "    Takes each sample from ARCTorchDataset and creates multiple training samples:\n",
    "    - Sample 0: input → predict target[0]\n",
    "    - Sample 1: input + target[0] → predict target[1]\n",
    "    - Sample 2: input + target[0:2] → predict target[2]\n",
    "    - etc.\n",
    "    \n",
    "    Expects both input and target to be in 3D vector format [value, x, y].\n",
    "    When adding target tokens:\n",
    "    1. Loop through input sequence and replace first PAD token with target token\n",
    "    2. If no PAD token found, append to end and remove first token\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, torch_dataset: ARCTorchDataset, tokenizer: ARCTokenizer, sequence_length: int = 5400):\n",
    "        self.torch_dataset = torch_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Create all exploded samples\n",
    "        self.exploded_samples = []\n",
    "        \n",
    "        print(f\"Exploding {len(torch_dataset)} base samples...\")\n",
    "        for base_idx in tqdm(range(len(torch_dataset))):\n",
    "            base_sample = torch_dataset[base_idx]\n",
    "            \n",
    "            # Get input and target as 3D tensors [seq_len, 3]\n",
    "            input_3d = base_sample['input']  # Shape: [max_length, 3]\n",
    "            target_3d = base_sample['target']  # Shape: [max_length, 3]\n",
    "            \n",
    "            # Find actual length of input (before padding)\n",
    "            # PAD token has value = PAD_TOKEN (10), x = -1, y = -1\n",
    "            input_actual_len = 0\n",
    "            for i in range(input_3d.shape[0]):\n",
    "                if input_3d[i, 0].item() == self.tokenizer.PAD_TOKEN:\n",
    "                    break\n",
    "            input_actual_len = i-1\n",
    "            \n",
    "            target_actual_len = 0\n",
    "            for i in range(target_3d.shape[0]):\n",
    "                if target_3d[i, 0].item() == self.tokenizer.PAD_TOKEN:\n",
    "                    break\n",
    "            target_actual_len = i-1\n",
    "            \n",
    "            #print(input_actual_len, target_actual_len)\n",
    "            \n",
    "            target_vectors = target_3d[0:target_actual_len]\n",
    "            \n",
    "            \n",
    "            # Optimized version - remove unnecessary cloning and use input_actual_len directly\n",
    "            # Replace the target_vectors collection and loop in cell 9 with this:\n",
    "\n",
    "            # In the target_vectors collection (around line 43-50):\n",
    "            # Change: target_vectors.append(target_3d[i].clone())\n",
    "            # To:     target_vectors.append(target_3d[i])  # No clone needed\n",
    "\n",
    "            # In the loop (around line 58-67):\n",
    "            # Replace the entire loop with this optimized version:\n",
    "\n",
    "            # Start with full input sequence (we'll modify it in place)\n",
    "            current_seq = input_3d.clone()\n",
    "            for i, target_vector in enumerate(target_vectors):\n",
    "                # Calculate position where we should place this target token\n",
    "                # Start from input_actual_len and add i (position in target sequence)\n",
    "                target_pos = input_actual_len + i\n",
    "                \n",
    "                if i>0:\n",
    "                    # first target vector is not added to the input sequence\n",
    "                    if target_pos < self.sequence_length:\n",
    "                        # Check if position has a PAD token\n",
    "                        if current_seq[target_pos, 0].item() == self.tokenizer.PAD_TOKEN:\n",
    "                            # Replace PAD token with target vector\n",
    "                            current_seq[target_pos] = target_vectors[i-1]\n",
    "                        else:\n",
    "                            #print(\"Sequence is full - append and remove from beginning\", target_pos, current_seq.shape)\n",
    "                            # Sequence is full - append and remove from beginning\n",
    "                            current_seq = torch.cat([current_seq[1:], target_vectors[i-1].unsqueeze(0)], dim=0)\n",
    "                    else:\n",
    "                        current_seq = torch.cat([current_seq[1:], target_vectors[i-1].unsqueeze(0)], dim=0)\n",
    "\n",
    "                # Store exploded sample\n",
    "                exploded_sample = {\n",
    "                    'input_3d': current_seq.clone(),\n",
    "                    'target_vector': target_vector.clone(),  # Clone here since we store it separately\n",
    "                    'target_position': i,\n",
    "                    'base_sample_idx': base_idx,\n",
    "                    'base_sample_id': base_sample.get('sample_id', f'sample_{base_idx}'),\n",
    "                    'challenge_id': base_sample.get('challenge_id', ''),\n",
    "                    'input_dims': base_sample.get('input_dims', []),\n",
    "                    'output_dims': base_sample.get('output_dims', []),\n",
    "                    'test_input_dims': base_sample.get('test_input_dims', (0, 0)),\n",
    "                    'test_output_dims': base_sample.get('test_output_dims', (0, 0)),\n",
    "                }\n",
    "\n",
    "                self.exploded_samples.append(exploded_sample)\n",
    "        \n",
    "        print(f\"Created {len(self.exploded_samples)} exploded samples from {len(torch_dataset)} base samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.exploded_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.exploded_samples[idx]\n",
    "        \n",
    "        input_3d = sample['input_3d']  # Shape: [max_length, 3]\n",
    "        target_vector = sample['target_vector']  # Shape: [3]\n",
    "        \n",
    "        # Create attention mask (1 for non-padding, 0 for padding)\n",
    "        attention_mask = (input_3d[:, 0] != self.tokenizer.PAD_TOKEN).long()\n",
    "        \n",
    "        return {\n",
    "            'input_3d': input_3d,  # [max_length, 3] - full 3D vectors\n",
    "            'target_vector': target_vector,  # [3] - target as 3D vector\n",
    "            'target_value': target_vector[0].item(),  # Just the value token for convenience\n",
    "            'attention_mask': attention_mask,  # [max_length]\n",
    "            'target_position': sample['target_position'],\n",
    "            'base_sample_idx': sample['base_sample_idx'],\n",
    "            'base_sample_id': sample['base_sample_id'],\n",
    "            'challenge_id': sample['challenge_id'],\n",
    "            'input_dims': sample['input_dims'],\n",
    "            'output_dims': sample['output_dims'],\n",
    "            'test_input_dims': sample['test_input_dims'],\n",
    "            'test_output_dims': sample['test_output_dims'],\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:05:45.371248Z",
     "iopub.status.busy": "2025-11-16T01:05:45.370888Z",
     "iopub.status.idle": "2025-11-16T01:05:45.381927Z",
     "shell.execute_reply": "2025-11-16T01:05:45.381000Z",
     "shell.execute_reply.started": "2025-11-16T01:05:45.371222Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# augmentation during training\n",
    "def apply_random_color_mapping(sample, apply_probability=1.0):\n",
    "    \"\"\"\n",
    "    Apply a random color permutation (0-9) to a sample.\n",
    "    OPTIMIZED: Uses vectorized PyTorch operations instead of Python loops.\n",
    "    \n",
    "    Args:\n",
    "        sample: Dict with 'input_3d' and 'target_vector'\n",
    "        apply_probability: Probability of applying augmentation (1.0 = always, 0.5 = 50% chance)\n",
    "    \n",
    "    Returns:\n",
    "        Augmented sample with permuted colors\n",
    "    \"\"\"\n",
    "    if np.random.random() > apply_probability:\n",
    "        return sample  # Skip augmentation\n",
    "    \n",
    "    # Create random permutation of colors 0-9\n",
    "    permuted_colors = torch.randperm(10, dtype=torch.int8)  # Vectorized permutation\n",
    "    \n",
    "    # Create lookup table: mapping[old_color] = new_color\n",
    "    # For colors 0-9, use permuted mapping; for special tokens (10+), keep original\n",
    "    mapping = torch.arange(18, dtype=torch.int8)  # Default: identity mapping\n",
    "    mapping[:10] = permuted_colors  # Apply permutation to colors 0-9\n",
    "    \n",
    "    # Apply to input_3d (vectorized - much faster!)\n",
    "    input_3d = sample['input_3d'].clone()\n",
    "    color_values = input_3d[:, 0].int()  # Extract color values\n",
    "    input_3d[:, 0] = mapping[color_values]  # Apply mapping in one operation\n",
    "    \n",
    "    # Apply to target_vector\n",
    "    target_vector = sample['target_vector'].clone()\n",
    "    target_color = target_vector[0].int()\n",
    "    target_vector[0] = mapping[target_color]\n",
    "    \n",
    "    # Create augmented sample\n",
    "    augmented_sample = sample.copy()\n",
    "    augmented_sample['input_3d'] = input_3d\n",
    "    augmented_sample['target_vector'] = target_vector\n",
    "    augmented_sample['target_value'] = int(target_vector[0].item())\n",
    "    \n",
    "    return augmented_sample\n",
    "\n",
    "# Dataset wrapper for live color augmentation\n",
    "class AugmentedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper that applies random color permutation augmentation on-the-fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset, apply_probability=1.0):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.apply_probability = apply_probability\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.base_dataset[idx]\n",
    "        return apply_random_color_mapping(sample, self.apply_probability)\n",
    "\n",
    "\n",
    "# Alternative: Augment in collate_fn (even simpler)\n",
    "def collate_fn_with_augmentation(batch, apply_probability=1.0):\n",
    "    \"\"\"\n",
    "    Collate function that applies color augmentation to each sample.\n",
    "    \n",
    "    Usage:\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset_split,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lambda b: collate_fn_with_augmentation(b, apply_probability=1.0),\n",
    "            num_workers=4\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Apply augmentation to each sample\n",
    "    augmented_batch = [apply_random_color_mapping(sample, apply_probability) for sample in batch]\n",
    "    \n",
    "    # Original collate logic\n",
    "    input_3d = torch.stack([item['input_3d'] for item in augmented_batch])\n",
    "    target_values = torch.stack([torch.tensor(item['target_value'], dtype=torch.long) for item in augmented_batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in augmented_batch])\n",
    "    \n",
    "    return {\n",
    "        'input_3d': input_3d,\n",
    "        'target_values': target_values,\n",
    "        'attention_mask': attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:06:22.707256Z",
     "iopub.status.busy": "2025-11-16T01:06:22.706963Z",
     "iopub.status.idle": "2025-11-16T01:06:48.437668Z",
     "shell.execute_reply": "2025-11-16T01:06:48.436497Z",
     "shell.execute_reply.started": "2025-11-16T01:06:22.707225Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Training challenges: 1000\n",
      "Test challenges: 240\n",
      "\n",
      "Training samples (with augmentation): 1000\n",
      "Test samples: 240\n",
      "\n",
      "Sample data:\n",
      "Sample ID: 00576224\n",
      "Challenge ID: 00576224\n",
      "Input sequence length: 5400\n",
      "Target sequence length: 1000\n",
      "Input dims: [(2, 2), (2, 2)]\n",
      "Output dims: [(6, 6), (6, 6)]\n",
      "Test input dims: (2, 2)\n",
      "Test output dims: (6, 6)\n",
      "Creating exploded training dataset...\n",
      "Exploding 1000 base samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:24<00:00, 40.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 204169 exploded samples from 1000 base samples\n"
     ]
    }
   ],
   "source": [
    "PATH='/kaggle/input/arc-prize-2025/'\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = ARCDataset(\n",
    "    challenges_path=PATH+'arc-agi_training_challenges.json',\n",
    "    solutions_path=PATH+'arc-agi_training_solutions.json'\n",
    ")\n",
    "\n",
    "test_dataset = ARCDataset(\n",
    "    challenges_path=PATH+'arc-agi_test_challenges.json'\n",
    ")\n",
    "\n",
    "print(f\"Training challenges: {len(train_dataset.get_all_challenges())}\")\n",
    "print(f\"Test challenges: {len(test_dataset.get_all_challenges())}\")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "tokenizer = ARCTokenizer()\n",
    "token_converter = TokenTo3DConverter(tokenizer)\n",
    "train_torch_dataset = ARCTorchDataset(train_dataset, tokenizer, token_converter=token_converter)\n",
    "test_torch_dataset = ARCTorchDataset(test_dataset, tokenizer, token_converter=token_converter)\n",
    "\n",
    "print(f\"\\nTraining samples (with augmentation): {len(train_torch_dataset)}\")\n",
    "print(f\"Test samples: {len(test_torch_dataset)}\")\n",
    "\n",
    "# Test data loading\n",
    "sample = train_torch_dataset[0]\n",
    "print(f\"\\nSample data:\")\n",
    "print(f\"Sample ID: {sample['sample_id']}\")\n",
    "print(f\"Challenge ID: {sample['challenge_id']}\")\n",
    "print(f\"Input sequence length: {len(sample['input'])}\")\n",
    "print(f\"Target sequence length: {len(sample['target'])}\")\n",
    "print(f\"Input dims: {sample['input_dims']}\")\n",
    "print(f\"Output dims: {sample['output_dims']}\")\n",
    "print(f\"Test input dims: {sample['test_input_dims']}\")\n",
    "print(f\"Test output dims: {sample['test_output_dims']}\")\n",
    "\n",
    "# Create exploded datasets from existing ARCTorchDataset\n",
    "print(\"Creating exploded training dataset...\")\n",
    "train_exploded_dataset = ARCExplodedDataset(train_torch_dataset, tokenizer, sequence_length=5400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:06:48.450948Z",
     "iopub.status.busy": "2025-11-16T01:06:48.450631Z",
     "iopub.status.idle": "2025-11-16T01:06:48.475266Z",
     "shell.execute_reply": "2025-11-16T01:06:48.474225Z",
     "shell.execute_reply.started": "2025-11-16T01:06:48.450919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# augmentation check\n",
    "if AUGMENTATION:\n",
    "    train_dataset_augmented = AugmentedDataset(train_exploded_dataset, apply_probability=1.0)\n",
    "\n",
    "\n",
    "if EVAL_COLOR_DIST:\n",
    "    from collections import Counter, defaultdict\n",
    "    def analyze_color_distribution(dataset):\n",
    "        \"\"\"Analyze the distribution of target colors (0-9) in the dataset\"\"\"\n",
    "        color_counts = Counter()\n",
    "        \n",
    "        print(\"Analyzing color distribution...\")\n",
    "        for idx in tqdm(range(len(dataset))):\n",
    "            sample = dataset[idx]\n",
    "            target_value = sample['target_value']\n",
    "            # Only count color tokens (0-9), ignore special tokens\n",
    "            color_counts[target_value] += 1\n",
    "        \n",
    "        return color_counts\n",
    "    aug_counts = analyze_color_distribution(train_dataset_augmented)\n",
    "    original_counts = analyze_color_distribution(train_exploded_dataset)\n",
    "    print(original_counts)\n",
    "    print(aug_counts) \n",
    "    # result, looks a lot better\n",
    "    # Counter({0: 86966, 8: 19131, 1: 17683, 4: 14558, 7: 13704, 3: 12134, 2: 11829, 5: 7086, 6: 5145, 9: 3042})\n",
    "    # Counter({2: 19417, 0: 19327, 4: 19150, 7: 19150, 8: 19144, 5: 19062, 3: 19055, 9: 19004, 6: 18995, 1: 18974})\n",
    "\n",
    "if AUGMENTATION:\n",
    "    train_exploded_dataset = train_dataset_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:06:48.476965Z",
     "iopub.status.busy": "2025-11-16T01:06:48.476633Z",
     "iopub.status.idle": "2025-11-16T01:07:51.658985Z",
     "shell.execute_reply": "2025-11-16T01:07:51.658099Z",
     "shell.execute_reply.started": "2025-11-16T01:06:48.476936Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total challenges: 1000\n",
      "Train challenges: 800\n",
      "Tiny val challenges: 20\n",
      "Full val challenges: 180\n",
      "\n",
      "Train samples: 161456\n",
      "Tiny val samples: 5083\n",
      "Full val samples: 37630\n",
      "\n",
      "Train batches: 631\n",
      "Tiny val batches: 20\n",
      "Full val batches: 147\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for batching\"\"\"\n",
    "    input_3d = torch.stack([item['input_3d'] for item in batch])\n",
    "    target_values = torch.stack([torch.tensor(item['target_value'], dtype=torch.long) for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'input_3d': input_3d,\n",
    "        'target_values': target_values,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    \n",
    "# Split dataset by challenge_id (ensures no data leakage)\n",
    "# Create TWO validation sets: tiny (frequent) and full (accurate)\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Group samples by challenge_id\n",
    "challenge_to_indices = defaultdict(list)\n",
    "for idx in range(len(train_exploded_dataset)):\n",
    "    sample = train_exploded_dataset[idx]\n",
    "    challenge_id = sample['challenge_id']\n",
    "    challenge_to_indices[challenge_id].append(idx)\n",
    "\n",
    "# Get unique challenge IDs\n",
    "challenge_ids = list(challenge_to_indices.keys())\n",
    "print(f\"Total challenges: {len(challenge_ids)}\")\n",
    "\n",
    "# Shuffle and split challenges (not individual samples)\n",
    "random.seed(42)\n",
    "random.shuffle(challenge_ids)\n",
    "\n",
    "train_ratio = 0.8\n",
    "split_idx = int(len(challenge_ids) * train_ratio)\n",
    "train_challenge_ids = set(challenge_ids[:split_idx])\n",
    "val_challenge_ids_all = set(challenge_ids[split_idx:])\n",
    "\n",
    "# Split validation challenges into tiny and full\n",
    "val_challenge_ids_list = list(val_challenge_ids_all)\n",
    "random.shuffle(val_challenge_ids_list)\n",
    "tiny_val_ratio = 0.1  # 10% of validation challenges for tiny set\n",
    "tiny_split_idx = int(len(val_challenge_ids_list) * tiny_val_ratio)\n",
    "tiny_val_challenge_ids = set(val_challenge_ids_list[:tiny_split_idx])\n",
    "full_val_challenge_ids = set(val_challenge_ids_list[tiny_split_idx:])\n",
    "\n",
    "print(f\"Train challenges: {len(train_challenge_ids)}\")\n",
    "print(f\"Tiny val challenges: {len(tiny_val_challenge_ids)}\")\n",
    "print(f\"Full val challenges: {len(full_val_challenge_ids)}\")\n",
    "\n",
    "# Collect indices for each split\n",
    "train_indices = []\n",
    "tiny_val_indices = []\n",
    "full_val_indices = []\n",
    "\n",
    "for challenge_id, indices in challenge_to_indices.items():\n",
    "    if challenge_id in train_challenge_ids:\n",
    "        train_indices.extend(indices)\n",
    "    elif challenge_id in tiny_val_challenge_ids:\n",
    "        tiny_val_indices.extend(indices)\n",
    "    elif challenge_id in full_val_challenge_ids:\n",
    "        full_val_indices.extend(indices)\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_indices)}\")\n",
    "print(f\"Tiny val samples: {len(tiny_val_indices)}\")\n",
    "print(f\"Full val samples: {len(full_val_indices)}\")\n",
    "\n",
    "# Create subset datasets\n",
    "train_dataset_split = Subset(train_exploded_dataset, train_indices)\n",
    "tiny_val_dataset_split = Subset(train_exploded_dataset, tiny_val_indices)\n",
    "full_val_dataset_split = Subset(train_exploded_dataset, full_val_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_split,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Tiny validation loader (for frequent checks)\n",
    "tiny_val_loader = DataLoader(\n",
    "    tiny_val_dataset_split,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "# Full validation loader (for accurate metrics)\n",
    "full_val_loader = DataLoader(\n",
    "    full_val_dataset_split,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Tiny val batches: {len(tiny_val_loader)}\")\n",
    "print(f\"Full val batches: {len(full_val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enriched attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseEnhancedAttentionLayerTopK(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom transformer layer that:\n",
    "    1. Computes full attention matrix Q @ K^T\n",
    "    2. Extracts upper triangular (without diagonal, i < j)\n",
    "    3. Applies ReLU to sparsify (only keep positive values)\n",
    "    4. Selects top seq_len pairs by attention score (always exactly seq_len pairs per batch)\n",
    "    5. Enriches non-zero attention scores with original vectors [attn_score, vec_i, vec_j]\n",
    "    6. Bottlenecks enhanced vectors back to sequence length\n",
    "    \n",
    "    This bypasses the V network entirely and learns direct transformations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, dim_feedforward=2048, dropout=0.1, \n",
    "                 bottleneck_hidden=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            dim_feedforward: Feedforward network hidden dimension\n",
    "            dropout: Dropout rate\n",
    "            bottleneck_hidden: Hidden dimension for bottleneck MLP (defaults to dim_feedforward)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Query and Key projections (no V projection needed!)\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Bottleneck MLP: processes [attn_score, vec_i, vec_j] -> d_model\n",
    "        # Input: [attn_score (1) + vec_i (d_model) + vec_j (d_model)] = d_model*2 + 1\n",
    "        bottleneck_hidden = bottleneck_hidden or dim_feedforward\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(d_model * 2 + 1, bottleneck_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bottleneck_hidden, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Feedforward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Layer norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "            src_key_padding_mask: [batch_size, seq_len] - True for padding tokens\n",
    "        \n",
    "        Returns:\n",
    "            [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Compute Q, K (no V needed!)\n",
    "        Q = self.w_q(x)  # [batch_size, seq_len, d_model]\n",
    "        K = self.w_k(x)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "        # [batch, seq_len, seq_len]\n",
    "        \n",
    "        # Apply padding mask if provided\n",
    "        if src_key_padding_mask is not None:\n",
    "            mask = src_key_padding_mask.unsqueeze(1)  # [batch, 1, seq_len]\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "            mask = src_key_padding_mask.unsqueeze(2)  # [batch, seq_len, 1]\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        # Extract upper triangular WITHOUT diagonal (strictly upper: i < j)\n",
    "        triu_indices = torch.triu_indices(seq_len, seq_len, offset=1, device=x.device)\n",
    "        i_indices = triu_indices[0]  # [num_pairs]\n",
    "        j_indices = triu_indices[1]  # [num_pairs]\n",
    "        \n",
    "        # Extract upper triangular attention scores\n",
    "        upper_tri_scores = scores[:, i_indices, j_indices]  # [batch, num_pairs]\n",
    "        \n",
    "        # Apply ReLU to sparsify (set negatives to 0)\n",
    "        sparse_scores = torch.relu(upper_tri_scores)  # [batch, num_pairs]\n",
    "        \n",
    "        # Process each batch separately\n",
    "        all_processed = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            # Get this batch's sparse scores\n",
    "            batch_sparse_scores = sparse_scores[b]  # [num_pairs]\n",
    "            \n",
    "            # Find non-zero pairs for this batch\n",
    "            batch_non_zero_mask = batch_sparse_scores > 0  # [num_pairs]\n",
    "            num_non_zero = batch_non_zero_mask.sum().item()\n",
    "            \n",
    "            if num_non_zero == 0:\n",
    "                # No non-zero pairs - create zero vectors\n",
    "                batch_processed = torch.zeros(seq_len, self.d_model, device=x.device)\n",
    "            else:\n",
    "                # Get non-zero scores and their pair indices\n",
    "                non_zero_scores_b = batch_sparse_scores[batch_non_zero_mask]  # [num_non_zero]\n",
    "                non_zero_pair_indices = torch.where(batch_non_zero_mask)[0]  # [num_non_zero]\n",
    "                \n",
    "                # Select exactly seq_len pairs (top-k)\n",
    "                if num_non_zero >= seq_len:\n",
    "                    # Select top seq_len pairs\n",
    "                    topk_scores, topk_local_indices = torch.topk(non_zero_scores_b, k=seq_len)\n",
    "                    selected_pair_indices = non_zero_pair_indices[topk_local_indices]\n",
    "                    selected_scores = topk_scores\n",
    "                else:\n",
    "                    # Fewer than seq_len pairs available - use all and pad with zeros\n",
    "                    selected_pair_indices = non_zero_pair_indices\n",
    "                    selected_scores = non_zero_scores_b\n",
    "                    \n",
    "                    # Pad to seq_len\n",
    "                    num_to_pad = seq_len - num_non_zero\n",
    "                    # Use first available pairs as placeholders (will have zero scores)\n",
    "                    pad_indices = torch.arange(num_to_pad, device=x.device) % len(non_zero_pair_indices)\n",
    "                    pad_pair_indices = non_zero_pair_indices[pad_indices]\n",
    "                    selected_pair_indices = torch.cat([selected_pair_indices, pad_pair_indices], dim=0)\n",
    "                    selected_scores = torch.cat([\n",
    "                        selected_scores,\n",
    "                        torch.zeros(num_to_pad, device=x.device)\n",
    "                    ], dim=0)\n",
    "                \n",
    "                # Get sequence indices for selected pairs\n",
    "                selected_i = i_indices[selected_pair_indices]  # [seq_len]\n",
    "                selected_j = j_indices[selected_pair_indices]  # [seq_len]\n",
    "                \n",
    "                # Gather original vectors for this batch's pairs\n",
    "                vec_i = x[b, selected_i, :]  # [seq_len, d_model]\n",
    "                vec_j = x[b, selected_j, :]  # [seq_len, d_model]\n",
    "                \n",
    "                # Create enhanced vectors for this batch\n",
    "                enhanced = torch.cat([\n",
    "                    selected_scores.unsqueeze(-1),  # [seq_len, 1]\n",
    "                    vec_i,  # [seq_len, d_model]\n",
    "                    vec_j   # [seq_len, d_model]\n",
    "                ], dim=-1)  # [seq_len, d_model*2 + 1]\n",
    "                \n",
    "                # Process through bottleneck (per batch)\n",
    "                batch_processed = self.bottleneck(enhanced)  # [seq_len, d_model]\n",
    "            \n",
    "            all_processed.append(batch_processed)\n",
    "        \n",
    "        # Stack batches\n",
    "        attn_output = torch.stack(all_processed, dim=0)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Output projection\n",
    "        attn_output = self.w_o(attn_output)\n",
    "        x = self.dropout(attn_output)  # No residual connection\n",
    "        \n",
    "        # Feedforward\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = residual + self.ffn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Enhanced Attention Layer\n",
    "class SparseEnhancedAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom transformer layer that:\n",
    "    1. Computes full attention matrix Q @ K^T\n",
    "    2. Extracts upper triangular (without diagonal, i < j)\n",
    "    3. Applies ReLU to sparsify (only keep positive values)\n",
    "    4. Enriches non-zero attention scores with original vectors [attn_score, vec_i, vec_j]\n",
    "    5. Bottlenecks enhanced vectors back to sequence length\n",
    "    \n",
    "    This bypasses the V network entirely and learns direct transformations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, dim_feedforward=2048, dropout=0.1, \n",
    "                 bottleneck_hidden=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Query and Key projections (no V projection needed!)\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Bottleneck MLP: processes [attn_score, vec_i, vec_j] -> d_model\n",
    "        # Input: [attn_score (1) + vec_i (d_model) + vec_j (d_model)] = d_model*2 + 1\n",
    "        bottleneck_hidden = bottleneck_hidden or dim_feedforward\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Linear(d_model * 2 + 1, bottleneck_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bottleneck_hidden, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Feedforward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Layer norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "            src_key_padding_mask: [batch_size, seq_len] - True for padding tokens\n",
    "        \n",
    "        Returns:\n",
    "            [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Compute Q, K (no V needed!)\n",
    "        Q = self.w_q(x)  # [batch_size, seq_len, d_model]\n",
    "        K = self.w_k(x)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model)\n",
    "        # [batch, seq_len, seq_len]\n",
    "        \n",
    "        # Apply padding mask if provided\n",
    "        if src_key_padding_mask is not None:\n",
    "            mask = src_key_padding_mask.unsqueeze(1)  # [batch, 1, seq_len]\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "            mask = src_key_padding_mask.unsqueeze(2)  # [batch, seq_len, 1]\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        # Extract upper triangular WITHOUT diagonal (strictly upper: i < j)\n",
    "        # Get indices for upper triangular (excluding diagonal)\n",
    "        triu_indices = torch.triu_indices(seq_len, seq_len, offset=1, device=x.device)\n",
    "        i_indices = triu_indices[0]  # [num_pairs]\n",
    "        j_indices = triu_indices[1]  # [num_pairs]\n",
    "        \n",
    "        # Extract upper triangular attention scores\n",
    "        upper_tri_scores = scores[:, i_indices, j_indices]  # [batch, num_pairs]\n",
    "        \n",
    "        # Apply ReLU to sparsify (set negatives to 0)\n",
    "        sparse_scores = torch.relu(upper_tri_scores)  # [batch, num_pairs]\n",
    "        \n",
    "        # Find non-zero pairs (sparse attention)\n",
    "        # For each batch, find which pairs have non-zero attention\n",
    "        non_zero_mask = sparse_scores > 0  # [batch, num_pairs]\n",
    "        \n",
    "                # Vectorized: process all non-zero pairs across all batches at once\n",
    "        # Get total number of non-zero pairs across all batches\n",
    "        total_non_zeros = non_zero_mask.sum().item()\n",
    "        \n",
    "        if total_non_zeros == 0:\n",
    "            # No non-zero attention in any batch, output zeros\n",
    "            attn_output = torch.zeros(batch_size, seq_len, self.d_model, device=x.device)\n",
    "        else:\n",
    "            # Get batch indices and pair indices for all non-zero pairs\n",
    "            batch_indices, pair_indices = torch.where(non_zero_mask)\n",
    "            # batch_indices: [total_non_zeros] - which batch each pair belongs to\n",
    "            # pair_indices: [total_non_zeros] - which pair index (in i_indices/j_indices)\n",
    "            \n",
    "            # Get the actual sequence indices for these pairs\n",
    "            non_zero_i = i_indices[pair_indices]  # [total_non_zeros]\n",
    "            non_zero_j = j_indices[pair_indices]  # [total_non_zeros]\n",
    "            \n",
    "            # Get attention scores for non-zero pairs\n",
    "            non_zero_scores = sparse_scores[batch_indices, pair_indices]  # [total_non_zeros]\n",
    "            \n",
    "            # Gather original vectors for all non-zero pairs (fully vectorized)\n",
    "            vec_i = x[batch_indices, non_zero_i, :]  # [total_non_zeros, d_model]\n",
    "            vec_j = x[batch_indices, non_zero_j, :]  # [total_non_zeros, d_model]\n",
    "            \n",
    "            # Create enhanced vectors: [attn_score, vec_i, vec_j]\n",
    "            enhanced = torch.cat([\n",
    "                non_zero_scores.unsqueeze(-1),  # [total_non_zeros, 1]\n",
    "                vec_i,  # [total_non_zeros, d_model]\n",
    "                vec_j   # [total_non_zeros, d_model]\n",
    "            ], dim=-1)  # [total_non_zeros, d_model*2 + 1]\n",
    "            \n",
    "            # Process through bottleneck (all at once, fully vectorized)\n",
    "            processed = self.bottleneck(enhanced)  # [total_non_zeros, d_model]\n",
    "            \n",
    "            # Aggregate back to sequence length using index_add_ (vectorized per batch)\n",
    "            # Initialize output tensor\n",
    "            attn_output = torch.zeros(batch_size, seq_len, self.d_model, device=x.device)\n",
    "            \n",
    "            # Use index_add_ for each batch (still need batch loop for indexing, but bottleneck is vectorized)\n",
    "            for b in range(batch_size):\n",
    "                batch_mask = batch_indices == b\n",
    "                if batch_mask.any():\n",
    "                    batch_non_zero_i = non_zero_i[batch_mask]\n",
    "                    batch_non_zero_j = non_zero_j[batch_mask]\n",
    "                    batch_processed = processed[batch_mask]\n",
    "                    \n",
    "                    # Sum contributions where position appears as i\n",
    "                    attn_output[b].index_add_(0, batch_non_zero_i, batch_processed)\n",
    "                    # Also sum contributions where position appears as j\n",
    "                    attn_output[b].index_add_(0, batch_non_zero_j, batch_processed)\n",
    "        \n",
    "        # Stack batch outputs\n",
    "        attn_output = torch.stack(batch_outputs, dim=0)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        # Output projection\n",
    "        attn_output = self.w_o(attn_output)\n",
    "        #x = residual + self.dropout(attn_output)\n",
    "        \n",
    "        # Feedforward\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = residual + self.ffn(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:07:51.692946Z",
     "iopub.status.busy": "2025-11-16T01:07:51.692576Z",
     "iopub.status.idle": "2025-11-16T01:07:51.726080Z",
     "shell.execute_reply": "2025-11-16T01:07:51.724864Z",
     "shell.execute_reply.started": "2025-11-16T01:07:51.692902Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom transformer encoder using CNN-filtered attention layers\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "            src_key_padding_mask: [batch_size, seq_len] - True for padding tokens\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_key_padding_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:09:10.142843Z",
     "iopub.status.busy": "2025-11-16T01:09:10.142518Z",
     "iopub.status.idle": "2025-11-16T01:09:22.027816Z",
     "shell.execute_reply": "2025-11-16T01:09:22.026945Z",
     "shell.execute_reply.started": "2025-11-16T01:09:10.142819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatthiaskargl\u001b[0m (\u001b[33mmatthiaskargl-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='9c6d131f5fcedb96565fa31f4680c2da83ea07d5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:09:22.030104Z",
     "iopub.status.busy": "2025-11-16T01:09:22.029518Z",
     "iopub.status.idle": "2025-11-16T01:09:22.042496Z",
     "shell.execute_reply": "2025-11-16T01:09:22.041383Z",
     "shell.execute_reply.started": "2025-11-16T01:09:22.030082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NextTokenPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model to predict next token from 3D vectors [value, x, y].\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=18, d_model=16, nhead=8, num_layers=4, \n",
    "                 dim_feedforward=1024, max_seq_length=5400, dropout=0.1, unet_base_channels=32, unet_num_downsample=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        # Embedding for token values (0-17)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Projection for x, y coordinates (add coordinate information)\n",
    "        self.coord_projection = nn.Linear(2, d_model)  # [x, y] -> d_model\n",
    "        \n",
    "        # Positional encoding (learned)\n",
    "        #self.pos_encoding = nn.Parameter(torch.randn(max_seq_length, d_model) * 0.02)\n",
    "        #self.pos_encoding = self.create_sinusoidal_positional_encoding(max_seq_length, d_model)\n",
    "        pos_encoding = self.create_sinusoidal_positional_encoding(max_seq_length, d_model)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "        \n",
    "        \n",
    "        USE_CUSTOM_ATTENTION = True  # Set to False for standard transformer\n",
    "        \n",
    "        if USE_CUSTOM_ATTENTION:\n",
    "            # Custom sparse enhanced attention layer\n",
    "            encoder_layer = SparseEnhancedAttentionLayerTopK(\n",
    "                d_model=d_model,\n",
    "                max_pairs_per_batch=seq_len,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                #dropout=dropout,\n",
    "            )\n",
    "            self.transformer = CustomTransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        else:\n",
    "            # Standard transformer encoder\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output projection to vocab\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def create_sinusoidal_positional_encoding(self, max_len, d_model):\n",
    "        \"\"\"\n",
    "        Create sinusoidal positional encoding (no learnable parameters).\n",
    "        \n",
    "        Args:\n",
    "            max_len: Maximum sequence length\n",
    "            d_model: Model dimension\n",
    "        \n",
    "        Returns:\n",
    "            [max_len, d_model] tensor with positional encodings\n",
    "        \"\"\"\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices: sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices: cos\n",
    "        \n",
    "        return pe  # [max_len, d_model]\n",
    "\n",
    "        \n",
    "    def forward(self, input_3d, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_3d: [batch_size, seq_len, 3] - [value, x, y] vectors\n",
    "            attention_mask: [batch_size, seq_len] - 1 for real tokens, 0 for padding\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, seq_len, vocab_size] - logits for each position\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = input_3d.shape\n",
    "        \n",
    "        # Extract components\n",
    "        token_values = input_3d[:, :, 0].long()  # [batch_size, seq_len] - token values\n",
    "        coordinates = input_3d[:, :, 1:3].float()  # [batch_size, seq_len, 2] - x, y\n",
    "        \n",
    "        # Embed tokens\n",
    "        token_emb = self.token_embedding(token_values)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Add coordinate information\n",
    "        coord_emb = self.coord_projection(coordinates)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Combine token and coordinate embeddings\n",
    "        x = token_emb + coord_emb  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_encoding[:seq_len].unsqueeze(0)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        #x = self.dropout(x)\n",
    "        if attention_mask is not None:\n",
    "            padding_mask = (attention_mask == 0).bool()  # True for padding, False for real tokens\n",
    "        else:\n",
    "            padding_mask = None\n",
    "        \n",
    "        # Apply transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=padding_mask)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Get logits for all positions\n",
    "        logits = self.output_proj(x)  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:09:22.043861Z",
     "iopub.status.busy": "2025-11-16T01:09:22.043596Z",
     "iopub.status.idle": "2025-11-16T01:09:22.096620Z",
     "shell.execute_reply": "2025-11-16T01:09:22.095635Z",
     "shell.execute_reply.started": "2025-11-16T01:09:22.043834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with 1,933,876 parameters\n",
      "Model device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = NextTokenPredictor(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=16,\n",
    "    nhead=1,\n",
    "    num_layers=1,\n",
    "    dim_feedforward=128,\n",
    "    max_seq_length=5400,\n",
    "    dropout=0.1,\n",
    "    unet_base_channels=2,\n",
    "    unet_num_downsample=2,\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T01:09:22.098896Z",
     "iopub.status.busy": "2025-11-16T01:09:22.098561Z",
     "iopub.status.idle": "2025-11-16T01:09:22.119346Z",
     "shell.execute_reply": "2025-11-16T01:09:22.118393Z",
     "shell.execute_reply.started": "2025-11-16T01:09:22.098872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "import os\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, batch_idx, val_loss, val_acc, train_loss, train_acc, \n",
    "                   checkpoint_dir='checkpoints', is_best=False):\n",
    "    \"\"\"\n",
    "    Save model checkpoint\n",
    "    \n",
    "    Args:\n",
    "        model: The model to save\n",
    "        optimizer: The optimizer to save\n",
    "        epoch: Current epoch number\n",
    "        batch_idx: Current batch index\n",
    "        val_loss: Validation loss\n",
    "        val_acc: Validation accuracy\n",
    "        train_loss: Training loss\n",
    "        train_acc: Training accuracy\n",
    "        checkpoint_dir: Directory to save checkpoints\n",
    "        is_best: Whether this is the best model so far\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'batch': batch_idx,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_acc,\n",
    "        'train_loss': train_loss,\n",
    "        'train_accuracy': train_acc,\n",
    "    }\n",
    "    \n",
    "    # Save regular checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch{epoch}_batch{batch_idx}.pt')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    # Save best model if applicable\n",
    "    if is_best:\n",
    "        best_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "        torch.save(checkpoint, best_path)\n",
    "        print(f\"  ✓ Best model saved: {best_path}\")\n",
    "    \n",
    "    return checkpoint_path\n",
    "    \n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model on dataset\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    last_logits = None  # Initialize\n",
    "    last_targets = None  # Initialize\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_3d = batch['input_3d'].to(device)\n",
    "            target_values = batch['target_values'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            logits = model(input_3d=input_3d, attention_mask=attention_mask)\n",
    "            \n",
    "            batch_size = input_3d.size(0)\n",
    "            seq_lengths = attention_mask.sum(dim=1) - 1\n",
    "            last_logits = logits[torch.arange(batch_size), seq_lengths]\n",
    "            last_targets = target_values\n",
    "            \n",
    "            loss = criterion(last_logits, target_values)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions = last_logits.argmax(dim=1)\n",
    "            correct += (predictions == target_values).sum().item()\n",
    "            total += target_values.size(0)\n",
    "    if last_logits is not None and last_targets is not None:\n",
    "        print(f\"Sample predictions: {last_logits[0:3].argmax(dim=1)}, targets: {last_targets[0:3]}\")\n",
    "    else:\n",
    "        print(\"No predictions or targets available\")\n",
    "    \n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Updated train_epoch with periodic validation during training\n",
    "def train_epoch(model, train_dataloader, tiny_val_loader, full_val_loader, criterion, optimizer, device, \n",
    "                log_every_n_batches=2, tiny_val_every_n_batches=10, full_val_every_n_batches=200):\n",
    "    \"\"\"\n",
    "    Train for one epoch with periodic validation using two validation sets\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        train_dataloader: Training data loader\n",
    "        tiny_val_loader: Tiny validation loader (for frequent checks)\n",
    "        full_val_loader: Full validation loader (for accurate metrics)\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: Device to run on\n",
    "        log_every_n_batches: Log to wandb every N batches\n",
    "        tiny_val_every_n_batches: Run tiny validation every N batches (default: 10)\n",
    "        full_val_every_n_batches: Run full validation every N batches (default: 200)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    best_val_los=1e6\n",
    "    \n",
    "    pbar = tqdm(train_dataloader, desc=\"Training\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move to device\n",
    "        input_3d = batch['input_3d'].to(device)  # [batch_size, seq_len, 3]\n",
    "        target_values = batch['target_values'].to(device)  # [batch_size]\n",
    "        attention_mask = batch['attention_mask'].to(device)  # [batch_size, seq_len]\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_3d=input_3d, attention_mask=attention_mask)  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        # Get logits for the last non-padding position (where we predict)\n",
    "        # Find last non-padding position for each sequence\n",
    "        batch_size = input_3d.size(0)\n",
    "        seq_lengths = attention_mask.sum(dim=1) - 1  # -1 because we want the position before the target\n",
    "        last_logits = logits[torch.arange(batch_size), seq_lengths]  # [batch_size, vocab_size]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(last_logits, target_values)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions = last_logits.argmax(dim=1)\n",
    "        batch_correct = (predictions == target_values).sum().item()\n",
    "        correct += batch_correct\n",
    "        total += target_values.size(0)\n",
    "        batch_acc = 100 * batch_correct / target_values.size(0)\n",
    "        \n",
    "        # Log to wandb every N batches (default: every other batch)\n",
    "        if batch_idx % log_every_n_batches == 0:\n",
    "            wandb.log({\n",
    "                \"batch_loss\": loss.item(),\n",
    "                \"batch_accuracy\": batch_acc,\n",
    "                \"running_accuracy\": 100 * correct / total,\n",
    "            })\n",
    "        \n",
    "        # Tiny validation (frequent, quick check)\n",
    "        if (batch_idx + 1) % tiny_val_every_n_batches == 0:\n",
    "            tiny_val_loss, tiny_val_acc = evaluate(model, tiny_val_loader, criterion, device)\n",
    "            \n",
    "            # Log tiny validation metrics\n",
    "            wandb.log({\n",
    "                \"tiny_val_loss\": tiny_val_loss,\n",
    "                \"tiny_val_accuracy\": tiny_val_acc,\n",
    "                \"train_batch\": batch_idx + 1,\n",
    "            })\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100 * correct / total:.2f}%',\n",
    "                'tiny_val': f'{tiny_val_acc:.1f}%'\n",
    "            })\n",
    "            \n",
    "            model.train()  # Switch back to training mode\n",
    "        \n",
    "        # Full validation (less frequent, more accurate)\n",
    "        if (batch_idx + 1) % full_val_every_n_batches == 0:\n",
    "            full_val_loss, full_val_acc = evaluate(model, full_val_loader, criterion, device)\n",
    "            is_better=full_val_loss<best_val_los\n",
    "            if is_better:\n",
    "                best_val_los=full_val_loss\n",
    "\n",
    "            save_checkpoint(model, optimizer, epoch, batch_idx, full_val_loss, full_val_acc, total_loss, batch_acc, \n",
    "                   checkpoint_dir='checkpoints', is_best=is_better)\n",
    "            \n",
    "            # Log full validation metrics\n",
    "            wandb.log({\n",
    "                \"full_val_loss\": full_val_loss,\n",
    "                \"full_val_accuracy\": full_val_acc,\n",
    "                \"train_batch\": batch_idx + 1,\n",
    "            })\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100 * correct / total:.2f}%',\n",
    "                'full_val': f'{full_val_acc:.1f}%'\n",
    "            })\n",
    "            \n",
    "            model.train()  # Switch back to training mode\n",
    "        \n",
    "        # Update progress bar (if no validation was run this batch)\n",
    "        if (batch_idx + 1) % tiny_val_every_n_batches != 0 and (batch_idx + 1) % full_val_every_n_batches != 0:\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100 * correct / total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-16T01:10:45.903Z",
     "iopub.execute_input": "2025-11-16T01:09:56.159365Z",
     "iopub.status.busy": "2025-11-16T01:09:56.158363Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 1 epochs...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20251116_010956-q6i36xaz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matthiaskargl-personal/arc-next-token-prediction/runs/q6i36xaz' target=\"_blank\">test-aug-colors-unet_try0</a></strong> to <a href='https://wandb.ai/matthiaskargl-personal/arc-next-token-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthiaskargl-personal/arc-next-token-prediction' target=\"_blank\">https://wandb.ai/matthiaskargl-personal/arc-next-token-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthiaskargl-personal/arc-next-token-prediction/runs/q6i36xaz' target=\"_blank\">https://wandb.ai/matthiaskargl-personal/arc-next-token-prediction/runs/q6i36xaz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/631 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Updated training loop with validation and wandb logging\n",
    "num_epochs = 1\n",
    "print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# Validation frequencies\n",
    "tiny_val_every_n_batches = 1   # Tiny validation every 10 batches (~30 seconds)\n",
    "full_val_every_n_batches = 5  # Full validation every 200 batches (~5-10 minutes)\n",
    "\n",
    "wandb.init(\n",
    "    name='test-aug-colors-unet_try0',\n",
    "    project=\"arc-next-token-prediction\",\n",
    "    config={\n",
    "        \"vocab_size\": tokenizer.vocab_size,\n",
    "        \"d_model\": model.d_model,\n",
    "        #\"nhead\": model.transformer.layers[0].self_attn.num_heads,\n",
    "        \"num_layers\": len(model.transformer.layers),\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "        \"max_seq_length\": model.max_seq_length,\n",
    "    }\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, \n",
    "        train_loader, \n",
    "        tiny_val_loader,  # Tiny validation for frequent checks\n",
    "        full_val_loader,  # Full validation for accurate metrics\n",
    "        criterion, \n",
    "        optimizer, \n",
    "        device,\n",
    "        tiny_val_every_n_batches=tiny_val_every_n_batches,\n",
    "        full_val_every_n_batches=full_val_every_n_batches\n",
    "    )\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_accuracy\": val_acc,\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "    print(f\"  Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"  Val   - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
    "    print(f\"  Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Track best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        print(f\"  ✓ New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        # Optionally save model checkpoint\n",
    "        # torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
