{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Tuple, Optional\nimport math\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nAUGMENTATION=True\nEVAL_COLOR_DIST=False\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:05:37.273003Z","iopub.execute_input":"2025-11-16T01:05:37.273319Z","iopub.status.idle":"2025-11-16T01:05:40.679506Z","shell.execute_reply.started":"2025-11-16T01:05:37.273293Z","shell.execute_reply":"2025-11-16T01:05:40.678507Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# helper classes","metadata":{}},{"cell_type":"code","source":"class ARCTokenizer:\n    \"\"\"Tokenizer for ARC challenges with special tokens for structure\"\"\"\n    \n    def __init__(self):\n        # Value tokens (0-9)\n        self.value_tokens = list(range(10))\n        \n        # Special tokens\n        self.PAD_TOKEN = 10\n        self.SOS_TOKEN = 11  # Start of sequence\n        self.EOS_TOKEN = 12  # End of sequence\n        self.TRAIN_TOKEN = 13  # Start of training example\n        self.TEST_TOKEN = 14  # Start of test example\n        self.INPUT_TOKEN = 15  # Start of input grid\n        self.OUTPUT_TOKEN = 16  # Start of output grid\n        self.NEWLINE_TOKEN = 17  # Grid separator (], [)\n        \n        self.vocab_size = 18\n        \n        # Token mappings\n        self.token_to_id = {\n            'PAD': self.PAD_TOKEN,\n            'SOS': self.SOS_TOKEN,\n            'EOS': self.EOS_TOKEN,\n            'TRAIN': self.TRAIN_TOKEN,\n            'TEST': self.TEST_TOKEN,\n            'INPUT': self.INPUT_TOKEN,\n            'OUTPUT': self.OUTPUT_TOKEN,\n            'NEWLINE': self.NEWLINE_TOKEN\n        }\n    \n    def grid_to_tokens(self, grid: List[List[int]]) -> List[int]:\n        \"\"\"Convert 2D grid to token sequence\"\"\"\n        if not grid or not grid[0]:\n            return []\n        \n        tokens = []\n        for i, row in enumerate(grid):\n            for j, value in enumerate(row):\n                tokens.append(value)  # Just the value, position will be encoded separately\n            if i < len(grid) - 1:  # Add newline between rows (except last)\n                tokens.append(self.NEWLINE_TOKEN)\n        \n        return tokens\n    \n    def tokens_to_grid(self, tokens: List[int], target_shape: Tuple[int, int]) -> List[List[int]]:\n        \"\"\"Convert token sequence back to 2D grid\"\"\"\n        h, w = target_shape\n        grid = [[0 for _ in range(w)] for _ in range(h)]\n        \n        # Filter out special tokens and newlines\n        values = [t for t in tokens if t < 10]  # Only keep value tokens (0-9)\n        \n        idx = 0\n        for i in range(h):\n            for j in range(w):\n                if idx < len(values):\n                    grid[i][j] = values[idx]\n                    idx += 1\n        \n        return grid\n    \n    def create_input_sequence(self, train_examples: List[Dict], test_input: List[List[int]]) -> List[int]:\n        \"\"\"Create input sequence from training examples and test input\"\"\"\n        sequence = [self.SOS_TOKEN]\n        \n        # Add training examples (exactly 2)\n        for i, example in enumerate(train_examples[:2]):\n            sequence.append(self.TRAIN_TOKEN)\n            \n            # Add input\n            sequence.append(self.INPUT_TOKEN)\n            input_tokens = self.grid_to_tokens(example['input'])\n            sequence.extend(input_tokens)\n            \n            # Add output\n            sequence.append(self.OUTPUT_TOKEN)\n            output_tokens = self.grid_to_tokens(example['output'])\n            sequence.extend(output_tokens)\n        \n        # Add test input\n        sequence.append(self.TEST_TOKEN)\n        sequence.append(self.INPUT_TOKEN)\n        test_tokens = self.grid_to_tokens(test_input)\n        sequence.extend(test_tokens)\n        \n        return sequence\n    \n    def create_target_sequence(self, target_grid: List[List[int]]) -> List[int]:\n        \"\"\"Create target sequence for training\"\"\"\n        sequence = [self.SOS_TOKEN]\n        sequence.append(self.OUTPUT_TOKEN)\n        target_tokens = self.grid_to_tokens(target_grid)\n        sequence.extend(target_tokens)\n        sequence.append(self.EOS_TOKEN)\n        return sequence\n    \n    def pad_sequence(self, sequence: List[int], max_length: int) -> List[int]:\n        \"\"\"Pad sequence to max_length\"\"\"\n        if len(sequence) > max_length:\n            return sequence[:max_length]\n        return sequence + [self.PAD_TOKEN] * (max_length - len(sequence))\n\n\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-16T01:05:43.592715Z","iopub.execute_input":"2025-11-16T01:05:43.593585Z","iopub.status.idle":"2025-11-16T01:05:43.606280Z","shell.execute_reply.started":"2025-11-16T01:05:43.593557Z","shell.execute_reply":"2025-11-16T01:05:43.605547Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class TokenTo3DConverter:\n    \"\"\"Converts token sequences to 3D vectors [value, x, y] with coordinate information\"\"\"\n    \n    def __init__(self, tokenizer: ARCTokenizer):\n        self.tokenizer = tokenizer\n    \n    def tokens_to_3d(self, \n                     tokens: List[int],\n                     input_dims: List[Tuple[int, int]],\n                     output_dims: List[Tuple[int, int]],\n                     test_input_dims: Tuple[int, int],\n                     test_output_dims: Optional[Tuple[int, int]] = None,\n                     is_target: bool = False) -> torch.Tensor:\n        \"\"\"\n        Convert token sequence to 3D vectors [value, x, y]\n        \n        Args:\n            tokens: List of token IDs\n            input_dims: List of (height, width) for training input grids\n            output_dims: List of (height, width) for training output grids\n            test_input_dims: (height, width) for test input grid\n            is_target: If True, this is a target sequence (starts with OUTPUT_TOKEN)\n        \n        Returns:\n            Tensor of shape [seq_len, 3] where each row is [value, x, y]\n            Special tokens have x=-1, y=-1\n        \"\"\"\n        result = []\n        \n        # Track current grid context\n        current_grid_type = None  # 'train_input', 'train_output', 'test_input'\n        current_grid_idx = 0\n        current_row = 0\n        current_col = 0\n        current_grid_dims = None\n        \n        i = 0\n        while i < len(tokens):\n            token = tokens[i]\n            \n            # Handle special tokens that change context\n            if token == self.tokenizer.SOS_TOKEN:\n                result.append([token, -1, -1])\n                i += 1\n                continue\n            elif token == self.tokenizer.EOS_TOKEN:\n                result.append([token, -1, -1])\n                i += 1\n                continue\n            elif token == self.tokenizer.PAD_TOKEN:\n                result.append([token, -1, -1])\n                i += 1\n                continue\n            elif token == self.tokenizer.TRAIN_TOKEN:\n                current_grid_type = None\n                current_grid_idx = 0\n                result.append([token, -1, -1])\n                i += 1\n                continue\n            elif token == self.tokenizer.TEST_TOKEN:\n                current_grid_type = None\n                result.append([token, -1, -1])\n                i += 1\n                continue\n            elif token == self.tokenizer.INPUT_TOKEN:\n                # Determine which input grid we're in\n                if is_target:\n                    # In target sequence, INPUT_TOKEN shouldn't appear\n                    result.append([token, -1, -1])\n                    i += 1\n                    continue\n                \n                if current_grid_type is None:\n                    # First INPUT after TRAIN - this is training input\n                    if current_grid_idx < len(input_dims):\n                        current_grid_dims = input_dims[current_grid_idx]\n                        current_grid_type = 'train_input'\n                elif current_grid_type == 'train_output':\n                    # INPUT after OUTPUT in training - next training example\n                    current_grid_idx += 1\n                    if current_grid_idx < len(input_dims):\n                        current_grid_dims = input_dims[current_grid_idx]\n                        current_grid_type = 'train_input'\n                elif current_grid_type is None:\n                    # INPUT after TEST - this is test input\n                    current_grid_dims = test_input_dims\n                    current_grid_type = 'test_input'\n                \n                current_row = 0\n                current_col = 0\n                result.append([token, -1, -1])\n                i += 1\n                continue\n            elif token == self.tokenizer.OUTPUT_TOKEN:\n                # Determine which output grid we're in\n                if current_grid_type == 'train_input':\n                    # OUTPUT after INPUT in training\n                    if current_grid_idx < len(output_dims):\n                        current_grid_dims = output_dims[current_grid_idx]\n                        current_grid_type = 'train_output'\n                elif current_grid_type is None:\n                    # OUTPUT at start (for target sequence) or after TEST\n                    if is_target:\n                        # For target sequence, use test_output_dims if available\n                        if test_output_dims is not None:\n                            current_grid_dims = test_output_dims\n                        elif len(output_dims) > 0:\n                            current_grid_dims = output_dims[0]  # Fallback to first output dims\n                        else:\n                            current_grid_dims = (1, 1)  # Default fallback\n                    elif len(output_dims) > 0:\n                        current_grid_dims = output_dims[0]\n                    current_grid_type = 'train_output'\n                \n                current_row = 0\n                current_col = 0\n                result.append([token, -1, -1])\n                i += 1\n                continue\n            elif token == self.tokenizer.NEWLINE_TOKEN:\n                # Move to next row\n                if current_grid_dims is not None:\n                    current_row += 1\n                    current_col = 0\n                result.append([token, -1, -1])\n                i += 1\n                continue\n            elif token < 10:  # Value token (0-9)\n                # This is a grid value - add coordinates\n                if current_grid_dims is not None:\n                    h, w = current_grid_dims\n                    # Clamp to valid ranges\n                    row = min(current_row, h - 1)\n                    col = min(current_col, w - 1)\n                    result.append([token, col, row])  # x=col, y=row\n                    \n                    # Move to next column\n                    current_col += 1\n                else:\n                    # No grid context, treat as special\n                    result.append([token, -1, -1])\n                i += 1\n            else:\n                # Unknown token, treat as special\n                result.append([token, -1, -1])\n                i += 1\n        \n        return torch.tensor(result, dtype=torch.int8)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-16T01:05:43.738521Z","iopub.execute_input":"2025-11-16T01:05:43.738813Z","iopub.status.idle":"2025-11-16T01:05:43.754339Z","shell.execute_reply.started":"2025-11-16T01:05:43.738793Z","shell.execute_reply":"2025-11-16T01:05:43.753443Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class ARCDataset:\n    \"\"\"Dataset class for ARC challenges with data augmentation\"\"\"\n    \n    def __init__(self, challenges_path: str, solutions_path: str = None):\n        self.challenges_path = challenges_path\n        self.solutions_path = solutions_path\n        \n        # Load challenges\n        with open(challenges_path, 'r') as f:\n            self.challenges = json.load(f)\n        \n        # Load solutions if provided\n        self.solutions = None\n        if solutions_path:\n            with open(solutions_path, 'r') as f:\n                self.solutions = json.load(f)\n    \n    def get_challenge_data(self, challenge_id: str) -> Dict:\n        \"\"\"Get data for a specific challenge\"\"\"\n        challenge = self.challenges[challenge_id]\n        \n        # Get training examples\n        train_examples = challenge.get('train', [])\n        \n        # Get test examples\n        test_examples = challenge.get('test', [])\n        \n        # Get solution if available\n        solution = None\n        if self.solutions and challenge_id in self.solutions:\n            solution = self.solutions[challenge_id][0]  # First solution\n        \n        return {\n            'train_examples': train_examples,\n            'test_examples': test_examples,\n            'solution': solution,\n            'challenge_id': challenge_id\n        }\n    \n    def get_all_challenges(self) -> List[str]:\n        \"\"\"Get list of all challenge IDs\"\"\"\n        return list(self.challenges.keys())\n    \n    def create_augmented_samples(self, challenge_id: str) -> List[Dict]:\n        \"\"\"Create a single training sample from a challenge with 2 train examples and 1 test input\"\"\"\n        data = self.get_challenge_data(challenge_id)\n        \n        samples = []\n        \n        # Always use first 2 training examples and first test input\n        if len(data['train_examples']) >= 2:\n            train_examples = data['train_examples'][:2]  # First 2 training examples\n            test_input = data['test_examples'][0]['input'] if data['test_examples'] else []\n            test_output = data['solution']\n            \n            samples.append({\n                'train_examples': train_examples,\n                'test_input': test_input,\n                'test_output': test_output,\n                'challenge_id': challenge_id,\n                'sample_id': challenge_id  # Simple ID, no augmentation suffix\n            })\n        \n        return samples","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-16T01:05:43.892036Z","iopub.execute_input":"2025-11-16T01:05:43.892367Z","iopub.status.idle":"2025-11-16T01:05:43.901651Z","shell.execute_reply.started":"2025-11-16T01:05:43.892343Z","shell.execute_reply":"2025-11-16T01:05:43.900584Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class ARCTorchDataset(Dataset):\n    \"\"\"PyTorch Dataset for ARC challenges\"\"\"\n    \n    def __init__(self, arc_dataset: ARCDataset, tokenizer: ARCTokenizer, \n                 token_converter = None):  # Optional converter\n        self.arc_dataset = arc_dataset\n        self.tokenizer = tokenizer\n        self.token_converter = token_converter  # Optional converter for 3D vectors\n        \n        # Create all samples with augmentation\n        self.samples = []\n        for challenge_id in arc_dataset.get_all_challenges():\n            samples = arc_dataset.create_augmented_samples(challenge_id)\n            self.samples.extend(samples)\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        \n        # Create input sequence\n        input_seq = self.tokenizer.create_input_sequence(\n            sample['train_examples'], \n            sample['test_input']\n        )\n        \n        # Create target sequence\n        if sample['test_output']:\n            target_seq = self.tokenizer.create_target_sequence(sample['test_output'])\n        else:\n            # Create dummy target for test data\n            target_seq = [self.tokenizer.SOS_TOKEN, self.tokenizer.EOS_TOKEN]\n        \n        # Pad sequences\n        input_seq = self.tokenizer.pad_sequence(input_seq, 5400)   # 5 * 30x30 + bunch of extra tokens + possible target 30x30= 6*30x30\n        target_seq = self.tokenizer.pad_sequence(target_seq, 1000) # max 30x30 + punch of extra tokens\n        \n        # Calculate dimensions\n        input_dims = []\n        output_dims = []\n        \n        for example in sample['train_examples']:\n            input_dims.append((len(example['input']), len(example['input'][0]) if example['input'] else 0))\n            output_dims.append((len(example['output']), len(example['output'][0]) if example['output'] else 0))\n        \n        test_input_dims = (len(sample['test_input']), len(sample['test_input'][0]) if sample['test_input'] else 0)\n        test_output_dims = (len(sample['test_output']), len(sample['test_output'][0]) if sample['test_output'] else 0)\n        \n        # Convert to 3D vectors if converter is provided\n        if self.token_converter is not None:\n            input_3d = self.token_converter.tokens_to_3d(\n                input_seq,\n                input_dims,\n                output_dims,\n                test_input_dims,\n                test_output_dims=test_output_dims,\n                is_target=False\n            )\n            target_3d = self.token_converter.tokens_to_3d(\n                target_seq,\n                input_dims,\n                output_dims,\n                test_input_dims,\n                test_output_dims=test_output_dims,\n                is_target=True\n            )\n            return {\n                'input': input_3d,  # Shape: [seq_len, 3] - [value, x, y]\n                'target': target_3d,  # Shape: [seq_len, 3] - [value, x, y]\n                'input_tokens': torch.tensor(input_seq, dtype=torch.int8),  # Keep original tokens too (int8 for memory efficiency)\n                'target_tokens': torch.tensor(target_seq, dtype=torch.int8),  # Keep original tokens too (int8 for memory efficiency)\n                'sample_id': sample['sample_id'],\n                'challenge_id': sample['challenge_id'],\n                'input_dims': input_dims,\n                'output_dims': output_dims,\n                'test_input_dims': test_input_dims,\n                'test_output_dims': test_output_dims\n            }\n        else:\n            # Return original token format\n            return {\n                'input': torch.tensor(input_seq, dtype=torch.int8),  # int8 for memory efficiency\n                'target': torch.tensor(target_seq, dtype=torch.int8),  # int8 for memory efficiency\n                'sample_id': sample['sample_id'],\n                'challenge_id': sample['challenge_id'],\n                'input_dims': input_dims,\n                'output_dims': output_dims,\n                'test_input_dims': test_input_dims,\n                'test_output_dims': test_output_dims\n            }","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-16T01:05:44.040098Z","iopub.execute_input":"2025-11-16T01:05:44.040940Z","iopub.status.idle":"2025-11-16T01:05:44.052970Z","shell.execute_reply.started":"2025-11-16T01:05:44.040909Z","shell.execute_reply":"2025-11-16T01:05:44.052047Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class ARCExplodedDataset(Dataset):\n    \"\"\"\n    Explodes ARCTorchDataset into trainable samples.\n    \n    Takes each sample from ARCTorchDataset and creates multiple training samples:\n    - Sample 0: input → predict target[0]\n    - Sample 1: input + target[0] → predict target[1]\n    - Sample 2: input + target[0:2] → predict target[2]\n    - etc.\n    \n    Expects both input and target to be in 3D vector format [value, x, y].\n    When adding target tokens:\n    1. Loop through input sequence and replace first PAD token with target token\n    2. If no PAD token found, append to end and remove first token\n    \"\"\"\n    \n    def __init__(self, torch_dataset: ARCTorchDataset, tokenizer: ARCTokenizer, sequence_length: int = 5400):\n        self.torch_dataset = torch_dataset\n        self.tokenizer = tokenizer\n        self.sequence_length = sequence_length\n        \n        # Create all exploded samples\n        self.exploded_samples = []\n        \n        print(f\"Exploding {len(torch_dataset)} base samples...\")\n        for base_idx in tqdm(range(len(torch_dataset))):\n            base_sample = torch_dataset[base_idx]\n            \n            # Get input and target as 3D tensors [seq_len, 3]\n            input_3d = base_sample['input']  # Shape: [max_length, 3]\n            target_3d = base_sample['target']  # Shape: [max_length, 3]\n            \n            # Find actual length of input (before padding)\n            # PAD token has value = PAD_TOKEN (10), x = -1, y = -1\n            input_actual_len = 0\n            for i in range(input_3d.shape[0]):\n                if input_3d[i, 0].item() == self.tokenizer.PAD_TOKEN:\n                    break\n            input_actual_len = i-1\n            \n            target_actual_len = 0\n            for i in range(target_3d.shape[0]):\n                if target_3d[i, 0].item() == self.tokenizer.PAD_TOKEN:\n                    break\n            target_actual_len = i-1\n            \n            #print(input_actual_len, target_actual_len)\n            \n            target_vectors = target_3d[0:target_actual_len]\n            \n            \n            # Optimized version - remove unnecessary cloning and use input_actual_len directly\n            # Replace the target_vectors collection and loop in cell 9 with this:\n\n            # In the target_vectors collection (around line 43-50):\n            # Change: target_vectors.append(target_3d[i].clone())\n            # To:     target_vectors.append(target_3d[i])  # No clone needed\n\n            # In the loop (around line 58-67):\n            # Replace the entire loop with this optimized version:\n\n            # Start with full input sequence (we'll modify it in place)\n            current_seq = input_3d.clone()\n            for i, target_vector in enumerate(target_vectors):\n                # Calculate position where we should place this target token\n                # Start from input_actual_len and add i (position in target sequence)\n                target_pos = input_actual_len + i\n                \n                if i>0:\n                    # first target vector is not added to the input sequence\n                    if target_pos < self.sequence_length:\n                        # Check if position has a PAD token\n                        if current_seq[target_pos, 0].item() == self.tokenizer.PAD_TOKEN:\n                            # Replace PAD token with target vector\n                            current_seq[target_pos] = target_vectors[i-1]\n                        else:\n                            #print(\"Sequence is full - append and remove from beginning\", target_pos, current_seq.shape)\n                            # Sequence is full - append and remove from beginning\n                            current_seq = torch.cat([current_seq[1:], target_vectors[i-1].unsqueeze(0)], dim=0)\n                    else:\n                        current_seq = torch.cat([current_seq[1:], target_vectors[i-1].unsqueeze(0)], dim=0)\n\n                # Store exploded sample\n                exploded_sample = {\n                    'input_3d': current_seq.clone(),\n                    'target_vector': target_vector.clone(),  # Clone here since we store it separately\n                    'target_position': i,\n                    'base_sample_idx': base_idx,\n                    'base_sample_id': base_sample.get('sample_id', f'sample_{base_idx}'),\n                    'challenge_id': base_sample.get('challenge_id', ''),\n                    'input_dims': base_sample.get('input_dims', []),\n                    'output_dims': base_sample.get('output_dims', []),\n                    'test_input_dims': base_sample.get('test_input_dims', (0, 0)),\n                    'test_output_dims': base_sample.get('test_output_dims', (0, 0)),\n                }\n\n                self.exploded_samples.append(exploded_sample)\n        \n        print(f\"Created {len(self.exploded_samples)} exploded samples from {len(torch_dataset)} base samples\")\n    \n    def __len__(self):\n        return len(self.exploded_samples)\n    \n    def __getitem__(self, idx):\n        sample = self.exploded_samples[idx]\n        \n        input_3d = sample['input_3d']  # Shape: [max_length, 3]\n        target_vector = sample['target_vector']  # Shape: [3]\n        \n        # Create attention mask (1 for non-padding, 0 for padding)\n        attention_mask = (input_3d[:, 0] != self.tokenizer.PAD_TOKEN).long()\n        \n        return {\n            'input_3d': input_3d,  # [max_length, 3] - full 3D vectors\n            'target_vector': target_vector,  # [3] - target as 3D vector\n            'target_value': target_vector[0].item(),  # Just the value token for convenience\n            'attention_mask': attention_mask,  # [max_length]\n            'target_position': sample['target_position'],\n            'base_sample_idx': sample['base_sample_idx'],\n            'base_sample_id': sample['base_sample_id'],\n            'challenge_id': sample['challenge_id'],\n            'input_dims': sample['input_dims'],\n            'output_dims': sample['output_dims'],\n            'test_input_dims': sample['test_input_dims'],\n            'test_output_dims': sample['test_output_dims'],\n        }\n\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-16T01:05:45.118760Z","iopub.execute_input":"2025-11-16T01:05:45.119481Z","iopub.status.idle":"2025-11-16T01:05:45.132776Z","shell.execute_reply.started":"2025-11-16T01:05:45.119442Z","shell.execute_reply":"2025-11-16T01:05:45.131962Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# augmentation during training\ndef apply_random_color_mapping(sample, apply_probability=1.0):\n    \"\"\"\n    Apply a random color permutation (0-9) to a sample.\n    OPTIMIZED: Uses vectorized PyTorch operations instead of Python loops.\n    \n    Args:\n        sample: Dict with 'input_3d' and 'target_vector'\n        apply_probability: Probability of applying augmentation (1.0 = always, 0.5 = 50% chance)\n    \n    Returns:\n        Augmented sample with permuted colors\n    \"\"\"\n    if np.random.random() > apply_probability:\n        return sample  # Skip augmentation\n    \n    # Create random permutation of colors 0-9\n    permuted_colors = torch.randperm(10, dtype=torch.int8)  # Vectorized permutation\n    \n    # Create lookup table: mapping[old_color] = new_color\n    # For colors 0-9, use permuted mapping; for special tokens (10+), keep original\n    mapping = torch.arange(18, dtype=torch.int8)  # Default: identity mapping\n    mapping[:10] = permuted_colors  # Apply permutation to colors 0-9\n    \n    # Apply to input_3d (vectorized - much faster!)\n    input_3d = sample['input_3d'].clone()\n    color_values = input_3d[:, 0].int()  # Extract color values\n    input_3d[:, 0] = mapping[color_values]  # Apply mapping in one operation\n    \n    # Apply to target_vector\n    target_vector = sample['target_vector'].clone()\n    target_color = target_vector[0].int()\n    target_vector[0] = mapping[target_color]\n    \n    # Create augmented sample\n    augmented_sample = sample.copy()\n    augmented_sample['input_3d'] = input_3d\n    augmented_sample['target_vector'] = target_vector\n    augmented_sample['target_value'] = int(target_vector[0].item())\n    \n    return augmented_sample\n\n# Dataset wrapper for live color augmentation\nclass AugmentedDataset(Dataset):\n    \"\"\"\n    Wrapper that applies random color permutation augmentation on-the-fly.\n    \"\"\"\n    def __init__(self, base_dataset, apply_probability=1.0):\n        self.base_dataset = base_dataset\n        self.apply_probability = apply_probability\n    \n    def __len__(self):\n        return len(self.base_dataset)\n    \n    def __getitem__(self, idx):\n        sample = self.base_dataset[idx]\n        return apply_random_color_mapping(sample, self.apply_probability)\n\n\n# Alternative: Augment in collate_fn (even simpler)\ndef collate_fn_with_augmentation(batch, apply_probability=1.0):\n    \"\"\"\n    Collate function that applies color augmentation to each sample.\n    \n    Usage:\n        train_loader = DataLoader(\n            train_dataset_split,\n            batch_size=batch_size,\n            shuffle=True,\n            collate_fn=lambda b: collate_fn_with_augmentation(b, apply_probability=1.0),\n            num_workers=4\n        )\n    \"\"\"\n    # Apply augmentation to each sample\n    augmented_batch = [apply_random_color_mapping(sample, apply_probability) for sample in batch]\n    \n    # Original collate logic\n    input_3d = torch.stack([item['input_3d'] for item in augmented_batch])\n    target_values = torch.stack([torch.tensor(item['target_value'], dtype=torch.long) for item in augmented_batch])\n    attention_mask = torch.stack([item['attention_mask'] for item in augmented_batch])\n    \n    return {\n        'input_3d': input_3d,\n        'target_values': target_values,\n        'attention_mask': attention_mask\n    }","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-16T01:05:45.370888Z","iopub.execute_input":"2025-11-16T01:05:45.371248Z","iopub.status.idle":"2025-11-16T01:05:45.381927Z","shell.execute_reply.started":"2025-11-16T01:05:45.371222Z","shell.execute_reply":"2025-11-16T01:05:45.381000Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# creating the dataset","metadata":{}},{"cell_type":"code","source":"PATH='/kaggle/input/arc-prize-2025/'\n\n# Load datasets\nprint(\"Loading datasets...\")\ntrain_dataset = ARCDataset(\n    challenges_path=PATH+'arc-agi_training_challenges.json',\n    solutions_path=PATH+'arc-agi_training_solutions.json'\n)\n\ntest_dataset = ARCDataset(\n    challenges_path=PATH+'arc-agi_test_challenges.json'\n)\n\nprint(f\"Training challenges: {len(train_dataset.get_all_challenges())}\")\nprint(f\"Test challenges: {len(test_dataset.get_all_challenges())}\")\n\n# Create PyTorch datasets\ntokenizer = ARCTokenizer()\ntoken_converter = TokenTo3DConverter(tokenizer)\ntrain_torch_dataset = ARCTorchDataset(train_dataset, tokenizer, token_converter=token_converter)\ntest_torch_dataset = ARCTorchDataset(test_dataset, tokenizer, token_converter=token_converter)\n\nprint(f\"\\nTraining samples (with augmentation): {len(train_torch_dataset)}\")\nprint(f\"Test samples: {len(test_torch_dataset)}\")\n\n# Test data loading\nsample = train_torch_dataset[0]\nprint(f\"\\nSample data:\")\nprint(f\"Sample ID: {sample['sample_id']}\")\nprint(f\"Challenge ID: {sample['challenge_id']}\")\nprint(f\"Input sequence length: {len(sample['input'])}\")\nprint(f\"Target sequence length: {len(sample['target'])}\")\nprint(f\"Input dims: {sample['input_dims']}\")\nprint(f\"Output dims: {sample['output_dims']}\")\nprint(f\"Test input dims: {sample['test_input_dims']}\")\nprint(f\"Test output dims: {sample['test_output_dims']}\")\n\n# Create exploded datasets from existing ARCTorchDataset\nprint(\"Creating exploded training dataset...\")\ntrain_exploded_dataset = ARCExplodedDataset(train_torch_dataset, tokenizer, sequence_length=5400)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:06:22.706963Z","iopub.execute_input":"2025-11-16T01:06:22.707256Z","iopub.status.idle":"2025-11-16T01:06:48.437668Z","shell.execute_reply.started":"2025-11-16T01:06:22.707225Z","shell.execute_reply":"2025-11-16T01:06:48.436497Z"}},"outputs":[{"name":"stdout","text":"Loading datasets...\nTraining challenges: 1000\nTest challenges: 240\n\nTraining samples (with augmentation): 1000\nTest samples: 240\n\nSample data:\nSample ID: 00576224\nChallenge ID: 00576224\nInput sequence length: 5400\nTarget sequence length: 1000\nInput dims: [(2, 2), (2, 2)]\nOutput dims: [(6, 6), (6, 6)]\nTest input dims: (2, 2)\nTest output dims: (6, 6)\nCreating exploded training dataset...\nExploding 1000 base samples...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [00:24<00:00, 40.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Created 204169 exploded samples from 1000 base samples\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# augmentation check\nif AUGMENTATION:\n    train_dataset_augmented = AugmentedDataset(train_exploded_dataset, apply_probability=1.0)\n\n\nif EVAL_COLOR_DIST:\n    from collections import Counter, defaultdict\n    def analyze_color_distribution(dataset):\n        \"\"\"Analyze the distribution of target colors (0-9) in the dataset\"\"\"\n        color_counts = Counter()\n        \n        print(\"Analyzing color distribution...\")\n        for idx in tqdm(range(len(dataset))):\n            sample = dataset[idx]\n            target_value = sample['target_value']\n            # Only count color tokens (0-9), ignore special tokens\n            color_counts[target_value] += 1\n        \n        return color_counts\n    aug_counts = analyze_color_distribution(train_dataset_augmented)\n    original_counts = analyze_color_distribution(train_exploded_dataset)\n    print(original_counts)\n    print(aug_counts) \n    # result, looks a lot better\n    # Counter({0: 86966, 8: 19131, 1: 17683, 4: 14558, 7: 13704, 3: 12134, 2: 11829, 5: 7086, 6: 5145, 9: 3042})\n    # Counter({2: 19417, 0: 19327, 4: 19150, 7: 19150, 8: 19144, 5: 19062, 3: 19055, 9: 19004, 6: 18995, 1: 18974})\n\nif AUGMENTATION:\n    train_exploded_dataset = train_dataset_augmented","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:06:48.450631Z","iopub.execute_input":"2025-11-16T01:06:48.450948Z","iopub.status.idle":"2025-11-16T01:06:48.475266Z","shell.execute_reply.started":"2025-11-16T01:06:48.450919Z","shell.execute_reply":"2025-11-16T01:06:48.474225Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Create DataLoader\ndef collate_fn(batch):\n    \"\"\"Collate function for batching\"\"\"\n    input_3d = torch.stack([item['input_3d'] for item in batch])\n    target_values = torch.stack([torch.tensor(item['target_value'], dtype=torch.long) for item in batch])\n    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n    \n    return {\n        'input_3d': input_3d,\n        'target_values': target_values,\n        'attention_mask': attention_mask\n    }\n    \n# Split dataset by challenge_id (ensures no data leakage)\n# Create TWO validation sets: tiny (frequent) and full (accurate)\nimport random\nfrom collections import defaultdict\nfrom torch.utils.data import Subset\n\n# Group samples by challenge_id\nchallenge_to_indices = defaultdict(list)\nfor idx in range(len(train_exploded_dataset)):\n    sample = train_exploded_dataset[idx]\n    challenge_id = sample['challenge_id']\n    challenge_to_indices[challenge_id].append(idx)\n\n# Get unique challenge IDs\nchallenge_ids = list(challenge_to_indices.keys())\nprint(f\"Total challenges: {len(challenge_ids)}\")\n\n# Shuffle and split challenges (not individual samples)\nrandom.seed(42)\nrandom.shuffle(challenge_ids)\n\ntrain_ratio = 0.8\nsplit_idx = int(len(challenge_ids) * train_ratio)\ntrain_challenge_ids = set(challenge_ids[:split_idx])\nval_challenge_ids_all = set(challenge_ids[split_idx:])\n\n# Split validation challenges into tiny and full\nval_challenge_ids_list = list(val_challenge_ids_all)\nrandom.shuffle(val_challenge_ids_list)\ntiny_val_ratio = 0.01  # 10% of validation challenges for tiny set\ntiny_split_idx = int(len(val_challenge_ids_list) * tiny_val_ratio)\ntiny_val_challenge_ids = set(val_challenge_ids_list[:tiny_split_idx])\nfull_val_challenge_ids = set(val_challenge_ids_list[tiny_split_idx:])\n\nprint(f\"Train challenges: {len(train_challenge_ids)}\")\nprint(f\"Tiny val challenges: {len(tiny_val_challenge_ids)}\")\nprint(f\"Full val challenges: {len(full_val_challenge_ids)}\")\n\n# Collect indices for each split\ntrain_indices = []\ntiny_val_indices = []\nfull_val_indices = []\n\nfor challenge_id, indices in challenge_to_indices.items():\n    if challenge_id in train_challenge_ids:\n        train_indices.extend(indices)\n    elif challenge_id in tiny_val_challenge_ids:\n        tiny_val_indices.extend(indices)\n    elif challenge_id in full_val_challenge_ids:\n        full_val_indices.extend(indices)\n\nprint(f\"\\nTrain samples: {len(train_indices)}\")\nprint(f\"Tiny val samples: {len(tiny_val_indices)}\")\nprint(f\"Full val samples: {len(full_val_indices)}\")\n\n# Create subset datasets\ntrain_dataset_split = Subset(train_exploded_dataset, train_indices)\ntiny_val_dataset_split = Subset(train_exploded_dataset, tiny_val_indices)\nfull_val_dataset_split = Subset(train_exploded_dataset, full_val_indices)\n\n# Create DataLoaders\nbatch_size = 4#256\n\ntrain_loader = DataLoader(\n    train_dataset_split,\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=4\n)\n\n# Tiny validation loader (for frequent checks)\ntiny_val_loader = DataLoader(\n    tiny_val_dataset_split,\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=1\n)\n\n# Full validation loader (for accurate metrics)\nfull_val_loader = DataLoader(\n    full_val_dataset_split,\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=1\n)\n\nprint(f\"\\nTrain batches: {len(train_loader)}\")\nprint(f\"Tiny val batches: {len(tiny_val_loader)}\")\nprint(f\"Full val batches: {len(full_val_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:06:48.476633Z","iopub.execute_input":"2025-11-16T01:06:48.476965Z","iopub.status.idle":"2025-11-16T01:07:51.658985Z","shell.execute_reply.started":"2025-11-16T01:06:48.476936Z","shell.execute_reply":"2025-11-16T01:07:51.658099Z"}},"outputs":[{"name":"stdout","text":"Total challenges: 1000\nTrain challenges: 800\nTiny val challenges: 20\nFull val challenges: 180\n\nTrain samples: 161456\nTiny val samples: 5083\nFull val samples: 37630\n\nTrain batches: 631\nTiny val batches: 20\nFull val batches: 147\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Attention U-net","metadata":{}},{"cell_type":"code","source":"PRINT_MEM = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.cuda.amp import autocast\n# U-Net for Attention Matrix Pattern Detection\nclass AttentionUNet(nn.Module):\n    \"\"\"\n    U-Net architecture for processing attention matrices.\n    Detects hierarchical patterns in attention and outputs pattern-enhanced attention.\n    \n    Input: [batch, 1, seq_len, seq_len] attention scores\n    Output: [batch, 1, seq_len, seq_len] pattern-enhanced attention\n    \"\"\"\n    \n    def __init__(self, base_channels=32, num_downsample=3):\n        super().__init__()\n        self.num_downsample = num_downsample\n        \n        # Encoder (downsampling path)\n        self.encoder_blocks = nn.ModuleList()\n        in_channels = 1\n        for i in range(num_downsample):\n            out_channels = base_channels * (2 ** i)\n            self.encoder_blocks.append(\n                nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n                    nn.BatchNorm2d(out_channels),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n                    nn.BatchNorm2d(out_channels),\n                    nn.ReLU(inplace=True)\n                )\n            )\n            in_channels = out_channels\n        \n        # Bottleneck (lowest resolution)\n        bottleneck_channels = base_channels * (2 ** num_downsample)\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(in_channels, bottleneck_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(bottleneck_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(bottleneck_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n        # Decoder (upsampling path with skip connections)\n        self.decoder_blocks = nn.ModuleList()\n        for i in range(num_downsample - 1, -1, -1):\n            in_channels = bottleneck_channels if i == num_downsample - 1 else base_channels * (2 ** (i + 1))\n            skip_channels = 0 #base_channels * (2 ** i)\n            out_channels = base_channels * (2 ** i)\n            \n            # Create decoder block (will be applied in forward)\n            self.decoder_blocks.append(nn.ModuleDict({\n                'upsample': nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n                'conv1': nn.Conv2d(out_channels + skip_channels, out_channels, kernel_size=3, padding=1),\n                'bn1': nn.BatchNorm2d(out_channels),\n                'conv2': nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n                'bn2': nn.BatchNorm2d(out_channels)\n            }))\n        \n        # Final output layer\n        self.final_conv = nn.Conv2d(base_channels, 1, kernel_size=1)\n\n    def forward(self, x, debug_memory=False):\n        \"\"\"\n        Args:\n            x: [batch, 1, seq_len, seq_len] attention scores\n            debug_memory: If True, print memory usage at each step\n        \n        Returns:\n            [batch, 1, seq_len, seq_len] pattern-enhanced attention\n        \"\"\"\n        def print_mem(label):\n            if PRINT_MEM and torch.cuda.is_available():\n                allocated = torch.cuda.memory_allocated() / 1e9\n                reserved = torch.cuda.memory_reserved() / 1e9\n                print(f\"  {label}: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n        \n        print_mem(\"U-Net input\")\n        # Encoder path (no skip connections - saves memory)\n        for i, encoder_block in enumerate(self.encoder_blocks):\n            x = encoder_block(x)\n            print_mem(f\"After encoder {i}\")\n            # Downsample (using max pooling)\n            x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n            print_mem(f\"After pooling {i}\")\n        \n        # Bottleneck\n        x = self.bottleneck(x)\n        print_mem(\"After bottleneck\")\n        \n        # Decoder path (without skip connections - saves memory)\n        for i, decoder_block in enumerate(self.decoder_blocks):\n            # Upsample\n            x = decoder_block['upsample'](x)\n            print_mem(f\"After upsample {i}\")\n            # No skip connection - just use upsampled features directly\n            # Apply conv layers (conv1 now expects only out_channels, not out_channels + skip_channels)\n            x = decoder_block['conv1'](x)\n            x = decoder_block['bn1'](x)\n            x = nn.functional.relu(x, inplace=True)\n            x = decoder_block['conv2'](x)\n            x = decoder_block['bn2'](x)\n            x = nn.functional.relu(x, inplace=True)\n            print_mem(f\"After decoder {i}\")\n        \n        # Final output\n        x = self.final_conv(x)\n        print_mem(\"U-Net output\")\n        return x\n        \n    #def forward(self, x):\n    #    \"\"\"\n    #    Args:\n    #        x: [batch, 1, seq_len, seq_len] attention scores\n    #    \n    #    Returns:\n    #        [batch, 1, seq_len, seq_len] pattern-enhanced attention\n    #    \"\"\"\n    #    # Encoder path (save skip connections)\n    #    #skip_connections = []\n    #    for encoder_block in self.encoder_blocks:\n    #        x = encoder_block(x)\n    #        #skip_connections.append(x)\n    #        # Downsample (using max pooling)\n    #        x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n    #    \n    #    # Bottleneck\n    #    x = self.bottleneck(x)\n    #    \n    #    # Decoder path (with skip connections)\n    #    for i, decoder_block in enumerate(self.decoder_blocks):\n    #        # Upsample\n    #        x = decoder_block['upsample'](x)\n            # Concatenate skip connection\n            #skip = skip_connections[-(i+1)]\n            # Handle size mismatch (due to pooling)\n            #if x.shape[2:] != skip.shape[2:]:\n            #    x = nn.functional.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n            #x = torch.cat([x, skip], dim=1)\n            # Apply conv layers\n    #       x = decoder_block['conv1'](x)\n    #       x = decoder_block['bn1'](x)\n    #       x = nn.functional.relu(x, inplace=True)\n    #       x = decoder_block['conv2'](x)\n    #       x = decoder_block['bn2'](x)\n    #       x = nn.functional.relu(x, inplace=True)\n    #   \n    #   # Final output\n    #   x = self.final_conv(x)\n    #    return x\n\n\n# Custom Transformer Layer with U-Net-filtered Attention\nclass UNetFilteredAttentionLayer(nn.Module):\n    \"\"\"\n    Custom transformer layer that uses U-Net to detect patterns in attention matrix.\n    \n    The idea is to:\n    1. Compute full attention matrix (Q @ K^T)\n    2. Apply U-Net to detect hierarchical patterns\n    3. Combine original and pattern-enhanced attention\n    4. Apply softmax and use for attention\n    \"\"\"\n    \n    def __init__(self, d_model, nhead=1, dim_feedforward=2048, dropout=0.1, \n                 unet_base_channels=32, unet_num_downsample=3):\n        super().__init__()\n        assert d_model % nhead == 0, \"d_model must be divisible by nhead\"\n        assert nhead == 1, \"Currently only supports single head\"\n        \n        self.d_model = d_model\n        self.nhead = nhead\n        self.d_k = d_model // nhead\n        \n        # Multi-head attention projections\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n        \n        # U-Net to detect patterns in attention matrix\n        self.unet = AttentionUNet(\n            base_channels=unet_base_channels,\n            num_downsample=unet_num_downsample\n        )\n        \n        # Learnable combination weight (optional)\n        self.pattern_weight = nn.Parameter(torch.tensor(0.5))\n        \n        # Feedforward network\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim_feedforward, d_model),\n            nn.Dropout(dropout)\n        )\n        \n        # Layer norms\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, src_key_padding_mask=None):\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len, d_model]\n            src_key_padding_mask: [batch_size, seq_len] - True for padding tokens\n        \n        Returns:\n            [batch_size, seq_len, d_model]\n        \"\"\"\n        # Self-attention with U-Net pattern detection\n        residual = x\n        x = self.norm1(x)\n        \n        batch_size, seq_len, _ = x.shape\n\n        def print_mem(label):\n            if PRINT_MEM and torch.cuda.is_available():\n                allocated = torch.cuda.memory_allocated() / 1e9\n                reserved = torch.cuda.memory_reserved() / 1e9\n                print(f\"  {label}: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n        \n        print_mem('before att')\n        # Compute Q, K, V\n        Q = self.w_q(x)  # [batch_size, seq_len, d_model]\n        K = self.w_k(x)  # [batch_size, seq_len, d_model]\n        V = self.w_v(x)  # [batch_size, seq_len, d_model]\n\n        print_mem('before att matrix')\n        # For single head, we don't need to reshape\n        # Compute attention scores (full attention matrix)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # [batch, seq_len, seq_len]\n\n        print_mem('after att matrix')\n        # Apply padding mask if provided\n        if src_key_padding_mask is not None:\n            # Expand mask: [batch, seq_len] -> [batch, seq_len, 1]\n            mask = src_key_padding_mask.unsqueeze(2)  # [batch, seq_len, 1]\n            scores = scores.masked_fill(mask, float('-inf'))\n            # Also mask columns\n            mask = src_key_padding_mask.unsqueeze(1)  # [batch, 1, seq_len]\n            scores = scores.masked_fill(mask, float('-inf'))\n        \n        # Apply U-Net to detect patterns\n        # Reshape for U-Net: [batch, seq_len, seq_len] -> [batch, 1, seq_len, seq_len]\n        scores_for_unet = scores.unsqueeze(1)  # [batch, 1, seq_len, seq_len]\n        \n        # U-Net processes attention matrix\n        #pattern_enhanced = self.unet(scores_for_unet)  # [batch, 1, seq_len, seq_len]\n        with autocast():\n            # Convert input to FP16 explicitly (autocast will handle operations inside U-Net)\n            scores_for_unet_fp16 = scores_for_unet.half()  # Explicitly convert to FP16\n            pattern_enhanced = self.unet(scores_for_unet_fp16)  # [batch, 1, seq_len, seq_len]\n            # Convert back to FP32 for combination with original scores\n            pattern_enhanced = pattern_enhanced.float()\n        pattern_enhanced = pattern_enhanced.squeeze(1)  # [batch, seq_len, seq_len]\n        \n        # Combine original and pattern-enhanced attention\n        # Option 1: Weighted combination (learnable weight)\n        scores = (1 - torch.sigmoid(self.pattern_weight)) * scores + torch.sigmoid(self.pattern_weight) * pattern_enhanced\n        \n        # Option 2: Add (uncomment to use instead)\n        # scores = scores + pattern_enhanced\n        \n        # Option 3: Multiply (pattern gating)\n        # scores = scores * torch.sigmoid(pattern_enhanced)\n        \n        # Apply softmax\n        attn_weights = torch.softmax(scores, dim=-1)  # [batch, seq_len, seq_len]\n        #attn_weights = self.dropout(attn_weights)\n        \n        # Apply attention to values\n        attn_output = torch.matmul(attn_weights, V)  # [batch, seq_len, d_model]\n        \n        # Output projection\n        attn_output = self.w_o(attn_output)\n        x = residual + self.dropout(attn_output)\n        \n        # Feedforward\n        residual = x\n        x = self.norm2(x)\n        x = residual + self.ffn(x)\n        \n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:07:51.659938Z","iopub.execute_input":"2025-11-16T01:07:51.660193Z","iopub.status.idle":"2025-11-16T01:07:51.691481Z","shell.execute_reply.started":"2025-11-16T01:07:51.660166Z","shell.execute_reply":"2025-11-16T01:07:51.690315Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class CustomTransformerEncoder(nn.Module):\n    \"\"\"\n    Custom transformer encoder using CNN-filtered attention layers\n    \"\"\"\n    def __init__(self, encoder_layer, num_layers):\n        super().__init__()\n        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n        \n    def forward(self, x, src_key_padding_mask=None):\n        \"\"\"\n        Args:\n            x: [batch_size, seq_len, d_model]\n            src_key_padding_mask: [batch_size, seq_len] - True for padding tokens\n        \"\"\"\n        for layer in self.layers:\n            x = layer(x, src_key_padding_mask)\n        return x","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-11-16T01:07:51.692576Z","iopub.execute_input":"2025-11-16T01:07:51.692946Z","iopub.status.idle":"2025-11-16T01:07:51.726080Z","shell.execute_reply.started":"2025-11-16T01:07:51.692902Z","shell.execute_reply":"2025-11-16T01:07:51.724864Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.login(key='9c6d131f5fcedb96565fa31f4680c2da83ea07d5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:09:10.142518Z","iopub.execute_input":"2025-11-16T01:09:10.142843Z","iopub.status.idle":"2025-11-16T01:09:22.027816Z","shell.execute_reply.started":"2025-11-16T01:09:10.142819Z","shell.execute_reply":"2025-11-16T01:09:22.026945Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatthiaskargl\u001b[0m (\u001b[33mmatthiaskargl-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"class NextTokenPredictor(nn.Module):\n    \"\"\"\n    Transformer model to predict next token from 3D vectors [value, x, y].\n    \"\"\"\n    \n    def __init__(self, vocab_size=18, d_model=16, nhead=8, num_layers=4, \n                 dim_feedforward=1024, max_seq_length=5400, dropout=0.1, unet_base_channels=32, unet_num_downsample=3):\n        super().__init__()\n        \n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        self.max_seq_length = max_seq_length\n        \n        # Embedding for token values (0-17)\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        \n        # Projection for x, y coordinates (add coordinate information)\n        self.coord_projection = nn.Linear(2, d_model)  # [x, y] -> d_model\n        \n        # Positional encoding (learned)\n        #self.pos_encoding = nn.Parameter(torch.randn(max_seq_length, d_model) * 0.02)\n        #self.pos_encoding = self.create_sinusoidal_positional_encoding(max_seq_length, d_model)\n        pos_encoding = self.create_sinusoidal_positional_encoding(max_seq_length, d_model)\n        self.register_buffer('pos_encoding', pos_encoding)\n        \n        \n        ## Transformer encoder\n        #encoder_layer = nn.TransformerEncoderLayer(\n        #    d_model=d_model,\n        #    nhead=nhead,\n        #    dim_feedforward=dim_feedforward,\n        #    dropout=dropout,\n        #    batch_first=True\n        #)\n        #self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # Transformer encoder\n        encoder_layer = UNetFilteredAttentionLayer(\n               d_model=d_model,\n               nhead=1,  # U-Net approach currently supports single head only\n               dim_feedforward=dim_feedforward,\n               dropout=dropout,\n               unet_base_channels=unet_base_channels,  # Base channels for U-Net (default: 32)\n               unet_num_downsample=unet_num_downsample  # Number of downsampling levels (default: 3)\n        )\n        self.transformer = CustomTransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        # Output projection to vocab\n        self.output_proj = nn.Linear(d_model, vocab_size)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def create_sinusoidal_positional_encoding(self, max_len, d_model):\n        \"\"\"\n        Create sinusoidal positional encoding (no learnable parameters).\n        \n        Args:\n            max_len: Maximum sequence length\n            d_model: Model dimension\n        \n        Returns:\n            [max_len, d_model] tensor with positional encodings\n        \"\"\"\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices: sin\n        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices: cos\n        \n        return pe  # [max_len, d_model]\n\n        \n    def forward(self, input_3d, attention_mask=None):\n        \"\"\"\n        Args:\n            input_3d: [batch_size, seq_len, 3] - [value, x, y] vectors\n            attention_mask: [batch_size, seq_len] - 1 for real tokens, 0 for padding\n        \n        Returns:\n            logits: [batch_size, seq_len, vocab_size] - logits for each position\n        \"\"\"\n        batch_size, seq_len, _ = input_3d.shape\n        \n        # Extract components\n        token_values = input_3d[:, :, 0].long()  # [batch_size, seq_len] - token values\n        coordinates = input_3d[:, :, 1:3].float()  # [batch_size, seq_len, 2] - x, y\n        \n        # Embed tokens\n        token_emb = self.token_embedding(token_values)  # [batch_size, seq_len, d_model]\n        \n        # Add coordinate information\n        coord_emb = self.coord_projection(coordinates)  # [batch_size, seq_len, d_model]\n        \n        # Combine token and coordinate embeddings\n        x = token_emb + coord_emb  # [batch_size, seq_len, d_model]\n        \n        # Add positional encoding\n        x = x + self.pos_encoding[:seq_len].unsqueeze(0)  # [batch_size, seq_len, d_model]\n        \n        #x = self.dropout(x)\n        if attention_mask is not None:\n            padding_mask = (attention_mask == 0).bool()  # True for padding, False for real tokens\n        else:\n            padding_mask = None\n        \n        # Apply transformer\n        x = self.transformer(x, src_key_padding_mask=padding_mask)  # [batch_size, seq_len, d_model]\n        \n        # Get logits for all positions\n        logits = self.output_proj(x)  # [batch_size, seq_len, vocab_size]\n        \n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:09:22.029518Z","iopub.execute_input":"2025-11-16T01:09:22.030104Z","iopub.status.idle":"2025-11-16T01:09:22.042496Z","shell.execute_reply.started":"2025-11-16T01:09:22.030082Z","shell.execute_reply":"2025-11-16T01:09:22.041383Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Create model\nmodel = NextTokenPredictor(\n    vocab_size=tokenizer.vocab_size,\n    d_model=16,\n    nhead=1,\n    num_layers=1,\n    dim_feedforward=128,\n    max_seq_length=5400,\n    dropout=0.1,\n    unet_base_channels=2,\n    unet_num_downsample=2,\n).to(device)\n\nprint(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\nprint(f\"Model device: {next(model.parameters()).device}\")\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n\n# Learning rate scheduler\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:09:22.043596Z","iopub.execute_input":"2025-11-16T01:09:22.043861Z","iopub.status.idle":"2025-11-16T01:09:22.096620Z","shell.execute_reply.started":"2025-11-16T01:09:22.043834Z","shell.execute_reply":"2025-11-16T01:09:22.095635Z"}},"outputs":[{"name":"stdout","text":"Model created with 1,933,876 parameters\nModel device: cpu\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Evaluation function\nimport os\n\ndef save_checkpoint(model, optimizer, epoch, batch_idx, val_loss, val_acc, train_loss, train_acc, \n                   checkpoint_dir='checkpoints', is_best=False):\n    \"\"\"\n    Save model checkpoint\n    \n    Args:\n        model: The model to save\n        optimizer: The optimizer to save\n        epoch: Current epoch number\n        batch_idx: Current batch index\n        val_loss: Validation loss\n        val_acc: Validation accuracy\n        train_loss: Training loss\n        train_acc: Training accuracy\n        checkpoint_dir: Directory to save checkpoints\n        is_best: Whether this is the best model so far\n    \"\"\"\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    checkpoint = {\n        'epoch': epoch,\n        'batch': batch_idx,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'val_loss': val_loss,\n        'val_accuracy': val_acc,\n        'train_loss': train_loss,\n        'train_accuracy': train_acc,\n    }\n    \n    # Save regular checkpoint\n    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch{epoch}_batch{batch_idx}.pt')\n    torch.save(checkpoint, checkpoint_path)\n    \n    # Save best model if applicable\n    if is_best:\n        best_path = os.path.join(checkpoint_dir, 'best_model.pt')\n        torch.save(checkpoint, best_path)\n        print(f\"  ✓ Best model saved: {best_path}\")\n    \n    return checkpoint_path\n    \ndef evaluate(model, dataloader, criterion, device):\n    \"\"\"Evaluate model on dataset\"\"\"\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            input_3d = batch['input_3d'].to(device)\n            target_values = batch['target_values'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            logits = model(input_3d=input_3d, attention_mask=attention_mask)\n            \n            batch_size = input_3d.size(0)\n            seq_lengths = attention_mask.sum(dim=1) - 1\n            last_logits = logits[torch.arange(batch_size), seq_lengths]\n            \n            loss = criterion(last_logits, target_values)\n            total_loss += loss.item()\n            \n            predictions = last_logits.argmax(dim=1)\n            correct += (predictions == target_values).sum().item()\n            total += target_values.size(0)\n    print(last_logits[0:3], target_values[0:3])\n    \n    avg_loss = total_loss / len(dataloader)\n    accuracy = 100 * correct / total\n    return avg_loss, accuracy\n\n# Updated train_epoch with periodic validation during training\ndef train_epoch(model, train_dataloader, tiny_val_loader, full_val_loader, criterion, optimizer, device, \n                log_every_n_batches=2, tiny_val_every_n_batches=10, full_val_every_n_batches=200):\n    \"\"\"\n    Train for one epoch with periodic validation using two validation sets\n    \n    Args:\n        model: The model to train\n        train_dataloader: Training data loader\n        tiny_val_loader: Tiny validation loader (for frequent checks)\n        full_val_loader: Full validation loader (for accurate metrics)\n        criterion: Loss function\n        optimizer: Optimizer\n        device: Device to run on\n        log_every_n_batches: Log to wandb every N batches\n        tiny_val_every_n_batches: Run tiny validation every N batches (default: 10)\n        full_val_every_n_batches: Run full validation every N batches (default: 200)\n    \"\"\"\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    best_val_los=1e6\n    \n    pbar = tqdm(train_dataloader, desc=\"Training\")\n    for batch_idx, batch in enumerate(pbar):\n        # Move to device\n        input_3d = batch['input_3d'].to(device)  # [batch_size, seq_len, 3]\n        target_values = batch['target_values'].to(device)  # [batch_size]\n        attention_mask = batch['attention_mask'].to(device)  # [batch_size, seq_len]\n        \n        # Forward pass\n        optimizer.zero_grad()\n        logits = model(input_3d=input_3d, attention_mask=attention_mask)  # [batch_size, seq_len, vocab_size]\n        \n        # Get logits for the last non-padding position (where we predict)\n        # Find last non-padding position for each sequence\n        batch_size = input_3d.size(0)\n        seq_lengths = attention_mask.sum(dim=1) - 1  # -1 because we want the position before the target\n        last_logits = logits[torch.arange(batch_size), seq_lengths]  # [batch_size, vocab_size]\n        \n        # Compute loss\n        loss = criterion(last_logits, target_values)\n        \n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n        optimizer.step()\n        \n        # Metrics\n        total_loss += loss.item()\n        predictions = last_logits.argmax(dim=1)\n        batch_correct = (predictions == target_values).sum().item()\n        correct += batch_correct\n        total += target_values.size(0)\n        batch_acc = 100 * batch_correct / target_values.size(0)\n        \n        # Log to wandb every N batches (default: every other batch)\n        if batch_idx % log_every_n_batches == 0:\n            wandb.log({\n                \"batch_loss\": loss.item(),\n                \"batch_accuracy\": batch_acc,\n                \"running_accuracy\": 100 * correct / total,\n            })\n        \n        # Tiny validation (frequent, quick check)\n        if (batch_idx + 1) % tiny_val_every_n_batches == 0:\n            tiny_val_loss, tiny_val_acc = evaluate(model, tiny_val_loader, criterion, device)\n            \n            # Log tiny validation metrics\n            wandb.log({\n                \"tiny_val_loss\": tiny_val_loss,\n                \"tiny_val_accuracy\": tiny_val_acc,\n                \"train_batch\": batch_idx + 1,\n            })\n            \n            pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{100 * correct / total:.2f}%',\n                'tiny_val': f'{tiny_val_acc:.1f}%'\n            })\n            \n            model.train()  # Switch back to training mode\n        \n        # Full validation (less frequent, more accurate)\n        if (batch_idx + 1) % full_val_every_n_batches == 0:\n            full_val_loss, full_val_acc = evaluate(model, full_val_loader, criterion, device)\n            is_better=full_val_loss<best_val_los\n            if is_better:\n                best_val_los=full_val_loss\n\n            save_checkpoint(model, optimizer, epoch, batch_idx, full_val_loss, full_val_acc, total_loss, batch_acc, \n                   checkpoint_dir='checkpoints', is_best=is_better)\n            \n            # Log full validation metrics\n            wandb.log({\n                \"full_val_loss\": full_val_loss,\n                \"full_val_accuracy\": full_val_acc,\n                \"train_batch\": batch_idx + 1,\n            })\n            \n            pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{100 * correct / total:.2f}%',\n                'full_val': f'{full_val_acc:.1f}%'\n            })\n            \n            model.train()  # Switch back to training mode\n        \n        # Update progress bar (if no validation was run this batch)\n        if (batch_idx + 1) % tiny_val_every_n_batches != 0 and (batch_idx + 1) % full_val_every_n_batches != 0:\n            pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{100 * correct / total:.2f}%'\n            })\n    \n    avg_loss = total_loss / len(train_dataloader)\n    accuracy = 100 * correct / total\n    return avg_loss, accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:09:22.098561Z","iopub.execute_input":"2025-11-16T01:09:22.098896Z","iopub.status.idle":"2025-11-16T01:09:22.119346Z","shell.execute_reply.started":"2025-11-16T01:09:22.098872Z","shell.execute_reply":"2025-11-16T01:09:22.118393Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Updated training loop with validation and wandb logging\nnum_epochs = 1\nprint(f\"\\nStarting training for {num_epochs} epochs...\")\nprint(\"=\" * 60)\n\nbest_val_acc = 0.0\n\n# Validation frequencies\ntiny_val_every_n_batches = 1000   # Tiny validation every 10 batches (~30 seconds)\nfull_val_every_n_batches = 20000  # Full validation every 200 batches (~5-10 minutes)\n\nwandb.init(\n    name='test-aug-colors-unet_try0',\n    project=\"arc-next-token-prediction\",\n    config={\n        \"vocab_size\": tokenizer.vocab_size,\n        \"d_model\": model.d_model,\n        #\"nhead\": model.transformer.layers[0].self_attn.num_heads,\n        \"num_layers\": len(model.transformer.layers),\n        \"batch_size\": batch_size,\n        \"learning_rate\": optimizer.param_groups[0]['lr'],\n        \"max_seq_length\": model.max_seq_length,\n    }\n)\n\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n    print(\"-\" * 60)\n\n    train_loss, train_acc = train_epoch(\n        model, \n        train_loader, \n        tiny_val_loader,  # Tiny validation for frequent checks\n        full_val_loader,  # Full validation for accurate metrics\n        criterion, \n        optimizer, \n        device,\n        tiny_val_every_n_batches=tiny_val_every_n_batches,\n        full_val_every_n_batches=full_val_every_n_batches\n    )\n    \n    # Update learning rate\n    scheduler.step()\n    \n    # Log to wandb\n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"train_loss\": train_loss,\n        \"train_accuracy\": train_acc,\n        \"val_loss\": val_loss,\n        \"val_accuracy\": val_acc,\n        \"learning_rate\": optimizer.param_groups[0]['lr']\n    })\n    \n    # Print results\n    print(f\"\\nEpoch {epoch + 1} Results:\")\n    print(f\"  Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n    print(f\"  Val   - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n    print(f\"  Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n    \n    # Track best model\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        print(f\"  ✓ New best validation accuracy: {best_val_acc:.2f}%\")\n        # Optionally save model checkpoint\n        # torch.save(model.state_dict(), 'best_model.pt')\n    \nprint(\"\\n\" + \"=\" * 60)\nprint(\"Training completed!\")\nprint(f\"Best validation accuracy: {best_val_acc:.2f}%\")\nprint(\"=\" * 60)\n\nwandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:09:56.158363Z","iopub.execute_input":"2025-11-16T01:09:56.159365Z","execution_failed":"2025-11-16T01:10:45.903Z"}},"outputs":[{"name":"stdout","text":"\nStarting training for 1 epochs...\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251116_010956-q6i36xaz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/matthiaskargl-personal/arc-next-token-prediction/runs/q6i36xaz' target=\"_blank\">test-aug-colors-unet_try0</a></strong> to <a href='https://wandb.ai/matthiaskargl-personal/arc-next-token-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/matthiaskargl-personal/arc-next-token-prediction' target=\"_blank\">https://wandb.ai/matthiaskargl-personal/arc-next-token-prediction</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/matthiaskargl-personal/arc-next-token-prediction/runs/q6i36xaz' target=\"_blank\">https://wandb.ai/matthiaskargl-personal/arc-next-token-prediction/runs/q6i36xaz</a>"},"metadata":{}},{"name":"stdout","text":"\nEpoch 1/1\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/631 [00:00<?, ?it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}