{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer and Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizer import ARCTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = ARCTokenizer()\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"Special tokens: {tokenizer.token_to_id}\")\n",
        "\n",
        "# Test tokenizer\n",
        "test_grid = [[1, 2, 3], [4, 5, 6]]\n",
        "tokens = tokenizer.grid_to_tokens(test_grid)\n",
        "print(f\"\\nTest grid: {test_grid}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Back to grid: {tokenizer.tokens_to_grid(tokens, (2, 3))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token converter (enrich with position info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from token_converter import TokenTo3DConverter\n",
        "# Initialize converter\n",
        "token_converter = TokenTo3DConverter(tokenizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loader with Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from dataset import ARCDataset, ARCTorchDataset \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "print(\"Loading datasets...\")\n",
        "train_dataset = ARCDataset(\n",
        "    challenges_path='arc-agi_training_challenges.json',\n",
        "    solutions_path='arc-agi_training_solutions.json'\n",
        ")\n",
        "\n",
        "test_dataset = ARCDataset(\n",
        "    challenges_path='arc-agi_test_challenges.json'\n",
        ")\n",
        "\n",
        "print(f\"Training challenges: {len(train_dataset.get_all_challenges())}\")\n",
        "print(f\"Test challenges: {len(test_dataset.get_all_challenges())}\")\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_torch_dataset = ARCTorchDataset(train_dataset, tokenizer, token_converter=token_converter)\n",
        "test_torch_dataset = ARCTorchDataset(test_dataset, tokenizer, token_converter=token_converter)\n",
        "\n",
        "print(f\"\\nTraining samples (with augmentation): {len(train_torch_dataset)}\")\n",
        "print(f\"Test samples: {len(test_torch_dataset)}\")\n",
        "\n",
        "# Test data loading\n",
        "sample = train_torch_dataset[0]\n",
        "print(f\"\\nSample data:\")\n",
        "print(f\"Sample ID: {sample['sample_id']}\")\n",
        "print(f\"Challenge ID: {sample['challenge_id']}\")\n",
        "print(f\"Input sequence length: {len(sample['input'])}\")\n",
        "print(f\"Target sequence length: {len(sample['target'])}\")\n",
        "print(f\"Input dims: {sample['input_dims']}\")\n",
        "print(f\"Output dims: {sample['output_dims']}\")\n",
        "print(f\"Test input dims: {sample['test_input_dims']}\")\n",
        "print(f\"Test output dims: {sample['test_output_dims']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# self.PAD_TOKEN = 10\n",
        "# self.SOS_TOKEN = 11  # Start of sequence\n",
        "# self.EOS_TOKEN = 12  # End of sequence\n",
        "# self.TRAIN_TOKEN = 13  # Start of training example\n",
        "# self.TEST_TOKEN = 14  # Start of test example\n",
        "# self.INPUT_TOKEN = 15  # Start of input grid\n",
        "# self.OUTPUT_TOKEN = 16  # Start of output grid\n",
        "# self.NEWLINE_TOKEN = 17  # Grid separator (], [)\n",
        "train_torch_dataset[0]['input']#[0:300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_torch_dataset[0]['target'][0:300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(train_torch_dataset), len(test_torch_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autoregressive Dataset for LLM Training\n",
        "\n",
        "For autoregressive training, we need to:\n",
        "1. Concatenate input + target into one sequence\n",
        "2. Create labels shifted by 1 position (next token prediction)\n",
        "3. Use causal masking so model can't see future tokens\n",
        "\n",
        "**Important**: This is NOT data leakage! During training, the model learns to predict the next token given previous tokens. During inference, we'll use the same autoregressive generation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from exploded_dataset import ARCExplodedDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create exploded datasets from existing ARCTorchDataset\n",
        "print(\"Creating exploded training dataset...\")\n",
        "train_exploded_dataset = ARCExplodedDataset(train_torch_dataset, tokenizer, sequence_length=5400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    print(train_exploded_dataset[i]['input_3d'][218:228], train_exploded_dataset[i]['target_vector'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## split to val / train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataLoader\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for batching\"\"\"\n",
        "    input_3d = torch.stack([item['input_3d'] for item in batch])\n",
        "    target_values = torch.stack([torch.tensor(item['target_value'], dtype=torch.long) for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "    \n",
        "    return {\n",
        "        'input_3d': input_3d,\n",
        "        'target_values': target_values,\n",
        "        'attention_mask': attention_mask\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split dataset by challenge_id (ensures no data leakage)\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Group samples by challenge_id\n",
        "challenge_to_indices = defaultdict(list)\n",
        "for idx in range(len(train_exploded_dataset)):\n",
        "    sample = train_exploded_dataset[idx]\n",
        "    challenge_id = sample['challenge_id']\n",
        "    challenge_to_indices[challenge_id].append(idx)\n",
        "\n",
        "# Get unique challenge IDs\n",
        "challenge_ids = list(challenge_to_indices.keys())\n",
        "print(f\"Total challenges: {len(challenge_ids)}\")\n",
        "\n",
        "# Shuffle and split challenges (not individual samples)\n",
        "random.seed(42)\n",
        "random.shuffle(challenge_ids)\n",
        "\n",
        "train_ratio = 0.8\n",
        "split_idx = int(len(challenge_ids) * train_ratio)\n",
        "train_challenge_ids = set(challenge_ids[:split_idx])\n",
        "val_challenge_ids = set(challenge_ids[split_idx:])\n",
        "\n",
        "print(f\"Train challenges: {len(train_challenge_ids)}\")\n",
        "print(f\"Val challenges: {len(val_challenge_ids)}\")\n",
        "\n",
        "# Collect indices for each split\n",
        "train_indices = []\n",
        "val_indices = []\n",
        "\n",
        "for challenge_id, indices in challenge_to_indices.items():\n",
        "    if challenge_id in train_challenge_ids:\n",
        "        train_indices.extend(indices)\n",
        "    else:\n",
        "        val_indices.extend(indices)\n",
        "\n",
        "print(f\"\\nTrain samples: {len(train_indices)}\")\n",
        "print(f\"Val samples: {len(val_indices)}\")\n",
        "\n",
        "# Create subset datasets\n",
        "train_dataset_split = Subset(train_exploded_dataset, train_indices)\n",
        "val_dataset_split = Subset(train_exploded_dataset, val_indices)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset_split,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset_split,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login(key='9c6d131f5fcedb96565fa31f4680c2da83ea07d5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NextTokenPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer model to predict next token from 3D vectors [value, x, y].\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size=18, d_model=16, nhead=8, num_layers=4, \n",
        "                 dim_feedforward=1024, max_seq_length=5400, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_length = max_seq_length\n",
        "        \n",
        "        # Embedding for token values (0-17)\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        \n",
        "        # Projection for x, y coordinates (add coordinate information)\n",
        "        self.coord_projection = nn.Linear(2, d_model)  # [x, y] -> d_model\n",
        "        \n",
        "        # Positional encoding (learned)\n",
        "        #self.pos_encoding = nn.Parameter(torch.randn(max_seq_length, d_model) * 0.02)\n",
        "        self.pos_encoding = self.create_sinusoidal_positional_encoding(max_seq_length, d_model)\n",
        "        \n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        # Output projection to vocab\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def create_sinusoidal_positional_encoding(self, max_len, d_model):\n",
        "        \"\"\"\n",
        "        Create sinusoidal positional encoding (no learnable parameters).\n",
        "        \n",
        "        Args:\n",
        "            max_len: Maximum sequence length\n",
        "            d_model: Model dimension\n",
        "        \n",
        "        Returns:\n",
        "            [max_len, d_model] tensor with positional encodings\n",
        "        \"\"\"\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices: sin\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices: cos\n",
        "        \n",
        "        return pe  # [max_len, d_model]\n",
        "\n",
        "        \n",
        "    def forward(self, input_3d, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_3d: [batch_size, seq_len, 3] - [value, x, y] vectors\n",
        "            attention_mask: [batch_size, seq_len] - 1 for real tokens, 0 for padding\n",
        "        \n",
        "        Returns:\n",
        "            logits: [batch_size, seq_len, vocab_size] - logits for each position\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = input_3d.shape\n",
        "        \n",
        "        # Extract components\n",
        "        token_values = input_3d[:, :, 0].long()  # [batch_size, seq_len] - token values\n",
        "        coordinates = input_3d[:, :, 1:3].float()  # [batch_size, seq_len, 2] - x, y\n",
        "        \n",
        "        # Embed tokens\n",
        "        token_emb = self.token_embedding(token_values)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Add coordinate information\n",
        "        coord_emb = self.coord_projection(coordinates)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Combine token and coordinate embeddings\n",
        "        x = token_emb + coord_emb  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = x + self.pos_encoding[:seq_len].unsqueeze(0)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        #x = self.dropout(x)\n",
        "        if attention_mask is not None:\n",
        "            padding_mask = (attention_mask == 0).bool()  # True for padding, False for real tokens\n",
        "        else:\n",
        "            padding_mask = None\n",
        "        \n",
        "        # Apply transformer\n",
        "        x = self.transformer(x, src_key_padding_mask=padding_mask)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Get logits for all positions\n",
        "        logits = self.output_proj(x)  # [batch_size, seq_len, vocab_size]\n",
        "        \n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = NextTokenPredictor(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    d_model=256,\n",
        "    nhead=8,\n",
        "    num_layers=4,\n",
        "    dim_feedforward=1024,\n",
        "    max_seq_length=5400,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(dataloader, desc=\"Training\")\n",
        "    for batch in pbar:\n",
        "        # Move to device\n",
        "        input_3d = batch['input_3d'].to(device)  # [batch_size, seq_len, 3]\n",
        "        target_values = batch['target_values'].to(device)  # [batch_size]\n",
        "        attention_mask = batch['attention_mask'].to(device)  # [batch_size, seq_len]\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_3d=input_3d, attention_mask=attention_mask)  # [batch_size, seq_len, vocab_size]\n",
        "        \n",
        "        # Get logits for the last non-padding position (where we predict)\n",
        "        # Find last non-padding position for each sequence\n",
        "        batch_size = input_3d.size(0)\n",
        "        seq_lengths = attention_mask.sum(dim=1) - 1  # -1 because we want the position before the target\n",
        "        last_logits = logits[torch.arange(batch_size), seq_lengths]  # [batch_size, vocab_size]\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = criterion(last_logits, target_values)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Metrics\n",
        "        total_loss += loss.item()\n",
        "        predictions = last_logits.argmax(dim=1)\n",
        "        correct += (predictions == target_values).sum().item()\n",
        "        total += target_values.size(0)\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'acc': f'{100 * correct / total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate model on dataset\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_3d = batch['input_3d'].to(device)\n",
        "            target_values = batch['target_values'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            \n",
        "            logits = model(input_3d=input_3d, attention_mask=attention_mask)\n",
        "            \n",
        "            batch_size = input_3d.size(0)\n",
        "            seq_lengths = attention_mask.sum(dim=1) - 1\n",
        "            last_logits = logits[torch.arange(batch_size), seq_lengths]\n",
        "            \n",
        "            loss = criterion(last_logits, target_values)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            predictions = last_logits.argmax(dim=1)\n",
        "            correct += (predictions == target_values).sum().item()\n",
        "            total += target_values.size(0)\n",
        "    \n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated training loop with validation and wandb logging\n",
        "num_epochs = 3\n",
        "print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "#wandb.init(\n",
        "#    name='test',\n",
        "#    project=\"arc-next-token-prediction\",\n",
        "#    config={\n",
        "#        \"vocab_size\": tokenizer.vocab_size,\n",
        "#        \"d_model\": model.d_model,\n",
        "#        \"nhead\": model.transformer.layers[0].self_attn.num_heads,\n",
        "#        \"num_layers\": len(model.transformer.layers),\n",
        "#        \"batch_size\": batch_size,\n",
        "#        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "#        \"max_seq_length\": model.max_seq_length,\n",
        "#    }\n",
        "#)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Log to wandb\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\nEpoch {epoch + 1} Results:\")\n",
        "    print(f\"  Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"  Val   - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
        "    print(f\"  Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Track best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        print(f\"  âœ“ New best validation accuracy: {best_val_acc:.2f}%\")\n",
        "        # Optionally save model checkpoint\n",
        "        # torch.save(model.state_dict(), 'best_model.pt')\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datascience",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
