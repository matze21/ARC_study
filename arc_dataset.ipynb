{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer and Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 18\n",
            "Special tokens: {'PAD': 10, 'SOS': 11, 'EOS': 12, 'TRAIN': 13, 'TEST': 14, 'INPUT': 15, 'OUTPUT': 16, 'NEWLINE': 17}\n",
            "\n",
            "Test grid: [[1, 2, 3], [4, 5, 6]]\n",
            "Tokens: [1, 2, 3, 17, 4, 5, 6]\n",
            "Back to grid: [[1, 2, 3], [4, 5, 6]]\n"
          ]
        }
      ],
      "source": [
        "class ARCTokenizer:\n",
        "    \"\"\"Tokenizer for ARC challenges with special tokens for structure\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Value tokens (0-9)\n",
        "        self.value_tokens = list(range(10))\n",
        "        \n",
        "        # Special tokens\n",
        "        self.PAD_TOKEN = 10\n",
        "        self.SOS_TOKEN = 11  # Start of sequence\n",
        "        self.EOS_TOKEN = 12  # End of sequence\n",
        "        self.TRAIN_TOKEN = 13  # Start of training example\n",
        "        self.TEST_TOKEN = 14  # Start of test example\n",
        "        self.INPUT_TOKEN = 15  # Start of input grid\n",
        "        self.OUTPUT_TOKEN = 16  # Start of output grid\n",
        "        self.NEWLINE_TOKEN = 17  # Grid separator (], [)\n",
        "        \n",
        "        self.vocab_size = 18\n",
        "        \n",
        "        # Token mappings\n",
        "        self.token_to_id = {\n",
        "            'PAD': self.PAD_TOKEN,\n",
        "            'SOS': self.SOS_TOKEN,\n",
        "            'EOS': self.EOS_TOKEN,\n",
        "            'TRAIN': self.TRAIN_TOKEN,\n",
        "            'TEST': self.TEST_TOKEN,\n",
        "            'INPUT': self.INPUT_TOKEN,\n",
        "            'OUTPUT': self.OUTPUT_TOKEN,\n",
        "            'NEWLINE': self.NEWLINE_TOKEN\n",
        "        }\n",
        "    \n",
        "    def grid_to_tokens(self, grid: List[List[int]]) -> List[int]:\n",
        "        \"\"\"Convert 2D grid to token sequence\"\"\"\n",
        "        if not grid or not grid[0]:\n",
        "            return []\n",
        "        \n",
        "        tokens = []\n",
        "        for i, row in enumerate(grid):\n",
        "            for j, value in enumerate(row):\n",
        "                tokens.append(value)  # Just the value, position will be encoded separately\n",
        "            if i < len(grid) - 1:  # Add newline between rows (except last)\n",
        "                tokens.append(self.NEWLINE_TOKEN)\n",
        "        \n",
        "        return tokens\n",
        "    \n",
        "    def tokens_to_grid(self, tokens: List[int], target_shape: Tuple[int, int]) -> List[List[int]]:\n",
        "        \"\"\"Convert token sequence back to 2D grid\"\"\"\n",
        "        h, w = target_shape\n",
        "        grid = [[0 for _ in range(w)] for _ in range(h)]\n",
        "        \n",
        "        # Filter out special tokens and newlines\n",
        "        values = [t for t in tokens if t < 10]  # Only keep value tokens (0-9)\n",
        "        \n",
        "        idx = 0\n",
        "        for i in range(h):\n",
        "            for j in range(w):\n",
        "                if idx < len(values):\n",
        "                    grid[i][j] = values[idx]\n",
        "                    idx += 1\n",
        "        \n",
        "        return grid\n",
        "    \n",
        "    def create_input_sequence(self, train_examples: List[Dict], test_input: List[List[int]]) -> List[int]:\n",
        "        \"\"\"Create input sequence from training examples and test input\"\"\"\n",
        "        sequence = [self.SOS_TOKEN]\n",
        "        \n",
        "        # Add training examples (exactly 2)\n",
        "        for i, example in enumerate(train_examples[:2]):\n",
        "            sequence.append(self.TRAIN_TOKEN)\n",
        "            \n",
        "            # Add input\n",
        "            sequence.append(self.INPUT_TOKEN)\n",
        "            input_tokens = self.grid_to_tokens(example['input'])\n",
        "            sequence.extend(input_tokens)\n",
        "            \n",
        "            # Add output\n",
        "            sequence.append(self.OUTPUT_TOKEN)\n",
        "            output_tokens = self.grid_to_tokens(example['output'])\n",
        "            sequence.extend(output_tokens)\n",
        "        \n",
        "        # Add test input\n",
        "        sequence.append(self.TEST_TOKEN)\n",
        "        sequence.append(self.INPUT_TOKEN)\n",
        "        test_tokens = self.grid_to_tokens(test_input)\n",
        "        sequence.extend(test_tokens)\n",
        "        \n",
        "        return sequence\n",
        "    \n",
        "    def create_target_sequence(self, target_grid: List[List[int]]) -> List[int]:\n",
        "        \"\"\"Create target sequence for training\"\"\"\n",
        "        sequence = [self.SOS_TOKEN]\n",
        "        sequence.append(self.OUTPUT_TOKEN)\n",
        "        target_tokens = self.grid_to_tokens(target_grid)\n",
        "        sequence.extend(target_tokens)\n",
        "        sequence.append(self.EOS_TOKEN)\n",
        "        return sequence\n",
        "    \n",
        "    def pad_sequence(self, sequence: List[int], max_length: int) -> List[int]:\n",
        "        \"\"\"Pad sequence to max_length\"\"\"\n",
        "        if len(sequence) > max_length:\n",
        "            return sequence[:max_length]\n",
        "        return sequence + [self.PAD_TOKEN] * (max_length - len(sequence))\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = ARCTokenizer()\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"Special tokens: {tokenizer.token_to_id}\")\n",
        "\n",
        "# Test tokenizer\n",
        "test_grid = [[1, 2, 3], [4, 5, 6]]\n",
        "tokens = tokenizer.grid_to_tokens(test_grid)\n",
        "print(f\"\\nTest grid: {test_grid}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Back to grid: {tokenizer.tokens_to_grid(tokens, (2, 3))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token converter (enrich with position info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Token to 3D Vector Converter\n",
        "\n",
        "class TokenTo3DConverter:\n",
        "    \"\"\"Converts token sequences to 3D vectors [value, x, y] with coordinate information\"\"\"\n",
        "    \n",
        "    def __init__(self, tokenizer: ARCTokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def tokens_to_3d(self, \n",
        "                     tokens: List[int],\n",
        "                     input_dims: List[Tuple[int, int]],\n",
        "                     output_dims: List[Tuple[int, int]],\n",
        "                     test_input_dims: Tuple[int, int],\n",
        "                     test_output_dims: Optional[Tuple[int, int]] = None,\n",
        "                     is_target: bool = False) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Convert token sequence to 3D vectors [value, x, y]\n",
        "        \n",
        "        Args:\n",
        "            tokens: List of token IDs\n",
        "            input_dims: List of (height, width) for training input grids\n",
        "            output_dims: List of (height, width) for training output grids\n",
        "            test_input_dims: (height, width) for test input grid\n",
        "            is_target: If True, this is a target sequence (starts with OUTPUT_TOKEN)\n",
        "        \n",
        "        Returns:\n",
        "            Tensor of shape [seq_len, 3] where each row is [value, x, y]\n",
        "            Special tokens have x=-1, y=-1\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        \n",
        "        # Track current grid context\n",
        "        current_grid_type = None  # 'train_input', 'train_output', 'test_input'\n",
        "        current_grid_idx = 0\n",
        "        current_row = 0\n",
        "        current_col = 0\n",
        "        current_grid_dims = None\n",
        "        \n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            token = tokens[i]\n",
        "            \n",
        "            # Handle special tokens that change context\n",
        "            if token == self.tokenizer.SOS_TOKEN:\n",
        "                result.append([token, -1, -1])\n",
        "                i += 1\n",
        "                continue\n",
        "            elif token == self.tokenizer.EOS_TOKEN:\n",
        "                result.append([token, -1, -1])\n",
        "                i += 1\n",
        "                continue\n",
        "            elif token == self.tokenizer.PAD_TOKEN:\n",
        "                result.append([token, -1, -1])\n",
        "                i += 1\n",
        "                continue\n",
        "            elif token == self.tokenizer.TRAIN_TOKEN:\n",
        "                current_grid_type = None\n",
        "                current_grid_idx = 0\n",
        "                result.append([token, -1, -1])\n",
        "                i += 1\n",
        "                continue\n",
        "            elif token == self.tokenizer.TEST_TOKEN:\n",
        "                current_grid_type = None\n",
        "                result.append([token, -1, -1])\n",
        "                i += 1\n",
        "                continue\n",
        "            elif token == self.tokenizer.INPUT_TOKEN:\n",
        "                # Determine which input grid we're in\n",
        "                if is_target:\n",
        "                    # In target sequence, INPUT_TOKEN shouldn't appear\n",
        "                    result.append([token, -1, -1])\n",
        "                    i += 1\n",
        "                    continue\n",
        "                \n",
        "                if current_grid_type is None:\n",
        "                    # First INPUT after TRAIN - this is training input\n",
        "                    if current_grid_idx < len(input_dims):\n",
        "                        current_grid_dims = input_dims[current_grid_idx]\n",
        "                        current_grid_type = 'train_input'\n",
        "                elif current_grid_type == 'train_output':\n",
        "                    # INPUT after OUTPUT in training - next training example\n",
        "                    current_grid_idx += 1\n",
        "                    if current_grid_idx < len(input_dims):\n",
        "                        current_grid_dims = input_dims[current_grid_idx]\n",
        "                        current_grid_type = 'train_input'\n",
        "                elif current_grid_type is None:\n",
        "                    # INPUT after TEST - this is test input\n",
        "                    current_grid_dims = test_input_dims\n",
        "                    current_grid_type = 'test_input'\n",
        "                \n",
        "                current_row = 0\n",
        "                current_col = 0\n",
        "                result.append([token, -1, -1])\n",
        "                i += 1\n",
        "                continue\n",
        "            elif token == self.tokenizer.OUTPUT_TOKEN:\n",
        "                # Determine which output grid we're in\n",
        "                if current_grid_type == 'train_input':\n",
        "                    # OUTPUT after INPUT in training\n",
        "                    if current_grid_idx < len(output_dims):\n",
        "                        current_grid_dims = output_dims[current_grid_idx]\n",
        "                        current_grid_type = 'train_output'\n",
        "                elif current_grid_type is None:\n",
        "                    # OUTPUT at start (for target sequence) or after TEST\n",
        "                    if is_target:\n",
        "                        # For target sequence, use test_output_dims if available\n",
        "                        if test_output_dims is not None:\n",
        "                            current_grid_dims = test_output_dims\n",
        "                        elif len(output_dims) > 0:\n",
        "                            current_grid_dims = output_dims[0]  # Fallback to first output dims\n",
        "                        else:\n",
        "                            current_grid_dims = (1, 1)  # Default fallback\n",
        "                    elif len(output_dims) > 0:\n",
        "                        current_grid_dims = output_dims[0]\n",
        "                    current_grid_type = 'train_output'\n",
        "                \n",
        "                current_row = 0\n",
        "                current_col = 0\n",
        "                result.append([token, -1, -1])\n",
        "                i += 1\n",
        "                continue\n",
        "            elif token == self.tokenizer.NEWLINE_TOKEN:\n",
        "                # Move to next row\n",
        "                if current_grid_dims is not None:\n",
        "                    current_row += 1\n",
        "                    current_col = 0\n",
        "                result.append([token, -1, -1])\n",
        "                i += 1\n",
        "                continue\n",
        "            elif token < 10:  # Value token (0-9)\n",
        "                # This is a grid value - add coordinates\n",
        "                if current_grid_dims is not None:\n",
        "                    h, w = current_grid_dims\n",
        "                    # Clamp to valid ranges\n",
        "                    row = min(current_row, h - 1)\n",
        "                    col = min(current_col, w - 1)\n",
        "                    result.append([token, col, row])  # x=col, y=row\n",
        "                    \n",
        "                    # Move to next column\n",
        "                    current_col += 1\n",
        "                else:\n",
        "                    # No grid context, treat as special\n",
        "                    result.append([token, -1, -1])\n",
        "                i += 1\n",
        "            else:\n",
        "                # Unknown token, treat as special\n",
        "                result.append([token, -1, -1])\n",
        "                i += 1\n",
        "        \n",
        "        return torch.tensor(result, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Initialize converter\n",
        "token_converter = TokenTo3DConverter(tokenizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loader with Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n",
            "Training challenges: 400\n",
            "Test challenges: 100\n",
            "\n",
            "Training samples (with augmentation): 614\n",
            "Test samples: 152\n",
            "\n",
            "Sample data:\n",
            "Sample ID: 007bbfb7_orig\n",
            "Challenge ID: 007bbfb7\n",
            "Input sequence length: 5400\n",
            "Target sequence length: 1000\n",
            "Input dims: [(3, 3), (3, 3)]\n",
            "Output dims: [(9, 9), (9, 9)]\n",
            "Test input dims: (3, 3)\n",
            "Test output dims: (9, 9)\n"
          ]
        }
      ],
      "source": [
        "class ARCDataset:\n",
        "    \"\"\"Dataset class for ARC challenges with data augmentation\"\"\"\n",
        "    \n",
        "    def __init__(self, challenges_path: str, solutions_path: str = None):\n",
        "        self.challenges_path = challenges_path\n",
        "        self.solutions_path = solutions_path\n",
        "        \n",
        "        # Load challenges\n",
        "        with open(challenges_path, 'r') as f:\n",
        "            self.challenges = json.load(f)\n",
        "        \n",
        "        # Load solutions if provided\n",
        "        self.solutions = None\n",
        "        if solutions_path:\n",
        "            with open(solutions_path, 'r') as f:\n",
        "                self.solutions = json.load(f)\n",
        "    \n",
        "    def get_challenge_data(self, challenge_id: str) -> Dict:\n",
        "        \"\"\"Get data for a specific challenge\"\"\"\n",
        "        challenge = self.challenges[challenge_id]\n",
        "        \n",
        "        # Get training examples\n",
        "        train_examples = challenge.get('train', [])\n",
        "        \n",
        "        # Get test examples\n",
        "        test_examples = challenge.get('test', [])\n",
        "        \n",
        "        # Get solution if available\n",
        "        solution = None\n",
        "        if self.solutions and challenge_id in self.solutions:\n",
        "            solution = self.solutions[challenge_id][0]  # First solution\n",
        "        \n",
        "        return {\n",
        "            'train_examples': train_examples,\n",
        "            'test_examples': test_examples,\n",
        "            'solution': solution,\n",
        "            'challenge_id': challenge_id\n",
        "        }\n",
        "    \n",
        "    def get_all_challenges(self) -> List[str]:\n",
        "        \"\"\"Get list of all challenge IDs\"\"\"\n",
        "        return list(self.challenges.keys())\n",
        "    \n",
        "    def create_augmented_samples(self, challenge_id: str) -> List[Dict]:\n",
        "        \"\"\"Create augmented training samples from a challenge\"\"\"\n",
        "        data = self.get_challenge_data(challenge_id)\n",
        "        \n",
        "        # For training data, we can create augmented samples using the original train examples\n",
        "        # and the test examples (which don't have outputs, so we can't use them for training)\n",
        "        samples = []\n",
        "        \n",
        "        # Use original training examples (these have both input and output)\n",
        "        if len(data['train_examples']) >= 2:\n",
        "            # Create sample with first 2 training examples\n",
        "            train_examples = data['train_examples'][:2]\n",
        "            test_input = data['test_examples'][0]['input'] if data['test_examples'] else []\n",
        "            test_output = data['solution']\n",
        "            \n",
        "            samples.append({\n",
        "                'train_examples': train_examples,\n",
        "                'test_input': test_input,\n",
        "                'test_output': test_output,\n",
        "                'challenge_id': challenge_id,\n",
        "                'sample_id': f\"{challenge_id}_orig\"\n",
        "            })\n",
        "            \n",
        "            # If we have more training examples, create additional samples\n",
        "            if len(data['train_examples']) >= 4:\n",
        "                # Use examples 2 and 3 as training\n",
        "                train_examples = data['train_examples'][2:4]\n",
        "                samples.append({\n",
        "                    'train_examples': train_examples,\n",
        "                    'test_input': test_input,\n",
        "                    'test_output': test_output,\n",
        "                    'challenge_id': challenge_id,\n",
        "                    'sample_id': f\"{challenge_id}_aug_0\"\n",
        "                })\n",
        "            \n",
        "            # If we have even more, use examples 1 and 3\n",
        "            if len(data['train_examples']) >= 4:\n",
        "                train_examples = [data['train_examples'][1], data['train_examples'][3]]\n",
        "                samples.append({\n",
        "                    'train_examples': train_examples,\n",
        "                    'test_input': test_input,\n",
        "                    'test_output': test_output,\n",
        "                    'challenge_id': challenge_id,\n",
        "                    'sample_id': f\"{challenge_id}_aug_1\"\n",
        "                })\n",
        "        \n",
        "        return samples\n",
        "\n",
        "class ARCTorchDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for ARC challenges\"\"\"\n",
        "    \n",
        "    def __init__(self, arc_dataset: ARCDataset, tokenizer: ARCTokenizer, \n",
        "                 token_converter = None):  # Optional converter\n",
        "        self.arc_dataset = arc_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_converter = token_converter  # Optional converter for 3D vectors\n",
        "        \n",
        "        # Create all samples with augmentation\n",
        "        self.samples = []\n",
        "        for challenge_id in arc_dataset.get_all_challenges():\n",
        "            samples = arc_dataset.create_augmented_samples(challenge_id)\n",
        "            self.samples.extend(samples)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        \n",
        "        # Create input sequence\n",
        "        input_seq = self.tokenizer.create_input_sequence(\n",
        "            sample['train_examples'], \n",
        "            sample['test_input']\n",
        "        )\n",
        "        \n",
        "        # Create target sequence\n",
        "        if sample['test_output']:\n",
        "            target_seq = self.tokenizer.create_target_sequence(sample['test_output'])\n",
        "        else:\n",
        "            # Create dummy target for test data\n",
        "            target_seq = [self.tokenizer.SOS_TOKEN, self.tokenizer.EOS_TOKEN]\n",
        "        \n",
        "        # Pad sequences\n",
        "        input_seq = self.tokenizer.pad_sequence(input_seq, 5400)   # 5 * 30x30 + bunch of extra tokens + possible target 30x30= 6*30x30\n",
        "        target_seq = self.tokenizer.pad_sequence(target_seq, 1000) # max 30x30 + punch of extra tokens\n",
        "        \n",
        "        # Calculate dimensions\n",
        "        input_dims = []\n",
        "        output_dims = []\n",
        "        \n",
        "        for example in sample['train_examples']:\n",
        "            input_dims.append((len(example['input']), len(example['input'][0]) if example['input'] else 0))\n",
        "            output_dims.append((len(example['output']), len(example['output'][0]) if example['output'] else 0))\n",
        "        \n",
        "        test_input_dims = (len(sample['test_input']), len(sample['test_input'][0]) if sample['test_input'] else 0)\n",
        "        test_output_dims = (len(sample['test_output']), len(sample['test_output'][0]) if sample['test_output'] else 0)\n",
        "        \n",
        "        # Convert to 3D vectors if converter is provided\n",
        "        if self.token_converter is not None:\n",
        "            input_3d = self.token_converter.tokens_to_3d(\n",
        "                input_seq,\n",
        "                input_dims,\n",
        "                output_dims,\n",
        "                test_input_dims,\n",
        "                test_output_dims=test_output_dims,\n",
        "                is_target=False\n",
        "            )\n",
        "            target_3d = self.token_converter.tokens_to_3d(\n",
        "                target_seq,\n",
        "                input_dims,\n",
        "                output_dims,\n",
        "                test_input_dims,\n",
        "                test_output_dims=test_output_dims,\n",
        "                is_target=True\n",
        "            )\n",
        "            return {\n",
        "                'input': input_3d,  # Shape: [seq_len, 3] - [value, x, y]\n",
        "                'target': target_3d,  # Shape: [seq_len, 3] - [value, x, y]\n",
        "                'input_tokens': torch.tensor(input_seq, dtype=torch.int8),  # Keep original tokens too (int8 for memory efficiency)\n",
        "                'target_tokens': torch.tensor(target_seq, dtype=torch.int8),  # Keep original tokens too (int8 for memory efficiency)\n",
        "                'sample_id': sample['sample_id'],\n",
        "                'challenge_id': sample['challenge_id'],\n",
        "                'input_dims': input_dims,\n",
        "                'output_dims': output_dims,\n",
        "                'test_input_dims': test_input_dims,\n",
        "                'test_output_dims': test_output_dims\n",
        "            }\n",
        "        else:\n",
        "            # Return original token format\n",
        "            return {\n",
        "                'input': torch.tensor(input_seq, dtype=torch.int8),  # int8 for memory efficiency\n",
        "                'target': torch.tensor(target_seq, dtype=torch.int8),  # int8 for memory efficiency\n",
        "                'sample_id': sample['sample_id'],\n",
        "                'challenge_id': sample['challenge_id'],\n",
        "                'input_dims': input_dims,\n",
        "                'output_dims': output_dims,\n",
        "                'test_input_dims': test_input_dims,\n",
        "                'test_output_dims': test_output_dims\n",
        "            }\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading datasets...\")\n",
        "train_dataset = ARCDataset(\n",
        "    challenges_path='arc-agi_training_challenges.json',\n",
        "    solutions_path='arc-agi_training_solutions.json'\n",
        ")\n",
        "\n",
        "test_dataset = ARCDataset(\n",
        "    challenges_path='arc-agi_test_challenges.json'\n",
        ")\n",
        "\n",
        "print(f\"Training challenges: {len(train_dataset.get_all_challenges())}\")\n",
        "print(f\"Test challenges: {len(test_dataset.get_all_challenges())}\")\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_torch_dataset = ARCTorchDataset(train_dataset, tokenizer, token_converter=token_converter)\n",
        "test_torch_dataset = ARCTorchDataset(test_dataset, tokenizer, token_converter=token_converter)\n",
        "\n",
        "print(f\"\\nTraining samples (with augmentation): {len(train_torch_dataset)}\")\n",
        "print(f\"Test samples: {len(test_torch_dataset)}\")\n",
        "\n",
        "# Test data loading\n",
        "sample = train_torch_dataset[0]\n",
        "print(f\"\\nSample data:\")\n",
        "print(f\"Sample ID: {sample['sample_id']}\")\n",
        "print(f\"Challenge ID: {sample['challenge_id']}\")\n",
        "print(f\"Input sequence length: {len(sample['input'])}\")\n",
        "print(f\"Target sequence length: {len(sample['target'])}\")\n",
        "print(f\"Input dims: {sample['input_dims']}\")\n",
        "print(f\"Output dims: {sample['output_dims']}\")\n",
        "print(f\"Test input dims: {sample['test_input_dims']}\")\n",
        "print(f\"Test output dims: {sample['test_output_dims']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "length_arr =[]\n",
        "for i in range(len(train_torch_dataset)):\n",
        "    length_arr.append(len(train_torch_dataset[i]['target']))\n",
        "max(length_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[11, -1, -1],\n",
              "        [13, -1, -1],\n",
              "        [15, -1, -1],\n",
              "        ...,\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# self.PAD_TOKEN = 10\n",
        "# self.SOS_TOKEN = 11  # Start of sequence\n",
        "# self.EOS_TOKEN = 12  # End of sequence\n",
        "# self.TRAIN_TOKEN = 13  # Start of training example\n",
        "# self.TEST_TOKEN = 14  # Start of test example\n",
        "# self.INPUT_TOKEN = 15  # Start of input grid\n",
        "# self.OUTPUT_TOKEN = 16  # Start of output grid\n",
        "# self.NEWLINE_TOKEN = 17  # Grid separator (], [)\n",
        "train_torch_dataset[0]['input']#[0:300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[11, -1, -1],\n",
              "        [16, -1, -1],\n",
              "        [ 7,  0,  0],\n",
              "        [ 0,  1,  0],\n",
              "        [ 7,  2,  0],\n",
              "        [ 0,  3,  0],\n",
              "        [ 0,  4,  0],\n",
              "        [ 0,  5,  0],\n",
              "        [ 7,  6,  0],\n",
              "        [ 0,  7,  0],\n",
              "        [ 7,  8,  0],\n",
              "        [17, -1, -1],\n",
              "        [ 7,  0,  1],\n",
              "        [ 0,  1,  1],\n",
              "        [ 7,  2,  1],\n",
              "        [ 0,  3,  1],\n",
              "        [ 0,  4,  1],\n",
              "        [ 0,  5,  1],\n",
              "        [ 7,  6,  1],\n",
              "        [ 0,  7,  1],\n",
              "        [ 7,  8,  1],\n",
              "        [17, -1, -1],\n",
              "        [ 7,  0,  2],\n",
              "        [ 7,  1,  2],\n",
              "        [ 0,  2,  2],\n",
              "        [ 0,  3,  2],\n",
              "        [ 0,  4,  2],\n",
              "        [ 0,  5,  2],\n",
              "        [ 7,  6,  2],\n",
              "        [ 7,  7,  2],\n",
              "        [ 0,  8,  2],\n",
              "        [17, -1, -1],\n",
              "        [ 7,  0,  3],\n",
              "        [ 0,  1,  3],\n",
              "        [ 7,  2,  3],\n",
              "        [ 0,  3,  3],\n",
              "        [ 0,  4,  3],\n",
              "        [ 0,  5,  3],\n",
              "        [ 7,  6,  3],\n",
              "        [ 0,  7,  3],\n",
              "        [ 7,  8,  3],\n",
              "        [17, -1, -1],\n",
              "        [ 7,  0,  4],\n",
              "        [ 0,  1,  4],\n",
              "        [ 7,  2,  4],\n",
              "        [ 0,  3,  4],\n",
              "        [ 0,  4,  4],\n",
              "        [ 0,  5,  4],\n",
              "        [ 7,  6,  4],\n",
              "        [ 0,  7,  4],\n",
              "        [ 7,  8,  4],\n",
              "        [17, -1, -1],\n",
              "        [ 7,  0,  5],\n",
              "        [ 7,  1,  5],\n",
              "        [ 0,  2,  5],\n",
              "        [ 0,  3,  5],\n",
              "        [ 0,  4,  5],\n",
              "        [ 0,  5,  5],\n",
              "        [ 7,  6,  5],\n",
              "        [ 7,  7,  5],\n",
              "        [ 0,  8,  5],\n",
              "        [17, -1, -1],\n",
              "        [ 7,  0,  6],\n",
              "        [ 0,  1,  6],\n",
              "        [ 7,  2,  6],\n",
              "        [ 7,  3,  6],\n",
              "        [ 0,  4,  6],\n",
              "        [ 7,  5,  6],\n",
              "        [ 0,  6,  6],\n",
              "        [ 0,  7,  6],\n",
              "        [ 0,  8,  6],\n",
              "        [17, -1, -1],\n",
              "        [ 7,  0,  7],\n",
              "        [ 0,  1,  7],\n",
              "        [ 7,  2,  7],\n",
              "        [ 7,  3,  7],\n",
              "        [ 0,  4,  7],\n",
              "        [ 7,  5,  7],\n",
              "        [ 0,  6,  7],\n",
              "        [ 0,  7,  7],\n",
              "        [ 0,  8,  7],\n",
              "        [17, -1, -1],\n",
              "        [ 7,  0,  8],\n",
              "        [ 7,  1,  8],\n",
              "        [ 0,  2,  8],\n",
              "        [ 7,  3,  8],\n",
              "        [ 7,  4,  8],\n",
              "        [ 0,  5,  8],\n",
              "        [ 0,  6,  8],\n",
              "        [ 0,  7,  8],\n",
              "        [ 0,  8,  8],\n",
              "        [12, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1],\n",
              "        [10, -1, -1]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_torch_dataset[0]['target'][0:300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(614, 152)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_torch_dataset), len(test_torch_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autoregressive Dataset for LLM Training\n",
        "\n",
        "For autoregressive training, we need to:\n",
        "1. Concatenate input + target into one sequence\n",
        "2. Create labels shifted by 1 position (next token prediction)\n",
        "3. Use causal masking so model can't see future tokens\n",
        "\n",
        "**Important**: This is NOT data leakage! During training, the model learns to predict the next token given previous tokens. During inference, we'll use the same autoregressive generation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating exploded training dataset...\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'ARCTorchDataset' object has no attribute 'max_length'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 131\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Create exploded datasets from existing ARCTorchDataset\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating exploded training dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m train_exploded_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mARCExplodedDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_torch_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[10], line 21\u001b[0m, in \u001b[0;36mARCExplodedDataset.__init__\u001b[0;34m(self, torch_dataset, tokenizer)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorch_dataset \u001b[38;5;241m=\u001b[39m torch_dataset\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Create all exploded samples\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploded_samples \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ARCTorchDataset' object has no attribute 'max_length'"
          ]
        }
      ],
      "source": [
        "# Refactored ARCExplodedDataset - works directly with 3D vectors\n",
        "class ARCExplodedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Explodes ARCTorchDataset into trainable samples.\n",
        "    \n",
        "    Takes each sample from ARCTorchDataset and creates multiple training samples:\n",
        "    - Sample 0: input → predict target[0]\n",
        "    - Sample 1: input + target[0] → predict target[1]\n",
        "    - Sample 2: input + target[0:2] → predict target[2]\n",
        "    - etc.\n",
        "    \n",
        "    Expects both input and target to be in 3D vector format [value, x, y].\n",
        "    When adding target tokens:\n",
        "    1. Loop through input sequence and replace first PAD token with target token\n",
        "    2. If no PAD token found, append to end and remove first token\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, torch_dataset: ARCTorchDataset, tokenizer: ARCTokenizer, sequence_length: int = 5400):\n",
        "        self.torch_dataset = torch_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sequence_length = sequence_length\n",
        "        \n",
        "        # Create all exploded samples\n",
        "        self.exploded_samples = []\n",
        "        \n",
        "        print(f\"Exploding {len(torch_dataset)} base samples...\")\n",
        "        for base_idx in tqdm(range(len(torch_dataset))):\n",
        "            base_sample = torch_dataset[base_idx]\n",
        "            \n",
        "            # Get input and target as 3D tensors [seq_len, 3]\n",
        "            input_3d = base_sample['input']  # Shape: [max_length, 3]\n",
        "            target_3d = base_sample['target']  # Shape: [max_length, 3]\n",
        "            \n",
        "            # Find actual length of input (before padding)\n",
        "            # PAD token has value = PAD_TOKEN (10), x = -1, y = -1\n",
        "            input_actual_len = 0\n",
        "            for i in range(input_3d.shape[0]):\n",
        "                if input_3d[i, 0].item() == self.tokenizer.PAD_TOKEN:\n",
        "                    break\n",
        "            input_actual_len = i-1\n",
        "            \n",
        "            target_actual_len = 0\n",
        "            for i in range(target_3d.shape[0]):\n",
        "                if target_3d[i, 0].item() == self.tokenizer.PAD_TOKEN:\n",
        "                    break\n",
        "            target_actual_len = i-1\n",
        "            \n",
        "            print(input_actual_len, target_actual_len)\n",
        "            \n",
        "            target_vectors = target_3d[0:target_actual_len]\n",
        "            \n",
        "            \n",
        "            # Optimized version - remove unnecessary cloning and use input_actual_len directly\n",
        "            # Replace the target_vectors collection and loop in cell 9 with this:\n",
        "\n",
        "            # In the target_vectors collection (around line 43-50):\n",
        "            # Change: target_vectors.append(target_3d[i].clone())\n",
        "            # To:     target_vectors.append(target_3d[i])  # No clone needed\n",
        "\n",
        "            # In the loop (around line 58-67):\n",
        "            # Replace the entire loop with this optimized version:\n",
        "\n",
        "            # Start with full input sequence (we'll modify it in place)\n",
        "            current_seq = input_3d.clone()\n",
        "            for i, target_vector in enumerate(target_vectors):\n",
        "                # Calculate position where we should place this target token\n",
        "                # Start from input_actual_len and add i (position in target sequence)\n",
        "                target_pos = input_actual_len + i\n",
        "                \n",
        "                if i>0:\n",
        "                    # first target vector is not added to the input sequence\n",
        "                    if target_pos < self.sequence_length:\n",
        "                        # Check if position has a PAD token\n",
        "                        if current_seq[target_pos, 0].item() == self.tokenizer.PAD_TOKEN:\n",
        "                            # Replace PAD token with target vector\n",
        "                            current_seq[target_pos] = target_vectors[i-1]\n",
        "                        else:\n",
        "                            #print(\"Sequence is full - append and remove from beginning\", target_pos, current_seq.shape)\n",
        "                            # Sequence is full - append and remove from beginning\n",
        "                            current_seq = torch.cat([current_seq[1:], target_vectors[i-1].unsqueeze(0)], dim=0)\n",
        "                    else:\n",
        "                        current_seq = torch.cat([current_seq[1:], target_vectors[i-1].unsqueeze(0)], dim=0)\n",
        "\n",
        "                # Store exploded sample\n",
        "                exploded_sample = {\n",
        "                    'input_3d': current_seq.clone(),\n",
        "                    'target_vector': target_vector.clone(),  # Clone here since we store it separately\n",
        "                    'target_position': i,\n",
        "                    'base_sample_idx': base_idx,\n",
        "                    'base_sample_id': base_sample.get('sample_id', f'sample_{base_idx}'),\n",
        "                    'challenge_id': base_sample.get('challenge_id', ''),\n",
        "                    'input_dims': base_sample.get('input_dims', []),\n",
        "                    'output_dims': base_sample.get('output_dims', []),\n",
        "                    'test_input_dims': base_sample.get('test_input_dims', (0, 0)),\n",
        "                    'test_output_dims': base_sample.get('test_output_dims', (0, 0)),\n",
        "                }\n",
        "\n",
        "                self.exploded_samples.append(exploded_sample)\n",
        "        \n",
        "        print(f\"Created {len(self.exploded_samples)} exploded samples from {len(torch_dataset)} base samples\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.exploded_samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.exploded_samples[idx]\n",
        "        \n",
        "        input_3d = sample['input_3d']  # Shape: [max_length, 3]\n",
        "        target_vector = sample['target_vector']  # Shape: [3]\n",
        "        \n",
        "        # Create attention mask (1 for non-padding, 0 for padding)\n",
        "        attention_mask = (input_3d[:, 0] != self.tokenizer.PAD_TOKEN).long()\n",
        "        \n",
        "        return {\n",
        "            'input_3d': input_3d,  # [max_length, 3] - full 3D vectors\n",
        "            'target_vector': target_vector,  # [3] - target as 3D vector\n",
        "            'target_value': target_vector[0].item(),  # Just the value token for convenience\n",
        "            'attention_mask': attention_mask,  # [max_length]\n",
        "            'target_position': sample['target_position'],\n",
        "            'base_sample_idx': sample['base_sample_idx'],\n",
        "            'base_sample_id': sample['base_sample_id'],\n",
        "            'challenge_id': sample['challenge_id'],\n",
        "            'input_dims': sample['input_dims'],\n",
        "            'output_dims': sample['output_dims'],\n",
        "            'test_input_dims': sample['test_input_dims'],\n",
        "            'test_output_dims': sample['test_output_dims'],\n",
        "        }\n",
        "\n",
        "# Create exploded datasets from existing ARCTorchDataset\n",
        "print(\"Creating exploded training dataset...\")\n",
        "train_exploded_dataset = ARCExplodedDataset(train_torch_dataset, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    print(train_exploded_dataset[i]['input_3d'][218:228], train_exploded_dataset[i]['target_vector'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datascience",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
