{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer and Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizer import ARCTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = ARCTokenizer()\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "print(f\"Special tokens: {tokenizer.token_to_id}\")\n",
        "\n",
        "# Test tokenizer\n",
        "test_grid = [[1, 2, 3], [4, 5, 6]]\n",
        "tokens = tokenizer.grid_to_tokens(test_grid)\n",
        "print(f\"\\nTest grid: {test_grid}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Back to grid: {tokenizer.tokens_to_grid(tokens, (2, 3))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token converter (enrich with position info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from token_converter import TokenTo3DConverter\n",
        "# Initialize converter\n",
        "token_converter = TokenTo3DConverter(tokenizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loader with Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from dataset import ARCDataset, ARCTorchDataset \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "print(\"Loading datasets...\")\n",
        "train_dataset = ARCDataset(\n",
        "    challenges_path='arc-agi_training_challenges.json',\n",
        "    solutions_path='arc-agi_training_solutions.json'\n",
        ")\n",
        "\n",
        "test_dataset = ARCDataset(\n",
        "    challenges_path='arc-agi_test_challenges.json'\n",
        ")\n",
        "\n",
        "print(f\"Training challenges: {len(train_dataset.get_all_challenges())}\")\n",
        "print(f\"Test challenges: {len(test_dataset.get_all_challenges())}\")\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_torch_dataset = ARCTorchDataset(train_dataset, tokenizer, token_converter=token_converter)\n",
        "test_torch_dataset = ARCTorchDataset(test_dataset, tokenizer, token_converter=token_converter)\n",
        "\n",
        "print(f\"\\nTraining samples (with augmentation): {len(train_torch_dataset)}\")\n",
        "print(f\"Test samples: {len(test_torch_dataset)}\")\n",
        "\n",
        "# Test data loading\n",
        "sample = train_torch_dataset[0]\n",
        "print(f\"\\nSample data:\")\n",
        "print(f\"Sample ID: {sample['sample_id']}\")\n",
        "print(f\"Challenge ID: {sample['challenge_id']}\")\n",
        "print(f\"Input sequence length: {len(sample['input'])}\")\n",
        "print(f\"Target sequence length: {len(sample['target'])}\")\n",
        "print(f\"Input dims: {sample['input_dims']}\")\n",
        "print(f\"Output dims: {sample['output_dims']}\")\n",
        "print(f\"Test input dims: {sample['test_input_dims']}\")\n",
        "print(f\"Test output dims: {sample['test_output_dims']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# self.PAD_TOKEN = 10\n",
        "# self.SOS_TOKEN = 11  # Start of sequence\n",
        "# self.EOS_TOKEN = 12  # End of sequence\n",
        "# self.TRAIN_TOKEN = 13  # Start of training example\n",
        "# self.TEST_TOKEN = 14  # Start of test example\n",
        "# self.INPUT_TOKEN = 15  # Start of input grid\n",
        "# self.OUTPUT_TOKEN = 16  # Start of output grid\n",
        "# self.NEWLINE_TOKEN = 17  # Grid separator (], [)\n",
        "train_torch_dataset[0]['input']#[0:300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_torch_dataset[0]['target'][0:300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(train_torch_dataset), len(test_torch_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autoregressive Dataset for LLM Training\n",
        "\n",
        "For autoregressive training, we need to:\n",
        "1. Concatenate input + target into one sequence\n",
        "2. Create labels shifted by 1 position (next token prediction)\n",
        "3. Use causal masking so model can't see future tokens\n",
        "\n",
        "**Important**: This is NOT data leakage! During training, the model learns to predict the next token given previous tokens. During inference, we'll use the same autoregressive generation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from exploded_dataset import ARCExplodedDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create exploded datasets from existing ARCTorchDataset\n",
        "print(\"Creating exploded training dataset...\")\n",
        "train_exploded_dataset = ARCExplodedDataset(train_torch_dataset, tokenizer, sequence_length=5400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    print(train_exploded_dataset[i]['input_3d'][218:228], train_exploded_dataset[i]['target_vector'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## split to val / train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split dataset by challenge_id (ensures no data leakage)\n",
        "# Create TWO validation sets: tiny (frequent) and full (accurate)\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Group samples by challenge_id\n",
        "challenge_to_indices = defaultdict(list)\n",
        "for idx in range(len(train_exploded_dataset)):\n",
        "    sample = train_exploded_dataset[idx]\n",
        "    challenge_id = sample['challenge_id']\n",
        "    challenge_to_indices[challenge_id].append(idx)\n",
        "\n",
        "# Get unique challenge IDs\n",
        "challenge_ids = list(challenge_to_indices.keys())\n",
        "print(f\"Total challenges: {len(challenge_ids)}\")\n",
        "\n",
        "# Shuffle and split challenges (not individual samples)\n",
        "random.seed(42)\n",
        "random.shuffle(challenge_ids)\n",
        "\n",
        "train_ratio = 0.8\n",
        "split_idx = int(len(challenge_ids) * train_ratio)\n",
        "train_challenge_ids = set(challenge_ids[:split_idx])\n",
        "val_challenge_ids_all = set(challenge_ids[split_idx:])\n",
        "\n",
        "# Split validation challenges into tiny and full\n",
        "val_challenge_ids_list = list(val_challenge_ids_all)\n",
        "random.shuffle(val_challenge_ids_list)\n",
        "tiny_val_ratio = 0.1  # 10% of validation challenges for tiny set\n",
        "tiny_split_idx = int(len(val_challenge_ids_list) * tiny_val_ratio)\n",
        "tiny_val_challenge_ids = set(val_challenge_ids_list[:tiny_split_idx])\n",
        "full_val_challenge_ids = set(val_challenge_ids_list[tiny_split_idx:])\n",
        "\n",
        "print(f\"Train challenges: {len(train_challenge_ids)}\")\n",
        "print(f\"Tiny val challenges: {len(tiny_val_challenge_ids)}\")\n",
        "print(f\"Full val challenges: {len(full_val_challenge_ids)}\")\n",
        "\n",
        "# Collect indices for each split\n",
        "train_indices = []\n",
        "tiny_val_indices = []\n",
        "full_val_indices = []\n",
        "\n",
        "for challenge_id, indices in challenge_to_indices.items():\n",
        "    if challenge_id in train_challenge_ids:\n",
        "        train_indices.extend(indices)\n",
        "    elif challenge_id in tiny_val_challenge_ids:\n",
        "        tiny_val_indices.extend(indices)\n",
        "    elif challenge_id in full_val_challenge_ids:\n",
        "        full_val_indices.extend(indices)\n",
        "\n",
        "print(f\"\\nTrain samples: {len(train_indices)}\")\n",
        "print(f\"Tiny val samples: {len(tiny_val_indices)}\")\n",
        "print(f\"Full val samples: {len(full_val_indices)}\")\n",
        "\n",
        "# Create subset datasets\n",
        "train_dataset_split = Subset(train_exploded_dataset, train_indices)\n",
        "tiny_val_dataset_split = Subset(train_exploded_dataset, tiny_val_indices)\n",
        "full_val_dataset_split = Subset(train_exploded_dataset, full_val_indices)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset_split,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "# Tiny validation loader (for frequent checks)\n",
        "tiny_val_loader = DataLoader(\n",
        "    tiny_val_dataset_split,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "# Full validation loader (for accurate metrics)\n",
        "full_val_loader = DataLoader(\n",
        "    full_val_dataset_split,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
        "print(f\"Tiny val batches: {len(tiny_val_loader)}\")\n",
        "print(f\"Full val batches: {len(full_val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimized live color augmentation during training (vectorized)\n",
        "import numpy as np\n",
        "\n",
        "def apply_random_color_mapping(sample, apply_probability=1.0):\n",
        "    \"\"\"\n",
        "    Apply a random color permutation (0-9) to a sample.\n",
        "    OPTIMIZED: Uses vectorized PyTorch operations instead of Python loops.\n",
        "    \n",
        "    Args:\n",
        "        sample: Dict with 'input_3d' and 'target_vector'\n",
        "        apply_probability: Probability of applying augmentation (1.0 = always, 0.5 = 50% chance)\n",
        "    \n",
        "    Returns:\n",
        "        Augmented sample with permuted colors\n",
        "    \"\"\"\n",
        "    if np.random.random() > apply_probability:\n",
        "        return sample  # Skip augmentation\n",
        "    \n",
        "    # Create random permutation of colors 0-9\n",
        "    permuted_colors = torch.randperm(10, dtype=torch.int8)  # Vectorized permutation\n",
        "    \n",
        "    # Create lookup table: mapping[old_color] = new_color\n",
        "    # For colors 0-9, use permuted mapping; for special tokens (10+), keep original\n",
        "    mapping = torch.arange(18, dtype=torch.int8)  # Default: identity mapping\n",
        "    mapping[:10] = permuted_colors  # Apply permutation to colors 0-9\n",
        "    \n",
        "    # Apply to input_3d (vectorized - much faster!)\n",
        "    input_3d = sample['input_3d'].clone()\n",
        "    color_values = input_3d[:, 0]  # Extract color values (already long)\n",
        "    input_3d[:, 0] = mapping[color_values]  # Apply mapping in one operation\n",
        "    \n",
        "    # Apply to target_vector\n",
        "    target_vector = sample['target_vector'].clone()\n",
        "    target_color = target_vector[0]  # Already long\n",
        "    target_vector[0] = mapping[target_color]\n",
        "    \n",
        "    # Create augmented sample\n",
        "    augmented_sample = sample.copy()\n",
        "    augmented_sample['input_3d'] = input_3d\n",
        "    augmented_sample['target_vector'] = target_vector\n",
        "    augmented_sample['target_value'] = int(target_vector[0].item())\n",
        "    \n",
        "    return augmented_sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset wrapper for live color augmentation\n",
        "class AugmentedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wrapper that applies random color permutation augmentation on-the-fly.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_dataset, apply_probability=1.0):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.apply_probability = apply_probability\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.base_dataset[idx]\n",
        "        return apply_random_color_mapping(sample, self.apply_probability)\n",
        "\n",
        "# Usage example:\n",
        "# Wrap your training dataset with augmentation\n",
        "# train_dataset_augmented = AugmentedDataset(train_dataset_split, apply_probability=1.0)\n",
        "# train_loader = DataLoader(train_dataset_augmented, ...)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Augment in collate_fn (even simpler)\n",
        "def collate_fn_with_augmentation(batch, apply_probability=1.0):\n",
        "    \"\"\"\n",
        "    Collate function that applies color augmentation to each sample.\n",
        "    \n",
        "    Usage:\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset_split,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=lambda b: collate_fn_with_augmentation(b, apply_probability=1.0),\n",
        "            num_workers=4\n",
        "        )\n",
        "    \"\"\"\n",
        "    # Apply augmentation to each sample\n",
        "    augmented_batch = [apply_random_color_mapping(sample, apply_probability) for sample in batch]\n",
        "    \n",
        "    # Original collate logic\n",
        "    input_3d = torch.stack([item['input_3d'] for item in augmented_batch])\n",
        "    target_values = torch.stack([torch.tensor(item['target_value'], dtype=torch.long) for item in augmented_batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in augmented_batch])\n",
        "    \n",
        "    return {\n",
        "        'input_3d': input_3d,\n",
        "        'target_values': target_values,\n",
        "        'attention_mask': attention_mask\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if AUGMENTATION:\n",
        "    train_dataset_augmented = AugmentedDataset(train_exploded_dataset, apply_probability=1.0)\n",
        "\n",
        "\n",
        "if EVAL_COLOR_DIST:\n",
        "    from collections import Counter, defaultdict\n",
        "    def analyze_color_distribution(dataset):\n",
        "        \"\"\"Analyze the distribution of target colors (0-9) in the dataset\"\"\"\n",
        "        color_counts = Counter()\n",
        "        \n",
        "        print(\"Analyzing color distribution...\")\n",
        "        for idx in tqdm(range(len(dataset))):\n",
        "            sample = dataset[idx]\n",
        "            target_value = sample['target_value']\n",
        "            # Only count color tokens (0-9), ignore special tokens\n",
        "            color_counts[target_value] += 1\n",
        "        \n",
        "        return color_counts\n",
        "    aug_counts = analyze_color_distribution(train_dataset_augmented)\n",
        "    original_counts = analyze_color_distribution(train_exploded_dataset)\n",
        "    print(original_counts)\n",
        "    print(aug_counts) \n",
        "    # result, looks a lot better\n",
        "    # Counter({0: 86966, 8: 19131, 1: 17683, 4: 14558, 7: 13704, 3: 12134, 2: 11829, 5: 7086, 6: 5145, 9: 3042})\n",
        "    # Counter({2: 19417, 0: 19327, 4: 19150, 7: 19150, 8: 19144, 5: 19062, 3: 19055, 9: 19004, 6: 18995, 1: 18974})\n",
        "\n",
        "if AUGMENTATION:\n",
        "    train_exploded_dataset = train_dataset_augmented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# U-Net for Attention Matrix Pattern Detection\n",
        "class AttentionUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net architecture for processing attention matrices.\n",
        "    Detects hierarchical patterns in attention and outputs pattern-enhanced attention.\n",
        "    \n",
        "    Input: [batch, 1, seq_len, seq_len] attention scores\n",
        "    Output: [batch, 1, seq_len, seq_len] pattern-enhanced attention\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, base_channels=32, num_downsample=3):\n",
        "        super().__init__()\n",
        "        self.num_downsample = num_downsample\n",
        "        \n",
        "        # Encoder (downsampling path)\n",
        "        self.encoder_blocks = nn.ModuleList()\n",
        "        in_channels = 1\n",
        "        for i in range(num_downsample):\n",
        "            out_channels = base_channels * (2 ** i)\n",
        "            self.encoder_blocks.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(inplace=True)\n",
        "                )\n",
        "            )\n",
        "            in_channels = out_channels\n",
        "        \n",
        "        # Bottleneck (lowest resolution)\n",
        "        bottleneck_channels = base_channels * (2 ** num_downsample)\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, bottleneck_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(bottleneck_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(bottleneck_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        \n",
        "        # Decoder (upsampling path without skip connections - saves memory)\n",
        "        self.decoder_blocks = nn.ModuleList()\n",
        "        for i in range(num_downsample - 1, -1, -1):\n",
        "            in_channels = bottleneck_channels if i == num_downsample - 1 else base_channels * (2 ** (i + 1))\n",
        "            out_channels = base_channels * (2 ** i)\n",
        "            \n",
        "            # Create decoder block (no skip connections, so conv1 only expects out_channels)\n",
        "            self.decoder_blocks.append(nn.ModuleDict({\n",
        "                'upsample': nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "                'conv1': nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                'bn1': nn.BatchNorm2d(out_channels),\n",
        "                'conv2': nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                'bn2': nn.BatchNorm2d(out_channels)\n",
        "            }))\n",
        "        \n",
        "        # Final output layer\n",
        "        self.final_conv = nn.Conv2d(base_channels, 1, kernel_size=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch, 1, seq_len, seq_len] attention scores\n",
        "        \n",
        "        Returns:\n",
        "            [batch, 1, seq_len, seq_len] pattern-enhanced attention\n",
        "        \"\"\"\n",
        "        # Encoder path (no skip connections - saves memory)\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x)\n",
        "            # Downsample (using max pooling)\n",
        "            x = nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        \n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        \n",
        "        # Decoder path (without skip connections - saves memory)\n",
        "        for i, decoder_block in enumerate(self.decoder_blocks):\n",
        "            # Upsample\n",
        "            x = decoder_block['upsample'](x)\n",
        "            # No skip connection - just use upsampled features directly\n",
        "            # Apply conv layers (conv1 now expects only out_channels, not out_channels + skip_channels)\n",
        "            x = decoder_block['conv1'](x)\n",
        "            x = decoder_block['bn1'](x)\n",
        "            x = nn.functional.relu(x, inplace=True)\n",
        "            x = decoder_block['conv2'](x)\n",
        "            x = decoder_block['bn2'](x)\n",
        "            x = nn.functional.relu(x, inplace=True)\n",
        "        \n",
        "        # Final output\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Custom Transformer Layer with U-Net-filtered Attention\n",
        "class UNetFilteredAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom transformer layer that uses U-Net to detect patterns in attention matrix.\n",
        "    \n",
        "    The idea is to:\n",
        "    1. Compute full attention matrix (Q @ K^T)\n",
        "    2. Apply U-Net to detect hierarchical patterns\n",
        "    3. Combine original and pattern-enhanced attention\n",
        "    4. Apply softmax and use for attention\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, nhead=1, dim_feedforward=2048, dropout=0.1, \n",
        "                 unet_base_channels=32, unet_num_downsample=3):\n",
        "        super().__init__()\n",
        "        assert d_model % nhead == 0, \"d_model must be divisible by nhead\"\n",
        "        assert nhead == 1, \"Currently only supports single head\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.d_k = d_model // nhead\n",
        "        \n",
        "        # Multi-head attention projections\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        # U-Net to detect patterns in attention matrix\n",
        "        self.unet = AttentionUNet(\n",
        "            base_channels=unet_base_channels,\n",
        "            num_downsample=unet_num_downsample\n",
        "        )\n",
        "        \n",
        "        # Learnable combination weight (optional)\n",
        "        self.pattern_weight = nn.Parameter(torch.tensor(0.5))\n",
        "        \n",
        "        # Feedforward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_feedforward, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "        # Layer norms\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, seq_len, d_model]\n",
        "            src_key_padding_mask: [batch_size, seq_len] - True for padding tokens\n",
        "        \n",
        "        Returns:\n",
        "            [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # Self-attention with U-Net pattern detection\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        \n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # Compute Q, K, V\n",
        "        Q = self.w_q(x)  # [batch_size, seq_len, d_model]\n",
        "        K = self.w_k(x)  # [batch_size, seq_len, d_model]\n",
        "        V = self.w_v(x)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # For single head, we don't need to reshape\n",
        "        # Compute attention scores (full attention matrix)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # [batch, seq_len, seq_len]\n",
        "        \n",
        "        # Apply padding mask if provided\n",
        "        if src_key_padding_mask is not None:\n",
        "            # Expand mask: [batch, seq_len] -> [batch, seq_len, 1]\n",
        "            mask = src_key_padding_mask.unsqueeze(2)  # [batch, seq_len, 1]\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "            # Also mask columns\n",
        "            mask = src_key_padding_mask.unsqueeze(1)  # [batch, 1, seq_len]\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "        \n",
        "        # Apply U-Net to detect patterns\n",
        "        # Reshape for U-Net: [batch, seq_len, seq_len] -> [batch, 1, seq_len, seq_len]\n",
        "        scores_for_unet = scores.unsqueeze(1)  # [batch, 1, seq_len, seq_len]\n",
        "        \n",
        "        # U-Net processes attention matrix (use FP16 to save memory)\n",
        "        # autocast() automatically converts operations to FP16 where supported\n",
        "        # This includes Conv2d, activations, and intermediate tensors (skip connections)\n",
        "        # BatchNorm runs in FP32 for stability, but activations are still FP16\n",
        "        from torch.cuda.amp import autocast\n",
        "        with autocast():\n",
        "            pattern_enhanced = self.unet(scores_for_unet)  # [batch, 1, seq_len, seq_len]\n",
        "        pattern_enhanced = pattern_enhanced.squeeze(1)  # [batch, seq_len, seq_len]\n",
        "        \n",
        "        # Combine original and pattern-enhanced attention\n",
        "        # Option 1: Weighted combination (learnable weight)\n",
        "        scores = (1 - torch.sigmoid(self.pattern_weight)) * scores + torch.sigmoid(self.pattern_weight) * pattern_enhanced\n",
        "        \n",
        "        # Option 2: Add (uncomment to use instead)\n",
        "        # scores = scores + pattern_enhanced\n",
        "        \n",
        "        # Option 3: Multiply (pattern gating)\n",
        "        # scores = scores * torch.sigmoid(pattern_enhanced)\n",
        "        \n",
        "        # Apply softmax\n",
        "        attn_weights = torch.softmax(scores, dim=-1)  # [batch, seq_len, seq_len]\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        attn_output = torch.matmul(attn_weights, V)  # [batch, seq_len, d_model]\n",
        "        \n",
        "        # Output projection\n",
        "        attn_output = self.w_o(attn_output)\n",
        "        x = residual + self.dropout(attn_output)\n",
        "        \n",
        "        # Feedforward\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = residual + self.ffn(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login(key='9c6d131f5fcedb96565fa31f4680c2da83ea07d5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory debugging helper - add this before training\n",
        "def debug_memory_usage(model, input_3d, attention_mask, device):\n",
        "    \"\"\"Debug GPU memory usage at different stages\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA not available, cannot debug memory\")\n",
        "        return\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"\\n=== MEMORY DEBUGGING ===\")\n",
        "    print(f\"Initial: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
        "    \n",
        "    # Move to device\n",
        "    input_3d = input_3d.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    print(f\"After moving to device: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "    \n",
        "    # Forward pass with debugging\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_3d=input_3d, attention_mask=attention_mask, debug_memory=True)\n",
        "        print(f\"After forward pass: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "    \n",
        "    print(\"=== END DEBUGGING ===\\n\")\n",
        "\n",
        "# To use: Get a small batch and call this\n",
        "# sample_batch = next(iter(train_loader))\n",
        "# debug_memory_usage(model, sample_batch['input_3d'][:1], sample_batch['attention_mask'][:1], device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# todo:\n",
        "- augment data such that network is not just predicting always 0\n",
        "- smaller feed forward dimension?\n",
        "- understand how do calculate attention matrix (only need to calculate upper triangular matrix?)\n",
        "- how to "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Transformer Layer with CNN-filtered Attention\n",
        "class CNNFilteredAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom transformer layer that computes full attention matrix and filters it with a CNN.\n",
        "    \n",
        "    The idea is to:\n",
        "    1. Compute full attention matrix (Q @ K^T)\n",
        "    2. Apply CNN to filter/modify attention weights\n",
        "    3. Apply softmax and use filtered attention\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, \n",
        "                 cnn_kernel_size=3, cnn_channels=32):\n",
        "        super().__init__()\n",
        "        assert d_model % nhead == 0, \"d_model must be divisible by nhead\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.d_k = d_model // nhead\n",
        "        \n",
        "        # Multi-head attention projections\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        # CNN to filter attention weights\n",
        "        # Input: [batch, nhead, seq_len, seq_len] attention matrix\n",
        "        # We'll treat it as [batch * nhead, 1, seq_len, seq_len] for CNN\n",
        "        self.cnn_filter = nn.Sequential(\n",
        "            nn.Conv2d(1, cnn_channels, kernel_size=cnn_kernel_size, padding=cnn_kernel_size//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(cnn_channels, 1, kernel_size=cnn_kernel_size, padding=cnn_kernel_size//2),\n",
        "        )\n",
        "        \n",
        "        # Feedforward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_feedforward, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "        # Layer norms\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, seq_len, d_model]\n",
        "            src_key_padding_mask: [batch_size, seq_len] - True for padding tokens\n",
        "        \n",
        "        Returns:\n",
        "            [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # Self-attention with CNN filtering\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        \n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # Compute Q, K, V\n",
        "        Q = self.w_q(x)  # [batch_size, seq_len, d_model]\n",
        "        K = self.w_k(x)  # [batch_size, seq_len, d_model]\n",
        "        V = self.w_v(x)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)  # [batch, nhead, seq_len, d_k]\n",
        "        K = K.view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)  # [batch, nhead, seq_len, d_k]\n",
        "        V = V.view(batch_size, seq_len, self.nhead, self.d_k).transpose(1, 2)  # [batch, nhead, seq_len, d_k]\n",
        "        \n",
        "        # Compute attention scores (full attention matrix)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # [batch, nhead, seq_len, seq_len]\n",
        "        \n",
        "        # Apply padding mask if provided\n",
        "        if src_key_padding_mask is not None:\n",
        "            # Expand mask: [batch, seq_len] -> [batch, 1, 1, seq_len]\n",
        "            mask = src_key_padding_mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq_len]\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "        \n",
        "        # Apply CNN to filter attention weights\n",
        "        # Reshape for CNN: [batch, nhead, seq_len, seq_len] -> [batch * nhead, 1, seq_len, seq_len]\n",
        "        attn_shape = scores.shape\n",
        "        scores_reshaped = scores.view(-1, 1, seq_len, seq_len)  # [batch * nhead, 1, seq_len, seq_len]\n",
        "        \n",
        "        # Apply CNN filter\n",
        "        filtered_scores = self.cnn_filter(scores_reshaped)  # [batch * nhead, 1, seq_len, seq_len]\n",
        "        \n",
        "        # Reshape back: [batch * nhead, 1, seq_len, seq_len] -> [batch, nhead, seq_len, seq_len]\n",
        "        filtered_scores = filtered_scores.view(attn_shape)\n",
        "        \n",
        "        # Combine original and filtered (you can experiment with this)\n",
        "        # Option 1: Use only filtered\n",
        "        # scores = filtered_scores\n",
        "        \n",
        "        # Option 2: Add residual connection\n",
        "        scores = scores + filtered_scores\n",
        "        \n",
        "        # Option 3: Weighted combination (uncomment to use)\n",
        "        # alpha = 0.5  # or learnable parameter\n",
        "        # scores = (1 - alpha) * scores + alpha * filtered_scores\n",
        "        \n",
        "        # Apply softmax\n",
        "        attn_weights = torch.softmax(scores, dim=-1)  # [batch, nhead, seq_len, seq_len]\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        attn_output = torch.matmul(attn_weights, V)  # [batch, nhead, seq_len, d_k]\n",
        "        \n",
        "        # Concatenate heads\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()  # [batch, seq_len, nhead, d_k]\n",
        "        attn_output = attn_output.view(batch_size, seq_len, self.d_model)  # [batch, seq_len, d_model]\n",
        "        \n",
        "        # Output projection\n",
        "        attn_output = self.w_o(attn_output)\n",
        "        x = residual + self.dropout(attn_output)\n",
        "        \n",
        "        # Feedforward\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = residual + self.ffn(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class CustomTransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom transformer encoder using CNN-filtered attention layers\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder_layer, num_layers):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
        "        \n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, seq_len, d_model]\n",
        "            src_key_padding_mask: [batch_size, seq_len] - True for padding tokens\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_key_padding_mask)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NextTokenPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer model to predict next token from 3D vectors [value, x, y].\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size=18, d_model=16, nhead=8, num_layers=4, \n",
        "                 dim_feedforward=1024, max_seq_length=5400, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_length = max_seq_length\n",
        "        \n",
        "        # Embedding for token values (0-17)\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        \n",
        "        # Projection for x, y coordinates (add coordinate information)\n",
        "        self.coord_projection = nn.Linear(2, d_model)  # [x, y] -> d_model\n",
        "        \n",
        "        # Positional encoding (learned)\n",
        "        #self.pos_encoding = nn.Parameter(torch.randn(max_seq_length, d_model) * 0.02)\n",
        "        #self.pos_encoding = self.create_sinusoidal_positional_encoding(max_seq_length, d_model)\n",
        "        pos_encoding = self.create_sinusoidal_positional_encoding(max_seq_length, d_model)\n",
        "        self.register_buffer('pos_encoding', pos_encoding)\n",
        "        \n",
        "        # Custom Transformer encoder with CNN-filtered attention\n",
        "        encoder_layer = CNNFilteredAttentionLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            cnn_kernel_size=3,  # You can experiment with different kernel sizes\n",
        "            cnn_channels=32     # You can experiment with different channel counts\n",
        "        )\n",
        "        self.transformer = CustomTransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        # Output projection to vocab\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def create_sinusoidal_positional_encoding(self, max_len, d_model):\n",
        "        \"\"\"\n",
        "        Create sinusoidal positional encoding (no learnable parameters).\n",
        "        \n",
        "        Args:\n",
        "            max_len: Maximum sequence length\n",
        "            d_model: Model dimension\n",
        "        \n",
        "        Returns:\n",
        "            [max_len, d_model] tensor with positional encodings\n",
        "        \"\"\"\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices: sin\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices: cos\n",
        "        \n",
        "        return pe  # [max_len, d_model]\n",
        "\n",
        "        \n",
        "    def forward(self, input_3d, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_3d: [batch_size, seq_len, 3] - [value, x, y] vectors\n",
        "            attention_mask: [batch_size, seq_len] - 1 for real tokens, 0 for padding\n",
        "        \n",
        "        Returns:\n",
        "            logits: [batch_size, seq_len, vocab_size] - logits for each position\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = input_3d.shape\n",
        "        \n",
        "        # Extract components\n",
        "        token_values = input_3d[:, :, 0].long()  # [batch_size, seq_len] - token values\n",
        "        coordinates = input_3d[:, :, 1:3].float()  # [batch_size, seq_len, 2] - x, y\n",
        "        \n",
        "        # Embed tokens\n",
        "        token_emb = self.token_embedding(token_values)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Add coordinate information\n",
        "        coord_emb = self.coord_projection(coordinates)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Combine token and coordinate embeddings\n",
        "        x = token_emb + coord_emb  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = x + self.pos_encoding[:seq_len].unsqueeze(0)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        #x = self.dropout(x)\n",
        "        if attention_mask is not None:\n",
        "            padding_mask = (attention_mask == 0).bool()  # True for padding, False for real tokens\n",
        "        else:\n",
        "            padding_mask = None\n",
        "        \n",
        "        # Apply transformer\n",
        "        x = self.transformer(x, src_key_padding_mask=padding_mask)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Get logits for all positions\n",
        "        logits = self.output_proj(x)  # [batch_size, seq_len, vocab_size]\n",
        "        \n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = NextTokenPredictor(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    d_model=16,\n",
        "    nhead=1,\n",
        "    num_layers=4,\n",
        "    dim_feedforward=128,\n",
        "    max_seq_length=5400,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "import os\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, batch_idx, val_loss, val_acc, train_loss, train_acc, \n",
        "                   checkpoint_dir='checkpoints', is_best=False):\n",
        "    \"\"\"\n",
        "    Save model checkpoint\n",
        "    \n",
        "    Args:\n",
        "        model: The model to save\n",
        "        optimizer: The optimizer to save\n",
        "        epoch: Current epoch number\n",
        "        batch_idx: Current batch index\n",
        "        val_loss: Validation loss\n",
        "        val_acc: Validation accuracy\n",
        "        train_loss: Training loss\n",
        "        train_acc: Training accuracy\n",
        "        checkpoint_dir: Directory to save checkpoints\n",
        "        is_best: Whether this is the best model so far\n",
        "    \"\"\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    \n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'batch': batch_idx,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_loss': val_loss,\n",
        "        'val_accuracy': val_acc,\n",
        "        'train_loss': train_loss,\n",
        "        'train_accuracy': train_acc,\n",
        "    }\n",
        "    \n",
        "    # Save regular checkpoint\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch{epoch}_batch{batch_idx}.pt')\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    \n",
        "    # Save best model if applicable\n",
        "    if is_best:\n",
        "        best_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
        "        torch.save(checkpoint, best_path)\n",
        "        print(f\"   Best model saved: {best_path}\")\n",
        "    \n",
        "    return checkpoint_path\n",
        "    \n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate model on dataset\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_3d = batch['input_3d'].to(device)\n",
        "            target_values = batch['target_values'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            \n",
        "            logits = model(input_3d=input_3d, attention_mask=attention_mask)\n",
        "            \n",
        "            batch_size = input_3d.size(0)\n",
        "            seq_lengths = attention_mask.sum(dim=1) - 1\n",
        "            last_logits = logits[torch.arange(batch_size), seq_lengths]\n",
        "            \n",
        "            loss = criterion(last_logits, target_values)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            predictions = last_logits.argmax(dim=1)\n",
        "            correct += (predictions == target_values).sum().item()\n",
        "            total += target_values.size(0)\n",
        "    \n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Updated train_epoch with periodic validation during training\n",
        "def train_epoch(model, train_dataloader, tiny_val_loader, full_val_loader, criterion, optimizer, device, \n",
        "                log_every_n_batches=2, tiny_val_every_n_batches=10, full_val_every_n_batches=200):\n",
        "    \"\"\"\n",
        "    Train for one epoch with periodic validation using two validation sets\n",
        "    \n",
        "    Args:\n",
        "        model: The model to train\n",
        "        train_dataloader: Training data loader\n",
        "        tiny_val_loader: Tiny validation loader (for frequent checks)\n",
        "        full_val_loader: Full validation loader (for accurate metrics)\n",
        "        criterion: Loss function\n",
        "        optimizer: Optimizer\n",
        "        device: Device to run on\n",
        "        log_every_n_batches: Log to wandb every N batches\n",
        "        tiny_val_every_n_batches: Run tiny validation every N batches (default: 10)\n",
        "        full_val_every_n_batches: Run full validation every N batches (default: 200)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    best_val_los=1e6\n",
        "    \n",
        "    pbar = tqdm(train_dataloader, desc=\"Training\")\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        # Move to device\n",
        "        input_3d = batch['input_3d'].to(device)  # [batch_size, seq_len, 3]\n",
        "        target_values = batch['target_values'].to(device)  # [batch_size]\n",
        "        attention_mask = batch['attention_mask'].to(device)  # [batch_size, seq_len]\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_3d=input_3d, attention_mask=attention_mask)  # [batch_size, seq_len, vocab_size]\n",
        "        \n",
        "        # Get logits for the last non-padding position (where we predict)\n",
        "        # Find last non-padding position for each sequence\n",
        "        batch_size = input_3d.size(0)\n",
        "        seq_lengths = attention_mask.sum(dim=1) - 1  # -1 because we want the position before the target\n",
        "        last_logits = logits[torch.arange(batch_size), seq_lengths]  # [batch_size, vocab_size]\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = criterion(last_logits, target_values)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Metrics\n",
        "        total_loss += loss.item()\n",
        "        predictions = last_logits.argmax(dim=1)\n",
        "        batch_correct = (predictions == target_values).sum().item()\n",
        "        correct += batch_correct\n",
        "        total += target_values.size(0)\n",
        "        batch_acc = 100 * batch_correct / target_values.size(0)\n",
        "        \n",
        "        # Log to wandb every N batches (default: every other batch)\n",
        "        if batch_idx % log_every_n_batches == 0:\n",
        "            wandb.log({\n",
        "                \"batch_loss\": loss.item(),\n",
        "                \"batch_accuracy\": batch_acc,\n",
        "                \"running_accuracy\": 100 * correct / total,\n",
        "            })\n",
        "        \n",
        "        # Tiny validation (frequent, quick check)\n",
        "        if (batch_idx + 1) % tiny_val_every_n_batches == 0:\n",
        "            tiny_val_loss, tiny_val_acc = evaluate(model, tiny_val_loader, criterion, device)\n",
        "            \n",
        "            # Log tiny validation metrics\n",
        "            wandb.log({\n",
        "                \"tiny_val_loss\": tiny_val_avg_loss,\n",
        "                \"tiny_val_accuracy\": tiny_val_acc,\n",
        "                \"train_batch\": batch_idx + 1,\n",
        "            })\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100 * correct / total:.2f}%',\n",
        "                'tiny_val': f'{tiny_val_acc:.1f}%'\n",
        "            })\n",
        "            \n",
        "            model.train()  # Switch back to training mode\n",
        "        \n",
        "        # Full validation (less frequent, more accurate)\n",
        "        if (batch_idx + 1) % full_val_every_n_batches == 0:\n",
        "            full_val_loss, full_val_acc = evaluate(model, full_val_loader, criterion, device)\n",
        "            is_better=full_val_loss<best_val_los\n",
        "            if is_better:\n",
        "                best_val_los=full_val_loss\n",
        "\n",
        "            save_checkpoint(model, optimizer, epoch, batch_idx, val_loss, val_acc, train_loss, train_acc, \n",
        "                   checkpoint_dir='checkpoints', is_best=is_better)\n",
        "            \n",
        "            # Log full validation metrics\n",
        "            wandb.log({\n",
        "                \"full_val_loss\": full_val_avg_loss,\n",
        "                \"full_val_accuracy\": full_val_acc,\n",
        "                \"train_batch\": batch_idx + 1,\n",
        "            })\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100 * correct / total:.2f}%',\n",
        "                'full_val': f'{full_val_acc:.1f}%'\n",
        "            })\n",
        "            \n",
        "            model.train()  # Switch back to training mode\n",
        "        \n",
        "        # Update progress bar (if no validation was run this batch)\n",
        "        if (batch_idx + 1) % tiny_val_every_n_batches != 0 and (batch_idx + 1) % full_val_every_n_batches != 0:\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100 * correct / total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated training loop with two-tier validation\n",
        "num_epochs = 3\n",
        "print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Validation frequencies\n",
        "tiny_val_every_n_batches = 10   # Tiny validation every 10 batches (~30 seconds)\n",
        "full_val_every_n_batches = 200  # Full validation every 200 batches (~5-10 minutes)\n",
        "\n",
        "num_train_batches = len(train_loader)\n",
        "print(f\"Training batches per epoch: {num_train_batches}\")\n",
        "print(f\"Tiny validation: every {tiny_val_every_n_batches} batches ({max_tiny_val_batches} batches, ~{num_train_batches // tiny_val_every_n_batches} times/epoch)\")\n",
        "print(f\"Full validation: every {full_val_every_n_batches} batches ({max_full_val_batches} batches, ~{num_train_batches // full_val_every_n_batches} times/epoch)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "#wandb.init(\n",
        "#    name='test',\n",
        "#    project=\"arc-next-token-prediction\",\n",
        "#    config={\n",
        "#        \"vocab_size\": tokenizer.vocab_size,\n",
        "#        \"d_model\": model.d_model,\n",
        "#        \"nhead\": model.transformer.layers[0].self_attn.num_heads,\n",
        "#        \"num_layers\": len(model.transformer.layers),\n",
        "#        \"batch_size\": batch_size,\n",
        "#        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "#        \"max_seq_length\": model.max_seq_length,\n",
        "#    }\n",
        "#)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Train with two-tier validation during epoch\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, \n",
        "        train_loader, \n",
        "        tiny_val_loader,  # Tiny validation for frequent checks\n",
        "        full_val_loader,  # Full validation for accurate metrics\n",
        "        criterion, \n",
        "        optimizer, \n",
        "        device,\n",
        "        tiny_val_every_n_batches=tiny_val_every_n_batches,\n",
        "        full_val_every_n_batches=full_val_every_n_batches\n",
        "    )\n",
        "    \n",
        "    # Full validation at end of epoch (optional - can skip if you want)\n",
        "    print(\"\\nRunning full validation at end of epoch...\")\n",
        "    val_loss, val_acc = evaluate(model, full_val_loader, criterion, device)\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Log to wandb\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\nEpoch {epoch + 1} Results:\")\n",
        "    print(f\"  Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"  Val   - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
        "    print(f\"  Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Track best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        print(f\"   New best validation accuracy: {best_val_acc:.2f}%\")\n",
        "        # Optionally save model checkpoint\n",
        "        # torch.save(model.state_dict(), 'best_model.pt')\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "datascience",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
